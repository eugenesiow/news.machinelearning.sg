<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Biology Named Entity Recognition with BioBERT | News @ machinelearning.sg</title><meta name=keywords content="Natural Language Processing,Deep Learning,Machine Learning,GPU,Source Code,PyTorch,Named Entity Recognition,Biology,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Biology Named Entity Recognition with BioBERT"><meta property="og:description" content="tl;dr A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/"><meta property="og:image" content="https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-30T10:00:00+08:00"><meta property="article:modified_time" content="2020-12-30T10:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert.jpg"><meta name=twitter:title content="Biology Named Entity Recognition with BioBERT"><meta name=twitter:description content="tl;dr A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"Biology Named Entity Recognition with BioBERT","item":"https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Biology Named Entity Recognition with BioBERT","name":"Biology Named Entity Recognition with BioBERT","description":"tl;dr A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.","keywords":["Natural Language Processing","Deep Learning","Machine Learning","GPU","Source Code","PyTorch","Named Entity Recognition","Biology","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.\nNamed Entity Recognition on BC5CDR (Chemical + Disease Corpus) with BioBERT Notebook to train/fine-tune a BioBERT model to perform named entity recognition (NER).\nThe dataset used is a pre-processed version of the BC5CDR (BioCreative V CDR task corpus: a resource for relation extraction) dataset from Li et al. (2016).\nThe current state-of-the-art model on this dataset is the NER+PA+RL model from Nooralahzadeh et al. (2019) has an F1-score of 89.93%. The authors did not release the source code for the paper.\nOur model trained on top of BioBERT has an F1-score of 89.3% which is slightly worse than the state-of-the-art but almost as good as the #2 BioFlair!\nThe notebook is structured as follows:\nSetting up the GPU Environment Getting Data Training and Testing the Model Using the Model (Running Inference) Task Description Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens.\nSetting up the GPU Environment Ensure we have a GPU runtime If youâ€™re running this notebook in Google Colab, select Runtime \u003e Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator. This will allow us to use the GPU to train the model subsequently.\nInstall Dependencies and Restart Runtime !pip install -q transformers !pip install -q simpletransformers You might see the error ERROR: google-colab X.X.X has requirement ipykernel~=X.X, but you'll have ipykernel X.X.X which is incompatible after installing the dependencies. This is normal and caused by the simpletransformers library.\nThe solution to this will be to reset the execution environment now. Go to the menu Runtime \u003e Restart runtime then continue on from the next section to download and process the data.\nGetting Data Pulling the data from Github The dataset, includes train, test and dev sets, which we pull from the Github repository.\nimport urllib.request from pathlib import Path def download_file(url, output_file): Path(output_file).parent.mkdir(parents=True, exist_ok=True) urllib.request.urlretrieve (url, output_file) download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/train.txt', '/content/data/train.txt') download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/test.txt', '/content/data/test.txt') download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/dev.txt', '/content/data/dev.txt') Since the data is formatted in the CoNLL BIO type format (you can read more on the tagging format from this wikipedia article), we need to format it into a pandas dataframe with the following function. The 3 important columns in the dataframe are a word token (for mandarin this is a single character), a BIO label and a sentence_id to differentiate samples/sentences.\nimport pandas as pd def read_conll(filename): df = pd.read_csv(filename, sep = '\\t', header = None, keep_default_na = False, names = ['words', 'pos', 'chunk', 'labels'], quoting = 3, skip_blank_lines = False) df = df[~df['words'].astype(str).str.startswith('-DOCSTART- ')] # Remove the -DOCSTART- header df['sentence_id'] = (df.words == '').cumsum() return df[df.words != ''] Now we execute the function on the train, test and dev sets we have downloaded from Github. We also .head() the training set dataframe for the first 100 rows to check that the words, labels and sentence_id have been split properly.\ntrain_df = read_conll('/content/data/train.txt') test_df = read_conll('/content/data/test.txt') dev_df = read_conll('/content/data/dev.txt') train_df.head(100) We now print out the statistics (number of sentences) of the train, dev and test sets.\ndata = [[train_df['sentence_id'].nunique(), test_df['sentence_id'].nunique(), dev_df['sentence_id'].nunique()]] # Prints out the dataset sizes of train and test sets per label. pd.DataFrame(data, columns=[\"Train\", \"Test\", \"Dev\"]) Training and Testing the Model Set up the Training Arguments We set up the training arguments. Here we train to 10 epochs to get accuracy close to the SOTA. The train, test and dev sets are relatively small so we donâ€™t have to wait too long. We set a sliding window as NER sequences can be quite long and because we have limited GPU memory we canâ€™t increase the max_seq_length too long.\ntrain_args = { 'reprocess_input_data': True, 'overwrite_output_dir': True, 'sliding_window': True, 'max_seq_length': 64, 'num_train_epochs': 10, 'train_batch_size': 32, 'fp16': True, 'output_dir': '/outputs/', 'best_model_dir': '/outputs/best_model/', 'evaluate_during_training': True, } The following line of code saves (to the variable custom_labels) a set of all the NER tags/labels in the dataset.\ncustom_labels = list(train_df['labels'].unique()) print(custom_labels) Train the Model Once we have setup the train_args dictionary, the next step would be to train the model. We use the pre-trained BioBERT model (by DMIS Lab, Korea University) from the awesome Hugging Face Transformers library as the base and use the Simple Transformers library on top of it to make it so we can train the NER (sequence tagging) model with just a few lines of code.\nfrom simpletransformers.ner import NERModel from transformers import AutoTokenizer import pandas as pd import logging logging.basicConfig(level=logging.DEBUG) transformers_logger = logging.getLogger('transformers') transformers_logger.setLevel(logging.WARNING) # We use the bio BERT pre-trained model. model = NERModel('bert', 'dmis-lab/biobert-v1.1', labels=custom_labels, args=train_args) # Train the model # https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping model.train_model(train_df, eval_data=dev_df) # Evaluate the model in terms of accuracy score result, model_outputs, preds_list = model.eval_model(test_df) The F1-score for the model is 89.3% (â€˜f1_scoreâ€™: 0.8927974947807933).\nFor now thats the #3-ranked SOTA NER model on BC5CDR!\nUsing the Model (Running Inference) Running the model to do some predictions/inference is as simple as calling model.predict(samples). First we get a sentence from the test set and print it out. Then we run the prediction on the sentence.\nsample = test_df[test_df.sentence_id == 10].words.str.cat(sep=' ') print(sample) samples = [sample] predictions, _ = model.predict(samples) for idx, sample in enumerate(samples): print('{}: '.format(idx)) for word in predictions[idx]: print('{}'.format(word)) We can connect to Google Drive with the following code to save any files you want to persist. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the model checkpount files which are saved in the /outputs/ directory to your Google Drive.\nimport shutil shutil.move('/outputs/', \"/content/drive/My Drive/outputs/\") More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\nAlternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\nGoogle Colab vs Paperspace Gradient ","wordCount":"1093","inLanguage":"en","image":"https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert.jpg","datePublished":"2020-12-30T10:00:00+08:00","dateModified":"2020-12-30T10:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Biology Named Entity Recognition with BioBERT</h1><div class=post-meta><span title='2020-12-30 10:00:00 +0800 +08'>December 30, 2020</span>&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert_hu5bd7e8534e29f41c875b9d97fbb6b474_103415_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert_hu5bd7e8534e29f41c875b9d97fbb6b474_103415_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert_hu5bd7e8534e29f41c875b9d97fbb6b474_103415_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert_hu5bd7e8534e29f41c875b9d97fbb6b474_103415_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert.jpg 1200w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/biology_named_entity_recognition_with_biobert/biobert.jpg alt="Biology NER with BioBERT to Extract Diseases and Chemicals." width=1200 height=900></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li><a href=#named-entity-recognition-on-bc5cdr-chemical--disease-corpus-with-biobert aria-label="Named Entity Recognition on BC5CDR (Chemical + Disease Corpus) with BioBERT">Named Entity Recognition on BC5CDR (Chemical + Disease Corpus) with BioBERT</a><ul><ul><li><a href=#task-description aria-label="Task Description">Task Description</a></li></ul></ul></li><li><a href=#setting-up-the-gpu-environment aria-label="Setting up the GPU Environment">Setting up the GPU Environment</a><ul><ul><li><a href=#ensure-we-have-a-gpu-runtime aria-label="Ensure we have a GPU runtime">Ensure we have a GPU runtime</a></li><li><a href=#install-dependencies-and-restart-runtime aria-label="Install Dependencies and Restart Runtime">Install Dependencies and Restart Runtime</a></li></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><ul><li><a href=#pulling-the-data-from-github aria-label="Pulling the data from Github">Pulling the data from Github</a></li></ul></ul></li><li><a href=#training-and-testing-the-model aria-label="Training and Testing the Model">Training and Testing the Model</a><ul><ul><li><a href=#set-up-the-training-arguments aria-label="Set up the Training Arguments">Set up the Training Arguments</a></li><li><a href=#train-the-model aria-label="Train the Model">Train the Model</a></li></ul></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li><li><a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li><a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> A step-by-step tutorial to train a BioBERT model for named entity recognition (NER), extracting diseases and
chemical on the BioCreative V CDR task corpus. Our model is #3-ranked and within 0.6 percentage points of the state-of-the-art.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_BC5CDR.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h2 id=named-entity-recognition-on-bc5cdr-chemical--disease-corpus-with-biobert>Named Entity Recognition on BC5CDR (Chemical + Disease Corpus) with BioBERT<a hidden class=anchor aria-hidden=true href=#named-entity-recognition-on-bc5cdr-chemical--disease-corpus-with-biobert>#</a></h2><p><a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_BC5CDR.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Notebook to train/fine-tune a BioBERT model to perform named entity recognition (NER).</p><p>The <a href=https://github.com/shreyashub/BioFLAIR/tree/master/data/ner/bc5cdr>dataset</a> used is a pre-processed version of the BC5CDR (BioCreative V CDR task corpus: a resource for relation extraction) dataset from <a href=https://pubmed.ncbi.nlm.nih.gov/27161011/>Li et al. (2016)</a>.</p><p>The current state-of-the-art model on this dataset is the NER+PA+RL model from <a href=https://www.aclweb.org/anthology/D19-6125/>Nooralahzadeh et al. (2019)</a> has an F1-score of <a href=https://paperswithcode.com/sota/named-entity-recognition-ner-on-bc5cdr><strong>89.93%</strong></a>. The authors did not release the source code for the paper.</p><p>Our model trained on top of BioBERT has an F1-score of <strong>89.3%</strong> which is slightly worse than the state-of-the-art but almost as good as the #2 <a href=https://github.com/shreyashub/BioFLAIR>BioFlair</a>!</p><p>The notebook is structured as follows:</p><ul><li>Setting up the GPU Environment</li><li>Getting Data</li><li>Training and Testing the Model</li><li>Using the Model (Running Inference)</li></ul><h4 id=task-description>Task Description<a hidden class=anchor aria-hidden=true href=#task-description>#</a></h4><blockquote><p>Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens.</p></blockquote><h2 id=setting-up-the-gpu-environment>Setting up the GPU Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-gpu-environment>#</a></h2><h4 id=ensure-we-have-a-gpu-runtime>Ensure we have a GPU runtime<a hidden class=anchor aria-hidden=true href=#ensure-we-have-a-gpu-runtime>#</a></h4><p>If you&rsquo;re running this notebook in Google Colab, select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>. This will allow us to use the GPU to train the model subsequently.</p><h4 id=install-dependencies-and-restart-runtime>Install Dependencies and Restart Runtime<a hidden class=anchor aria-hidden=true href=#install-dependencies-and-restart-runtime>#</a></h4><pre tabindex=0><code>!pip install -q transformers
!pip install -q simpletransformers
</code></pre><p>You might see the error <code>ERROR: google-colab X.X.X has requirement ipykernel~=X.X, but you'll have ipykernel X.X.X which is incompatible</code> after installing the dependencies. <strong>This is normal</strong> and caused by the <code>simpletransformers</code> library.</p><p>The <strong>solution</strong> to this will be to <strong>reset the execution environment</strong> now. Go to the menu <code>Runtime</code> > <code>Restart runtime</code> then continue on from the next section to download and process the data.</p><h2 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h2><h4 id=pulling-the-data-from-github>Pulling the data from Github<a hidden class=anchor aria-hidden=true href=#pulling-the-data-from-github>#</a></h4><p>The dataset, includes train, test and dev sets, which we pull from the <a href=https://github.com/shreyashub/BioFLAIR/tree/master/data/ner/bc5cdr>Github repository</a>.</p><pre tabindex=0><code>import urllib.request
from pathlib import Path

def download_file(url, output_file):
  Path(output_file).parent.mkdir(parents=True, exist_ok=True)
  urllib.request.urlretrieve (url, output_file)

download_file(&#39;https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/train.txt&#39;, &#39;/content/data/train.txt&#39;)
download_file(&#39;https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/test.txt&#39;, &#39;/content/data/test.txt&#39;)
download_file(&#39;https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/dev.txt&#39;, &#39;/content/data/dev.txt&#39;)
</code></pre><p>Since the data is formatted in the CoNLL <code>BIO</code> type format (you can read more on the tagging format from this <a href=https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)>wikipedia article</a>), we need to format it into a <code>pandas</code> dataframe with the following function. The 3 important columns in the dataframe are a word token (for mandarin this is a single character), a <code>BIO</code> label and a sentence_id to differentiate samples/sentences.</p><pre tabindex=0><code>import pandas as pd
def read_conll(filename):
    df = pd.read_csv(filename,
                    sep = &#39;\t&#39;, header = None, keep_default_na = False,
                    names = [&#39;words&#39;, &#39;pos&#39;, &#39;chunk&#39;, &#39;labels&#39;],
                    quoting = 3, skip_blank_lines = False)
    df = df[~df[&#39;words&#39;].astype(str).str.startswith(&#39;-DOCSTART- &#39;)] # Remove the -DOCSTART- header
    df[&#39;sentence_id&#39;] = (df.words == &#39;&#39;).cumsum()
    return df[df.words != &#39;&#39;]
</code></pre><p>Now we execute the function on the train, test and dev sets we have downloaded from Github. We also <code>.head()</code> the training set dataframe for the first 100 rows to check that the words, labels and sentence_id have been split properly.</p><pre tabindex=0><code>train_df = read_conll(&#39;/content/data/train.txt&#39;)
test_df = read_conll(&#39;/content/data/test.txt&#39;)
dev_df = read_conll(&#39;/content/data/dev.txt&#39;)
train_df.head(100)
</code></pre><p>We now print out the statistics (number of sentences) of the train, dev and test sets.</p><pre tabindex=0><code>data = [[train_df[&#39;sentence_id&#39;].nunique(), test_df[&#39;sentence_id&#39;].nunique(), dev_df[&#39;sentence_id&#39;].nunique()]]

# Prints out the dataset sizes of train and test sets per label.
pd.DataFrame(data, columns=[&#34;Train&#34;, &#34;Test&#34;, &#34;Dev&#34;])
</code></pre><h2 id=training-and-testing-the-model>Training and Testing the Model<a hidden class=anchor aria-hidden=true href=#training-and-testing-the-model>#</a></h2><h4 id=set-up-the-training-arguments>Set up the Training Arguments<a hidden class=anchor aria-hidden=true href=#set-up-the-training-arguments>#</a></h4><p>We set up the training arguments. Here we train to 10 epochs to get accuracy close to the SOTA. The train, test and dev sets are relatively small so we don&rsquo;t have to wait too long. We set a sliding window as NER sequences can be quite long and because we have limited GPU memory we can&rsquo;t increase the <code>max_seq_length</code> too long.</p><pre tabindex=0><code>train_args = {
    &#39;reprocess_input_data&#39;: True,
    &#39;overwrite_output_dir&#39;: True,
    &#39;sliding_window&#39;: True,
    &#39;max_seq_length&#39;: 64,
    &#39;num_train_epochs&#39;: 10,
    &#39;train_batch_size&#39;: 32,
    &#39;fp16&#39;: True,
    &#39;output_dir&#39;: &#39;/outputs/&#39;,
    &#39;best_model_dir&#39;: &#39;/outputs/best_model/&#39;,
    &#39;evaluate_during_training&#39;: True,
}
</code></pre><p>The following line of code saves (to the variable <code>custom_labels</code>) a set of all the NER tags/labels in the dataset.</p><pre tabindex=0><code>custom_labels = list(train_df[&#39;labels&#39;].unique())
print(custom_labels)
</code></pre><h4 id=train-the-model>Train the Model<a hidden class=anchor aria-hidden=true href=#train-the-model>#</a></h4><p>Once we have setup the <code>train_args</code> dictionary, the next step would be to train the model. We use the pre-trained BioBERT model (by <a href=https://huggingface.co/dmis-lab>DMIS Lab, Korea University</a>) from the awesome <a href=https://github.com/huggingface/transformers>Hugging Face Transformers</a> library as the base and use the <a href=https://simpletransformers.ai/docs/classification-models/>Simple Transformers library</a> on top of it to make it so we can train the NER (sequence tagging) model with just a few lines of code.</p><pre tabindex=0><code>from simpletransformers.ner import NERModel
from transformers import AutoTokenizer
import pandas as pd
import logging

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger(&#39;transformers&#39;)
transformers_logger.setLevel(logging.WARNING)

# We use the bio BERT pre-trained model.
model = NERModel(&#39;bert&#39;, &#39;dmis-lab/biobert-v1.1&#39;, labels=custom_labels, args=train_args)

# Train the model
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result, model_outputs, preds_list = model.eval_model(test_df)
</code></pre><p>The F1-score for the model is <strong>89.3%</strong> (&lsquo;f1_score&rsquo;: 0.8927974947807933).</p><blockquote><p>For now thats the #3-ranked SOTA NER model on BC5CDR!</p></blockquote><h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2><p>Running the model to do some predictions/inference is as simple as calling <code>model.predict(samples)</code>. First we get a sentence from the test set and print it out. Then we run the prediction on the sentence.</p><pre tabindex=0><code>sample = test_df[test_df.sentence_id == 10].words.str.cat(sep=&#39; &#39;)
print(sample)
</code></pre><pre tabindex=0><code>samples = [sample]
predictions, _ = model.predict(samples)
for idx, sample in enumerate(samples):
  print(&#39;{}: &#39;.format(idx))
  for word in predictions[idx]:
    print(&#39;{}&#39;.format(word))
</code></pre><p>We can connect to Google Drive with the following code to save any files you want to persist. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><pre tabindex=0><code>from google.colab import drive
drive.mount(&#39;/content/drive/&#39;)
</code></pre><p>You can move the model checkpount files which are saved in the <code>/outputs/</code> directory to your Google Drive.</p><pre tabindex=0><code>import shutil
shutil.move(&#39;/outputs/&#39;, &#34;/content/drive/My Drive/outputs/&#34;)
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2><p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p><iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe><h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2><p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p><ul><li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/natural-language-processing/>Natural Language Processing</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/gpu/>GPU</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch/>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/named-entity-recognition/>Named Entity Recognition</a></li><li><a href=https://news.machinelearning.sg/tags/biology/>Biology</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook/>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab/>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on twitter" href="https://twitter.com/intent/tweet/?text=Biology%20Named%20Entity%20Recognition%20with%20BioBERT&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f&hashtags=NaturalLanguageProcessing%2cDeepLearning%2cMachineLearning%2cGPU%2cSourceCode%2cPyTorch%2cNamedEntityRecognition%2cBiology%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f&title=Biology%20Named%20Entity%20Recognition%20with%20BioBERT&summary=Biology%20Named%20Entity%20Recognition%20with%20BioBERT&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f&title=Biology%20Named%20Entity%20Recognition%20with%20BioBERT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on whatsapp" href="https://api.whatsapp.com/send?text=Biology%20Named%20Entity%20Recognition%20with%20BioBERT%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Biology Named Entity Recognition with BioBERT on telegram" href="https://telegram.me/share/url?text=Biology%20Named%20Entity%20Recognition%20with%20BioBERT&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fbiology_named_entity_recognition_with_biobert%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>