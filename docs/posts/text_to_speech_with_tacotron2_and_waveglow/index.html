<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Text to Speech with Tacotron2 and WaveGlow | News @ machinelearning.sg</title>
<meta name=keywords content="Speech,Deep Learning,Machine Learning,Source Code,PyTorch,Text-to-Speech,Jupyter Notebook,Colab">
<meta name=description content="tl;dr A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia&rsquo;s Tacotron2 and WaveGlow models and applying speech enhancement.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<meta name=author content="Eugene">
<link rel=canonical href=https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/>
<link href=https://news.machinelearning.sg/assets/css/stylesheet.min.d1ce48677bc3d2b81f6398ed4ae6ddf7262d885c01ffefc464636bbea61c0201.css integrity="sha256-0c5IZ3vD0rgfY5jtSubd9yYtiFwB/+/EZGNrvqYcAgE=" rel="preload stylesheet" as=style>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png>
<link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png>
<link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-185405110-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<meta property="og:title" content="Text to Speech with Tacotron2 and WaveGlow">
<meta property="og:description" content="tl;dr A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia&rsquo;s Tacotron2 and WaveGlow models and applying speech enhancement.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/">
<meta property="og:image" content="https://news.machinelearning.sg/tts_waveglow.jpg"><meta property="article:published_time" content="2021-05-31T18:00:00+08:00">
<meta property="article:modified_time" content="2021-05-31T18:00:00+08:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://news.machinelearning.sg/tts_waveglow.jpg">
<meta name=twitter:title content="Text to Speech with Tacotron2 and WaveGlow">
<meta name=twitter:description content="tl;dr A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia&rsquo;s Tacotron2 and WaveGlow models and applying speech enhancement.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Text to Speech with Tacotron2 and WaveGlow","name":"Text to Speech with Tacotron2 and WaveGlow","description":"tl;dr A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia\u0026amp;rsquo;s Tacotron2 and WaveGlow models and applying speech enhancement. …","keywords":["Speech","Deep Learning","Machine Learning","Source Code","PyTorch","Text-to-Speech","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia’s Tacotron2 and WaveGlow models and applying speech enhancement.\n Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. \nContinue on if you prefer reading the code here.\nText to Speech with Tacotron2 and WaveGlow Notebook to convert (synthesize) an input piece of text into a speech audio file automatically.\nText-To-Speech synthesis is the task of converting written text in natural language to speech.\nThe models used combines a pipeline of a Tacotron 2 model that produces mel spectrograms from input text using an encoder-decoder architecture and a WaveGlow flow-based model that consumes the mel spectrograms to generate speech.\nBoth steps in the pipeline will utilise pre-trained models from the PyTorch Hub by NVIDIA. Both the Tacotron 2 and WaveGlow models are trained on a publicly available LJ Speech dataset.\nDo note that the models are under a BSD 3 License.\nThe notebook is structured as follows:\n Setting up the Environment Using the Model (Running Inference) Apply Speech Enhancement/Noise Reduction  Setting up the Environment Ensure we have a GPU runtime If you’re running this notebook in Google Colab, select Runtime  Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator. This will allow us to use the GPU to train the model subsequently.\nSetup Dependencies We need to install unidecode for this example to run, so execute the command below to setup the dependencies.\n!pip install -q unidecode Using the Model (Running Inference) Now we want to load the Tacotron2 and WaveGlow models from PyTorch hub and prepare the models for inference.\nSpecifically we are running the following steps:\n torch.hub.load() - Downloads and loads the pre-trained model from torchhub. In particular, we specify to use the silero_tts model with the en (English) language speaker lj_16khz. .to(device) - We load both the models to the GPU for inferencing.  import torch tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2') tacotron2 = tacotron2.to('cuda') waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow') waveglow = waveglow.remove_weightnorm(waveglow) waveglow = waveglow.to('cuda') Using cache found in /root/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub Using cache found in /root/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub  Now we define the example_text variable, a piece of text that we want to convert to a speech audio file. Next, we synthesize/generate the audio file.\n tacotron2.text_to_sequence() - Creates a tensor representation of the input text sequence (example_text). tacotron2.infer() - Tacotron2 generates mel spectrogram given tensor representation from the previous step (sequence). waveglow.infer() - Waveglow generates sound given the mel spectrogram display() - The notebook will then display a playback widget of the audio sample, audio_numpy.  from IPython.display import Audio, display import numpy as np example_text = 'What is umbrage? According to the Oxford Languages dictionary, Umbrage is a noun that means offence or annoyance.' # preprocessing sequence = np.array(tacotron2.text_to_sequence(example_text, ['english_cleaners']))[None, :] sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64) # run the models with torch.no_grad(): _, mel, _, _ = tacotron2.infer(sequence) audio = waveglow.infer(mel) audio_numpy = audio[0].data.cpu().numpy() rate = 22050 display(Audio(audio_numpy, rate=rate)) We notice that there is some slight noise in the generated sample which can easily be reduced to enhance the quality of speech using a speech enhancement model. We try this in the next section. This is entirely optional.\nApply Speech Enhancement/Noise Reduction We use the simple and convenient LogMMSE algorithm (Log Minimum Mean Square Error) with the logmmse library.\n!pip install -q logmmse Run the LogMMSE algorithm on the generated audio audio[0] and display the enhanced audio sample produced in an audio player.\nimport numpy as np from logmmse import logmmse enhanced = logmmse(audio_numpy, rate, output_file=None, initial_noise=1, window_size=160, noise_threshold=0.15) display(Audio(enhanced, rate=rate)) Save the enhanced audio to file.\nfrom scipy.io.wavfile import write write('/content/audio.wav', rate, enhanced) We can connect to Google Drive with the following code. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the output files which are saved in the /content/ directory to the root of your Google Drive.\nimport shutil shutil.move('/content/audio.wav', '/content/drive/My Drive/audio.wav') More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\n Alternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\n Google Colab vs Paperspace Gradient  ","wordCount":"763","inLanguage":"en","image":"https://news.machinelearning.sg/tts_waveglow.jpg","datePublished":"2021-05-31T18:00:00+08:00","dateModified":"2021-05-31T18:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script>
<script async data-id=defa0fb0-3f27-4afe-b567-4f57d47c86e9 src=https://tinyads.io/e></script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>.theme-toggle,.top-link{display:none}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://news.machinelearning.sg/ accesskey=h>News
</a>
<span class=logo-switches>
<span class=theme-toggle>
<a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a>
</span>
<span class=lang-switch>
<ul>
<li>
<a href=https://machinelearning.sg>
machinelearning.sg
</a>
</li>
</ul>
</span>
</span>
</div>
<ul class=menu id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://news.machinelearning.sg/about/>
<span>
About
</span>
</a>
</li>
<li>
<a href=https://news.machinelearning.sg/archives/>
<span>
Archive
</span>
</a>
</li>
<li>
<a href=https://news.machinelearning.sg/tags/>
<span>
Tags
</span>
</a>
</li></ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Text to Speech with Tacotron2 and WaveGlow
</h1>
<div class=post-meta>
May 31, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Eugene
</div>
</header>
<figure class=entry-cover>
<img srcset="https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/tts_waveglow_hu5347d6e177d31909c3fd4399b690bcdf_9127_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/tts_waveglow_hu5347d6e177d31909c3fd4399b690bcdf_9127_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/tts_waveglow.jpg 640w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/text_to_speech_with_tacotron2_and_waveglow/tts_waveglow.jpg alt="Text to Speech with Tacotron2 and WaveGlow. Image from Unsplash by Hrayr Movsisyan.">
</figure>
<div class=toc>
<details>
<summary>
<div class=details accesskey=c>Table of Contents</div>
</summary>
<blockquote><ul><ul><li>
<a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li></ul>
<li>
<a href=#text-to-speech-with-tacotron2-and-waveglow aria-label="Text to Speech with Tacotron2 and WaveGlow">Text to Speech with Tacotron2 and WaveGlow</a></li><li>
<a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul>
<ul>
<ul>
<li>
<a href=#ensure-we-have-a-gpu-runtime aria-label="Ensure we have a GPU runtime">Ensure we have a GPU runtime</a></li><li>
<a href=#setup-dependencies aria-label="Setup Dependencies">Setup Dependencies</a></li></ul>
</ul>
</ul>
</li><li>
<a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li><li>
<a href=#apply-speech-enhancementnoise-reduction aria-label="Apply Speech Enhancement/Noise Reduction">Apply Speech Enhancement/Noise Reduction</a><ul>
<li>
<a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li>
<a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul>
</li></ul>
</blockquote>
</details>
</div>
<div class=post-content>
<blockquote>
<p><strong>tl;dr</strong> A step-by-step tutorial to generate spoken audio from text automatically using a pipeline of Nvidia&rsquo;s Tacotron2 and WaveGlow models and applying speech enhancement.</p>
</blockquote>
<h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2>
<p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p>
<p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Text_to_Speech_with_Tacotron2_and_WaveGlow.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p>
<p>Continue on if you prefer reading the code here.</p>
<h1 id=text-to-speech-with-tacotron2-and-waveglow>Text to Speech with Tacotron2 and WaveGlow<a hidden class=anchor aria-hidden=true href=#text-to-speech-with-tacotron2-and-waveglow>#</a></h1>
<p>Notebook to convert (synthesize) an input piece of text into a speech audio file automatically.</p>
<p><a href=https://paperswithcode.com/task/text-to-speech-synthesis>Text-To-Speech synthesis</a> is the task of converting written text in natural language to speech.</p>
<p>The models used combines a pipeline of a <a href=https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/>Tacotron 2</a> model that produces mel spectrograms from input text using an encoder-decoder architecture and a <a href=https://pytorch.org/hub/nvidia_deeplearningexamples_waveglow/>WaveGlow</a> flow-based model that consumes the mel spectrograms to generate speech.</p>
<p>Both steps in the pipeline will utilise pre-trained models from the PyTorch Hub by NVIDIA. Both the Tacotron 2 and WaveGlow models are trained on a publicly available <a href=https://keithito.com/LJ-Speech-Dataset/>LJ Speech</a> dataset.</p>
<p>Do note that the models are under a <a href=https://opensource.org/licenses/BSD-3-Clause>BSD 3 License</a>.</p>
<p>The notebook is structured as follows:</p>
<ul>
<li>Setting up the Environment</li>
<li>Using the Model (Running Inference)</li>
<li>Apply Speech Enhancement/Noise Reduction</li>
</ul>
<h1 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h1>
<h4 id=ensure-we-have-a-gpu-runtime>Ensure we have a GPU runtime<a hidden class=anchor aria-hidden=true href=#ensure-we-have-a-gpu-runtime>#</a></h4>
<p>If you&rsquo;re running this notebook in Google Colab, select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>. This will allow us to use the GPU to train the model subsequently.</p>
<h4 id=setup-dependencies>Setup Dependencies<a hidden class=anchor aria-hidden=true href=#setup-dependencies>#</a></h4>
<p>We need to install <code>unidecode</code> for this example to run, so execute the command below to setup the dependencies.</p>
<pre tabindex=0><code>!pip install -q unidecode
</code></pre><h1 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h1>
<p>Now we want to load the Tacotron2 and WaveGlow models from PyTorch hub and prepare the models for inference.</p>
<p>Specifically we are running the following steps:</p>
<ul>
<li><code>torch.hub.load()</code> - Downloads and loads the pre-trained model from torchhub. In particular, we specify to use the <code>silero_tts</code> model with the <code>en</code> (English) language speaker <code>lj_16khz</code>.</li>
<li><code>.to(device)</code> - We load both the models to the <code>GPU</code> for inferencing.</li>
</ul>
<pre tabindex=0><code>import torch

tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')
tacotron2 = tacotron2.to('cuda')

waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')
waveglow = waveglow.remove_weightnorm(waveglow)
waveglow = waveglow.to('cuda')
</code></pre><pre><code>Using cache found in /root/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub
Using cache found in /root/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub
</code></pre>
<p>Now we define the <code>example_text</code> variable, a piece of text that we want to convert to a speech audio file. Next, we synthesize/generate the audio file.</p>
<ul>
<li><code>tacotron2.text_to_sequence()</code> - Creates a tensor representation of the input text sequence (<code>example_text</code>).</li>
<li><code>tacotron2.infer()</code> - Tacotron2 generates mel spectrogram given tensor representation from the previous step (<code>sequence</code>).</li>
<li><code>waveglow.infer()</code> - Waveglow generates sound given the mel spectrogram</li>
<li><code>display()</code> - The notebook will then display a playback widget of the audio sample, <code>audio_numpy</code>.</li>
</ul>
<pre tabindex=0><code>from IPython.display import Audio, display
import numpy as np

example_text = 'What is umbrage? According to the Oxford Languages dictionary, Umbrage is a noun that means offence or annoyance.'

# preprocessing
sequence = np.array(tacotron2.text_to_sequence(example_text, ['english_cleaners']))[None, :]
sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)

# run the models
with torch.no_grad():
    _, mel, _, _ = tacotron2.infer(sequence)
    audio = waveglow.infer(mel)
audio_numpy = audio[0].data.cpu().numpy()
rate = 22050

display(Audio(audio_numpy, rate=rate))
</code></pre><p>We notice that there is some slight noise in the generated sample which can easily be reduced to enhance the quality of speech using a speech enhancement model. We try this in the next section. This is entirely optional.</p>
<h1 id=apply-speech-enhancementnoise-reduction>Apply Speech Enhancement/Noise Reduction<a hidden class=anchor aria-hidden=true href=#apply-speech-enhancementnoise-reduction>#</a></h1>
<p>We use the simple and convenient LogMMSE algorithm (Log Minimum Mean Square Error) with the <a href=https://github.com/wilsonchingg/logmmse>logmmse library</a>.</p>
<pre tabindex=0><code>!pip install -q logmmse
</code></pre><p>Run the LogMMSE algorithm on the generated audio <code>audio[0]</code> and display the enhanced audio sample produced in an audio player.</p>
<pre tabindex=0><code>import numpy as np
from logmmse import logmmse

enhanced = logmmse(audio_numpy, rate, output_file=None, initial_noise=1, window_size=160, noise_threshold=0.15)
display(Audio(enhanced, rate=rate))
</code></pre><p>Save the enhanced audio to file.</p>
<pre tabindex=0><code>from scipy.io.wavfile import write

write('/content/audio.wav', rate, enhanced)
</code></pre><p>We can connect to Google Drive with the following code. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p>
<p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p>
<pre tabindex=0><code>from google.colab import drive
drive.mount('/content/drive/')
</code></pre><p>You can move the output files which are saved in the <code>/content/</code> directory to the root of your Google Drive.</p>
<pre tabindex=0><code>import shutil
shutil.move('/content/audio.wav', '/content/drive/My Drive/audio.wav')
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2>
<p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p>
<iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe>
<h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2>
<p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p>
<ul>
<li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://news.machinelearning.sg/tags/speech>Speech</a></li>
<li><a href=https://news.machinelearning.sg/tags/deep-learning>Deep Learning</a></li>
<li><a href=https://news.machinelearning.sg/tags/machine-learning>Machine Learning</a></li>
<li><a href=https://news.machinelearning.sg/tags/source-code>Source Code</a></li>
<li><a href=https://news.machinelearning.sg/tags/pytorch>PyTorch</a></li>
<li><a href=https://news.machinelearning.sg/tags/text-to-speech>Text-to-Speech</a></li>
<li><a href=https://news.machinelearning.sg/tags/jupyter-notebook>Jupyter Notebook</a></li>
<li><a href=https://news.machinelearning.sg/tags/colab>Colab</a></li>
</ul>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on twitter" href="https://twitter.com/intent/tweet/?text=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f&hashtags=Speech%2cDeepLearning%2cMachineLearning%2cSourceCode%2cPyTorch%2cText-to-Speech%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f&title=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow&summary=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f&title=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on whatsapp" href="https://api.whatsapp.com/send?text=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Text to Speech with Tacotron2 and WaveGlow on telegram" href="https://telegram.me/share/url?text=Text%20to%20Speech%20with%20Tacotron2%20and%20WaveGlow&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftext_to_speech_with_tacotron2_and_waveglow%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main><footer class=footer>
<span>&copy; 2021 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top" accesskey=g>
<button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=https://news.machinelearning.sg/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(a){a.preventDefault();var b=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(b)}']`).scrollIntoView({behavior:"smooth"})})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>