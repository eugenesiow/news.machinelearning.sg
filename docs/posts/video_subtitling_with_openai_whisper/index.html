<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Video Subtitling with OpenAI Whisper | News @ machinelearning.sg</title><meta name=keywords content="Speech,Deep Learning,Machine Learning,GPU,Source Code,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Video Subtitling with OpenAI Whisper"><meta property="og:description" content="tl;dr A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/"><meta property="og:image" content="https://news.machinelearning.sg/splash/Video_Subtitling_with_OpenAI_Whisper_9.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-16T17:22:05+08:00"><meta property="article:modified_time" content="2022-12-16T17:22:05+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/splash/Video_Subtitling_with_OpenAI_Whisper_9.png"><meta name=twitter:title content="Video Subtitling with OpenAI Whisper"><meta name=twitter:description content="tl;dr A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"Video Subtitling with OpenAI Whisper","item":"https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Video Subtitling with OpenAI Whisper","name":"Video Subtitling with OpenAI Whisper","description":"tl;dr A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.","keywords":["Speech","Deep Learning","Machine Learning","GPU","Source Code","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.\nVideo Subtitling with OpenAI Whisper Notebook to automatically generate subtitles from a video using OpenAI’s Whisper Speech-to-Text Model.\nThe notebook consists of a few steps to generate the subtitles. First the audio of the video needs to be extracted and segmented. Then Speech Recognition, the task of recognising speech within audio and converting it into text, is performed on the segments. The text output is used to generate a subtitles file which is then rendered on the video.\nThe notebook is structured as follows:\nSetting up the Environment Getting Data Using the Models (Running Inference) Setting up the Environment Setting up the Dependencies First we pip install the required dependencies. This downloads and installs the python libraries required from the PyPi repository online.\n!pip install -q transformers==4.25.1 soundfile==0.11.0 auditok==0.2.0 pytube==12.1.0 \u001b[K |████████████████████████████████| 5.8 MB 4.8 MB/s \u001b[K |████████████████████████████████| 1.5 MB 50.3 MB/s \u001b[K |████████████████████████████████| 56 kB 762 kB/s \u001b[K |████████████████████████████████| 7.6 MB 46.3 MB/s \u001b[K |████████████████████████████████| 182 kB 19.8 MB/s \u001b[?25h\rGetting Data Getting a Video from Youtube Video of Jensen Huang, CEO of Nvidia, talking about new technologies like conversational AI at the GTC 2021 Fall Keynote. ## Youtube video URL video_url = 'https://www.youtube.com/watch?v=oY4RjiKHACo' We’ll download this video to our local notebook storage using pytube. We are downloading a progressive version, a version with both audio and video.\nfrom pytube import YouTube ## Choose a video stream with resolution of 360p ## streams = YouTube(video_url).streams.filter(adaptive=True, subtype=\"mp4\", resolution=\"360p\") streams = YouTube(video_url).streams.filter(progressive=True, subtype=\"mp4\", resolution=\"360p\") ## Check if there is a valid stream if len(streams) == 0: raise \"No suitable stream found for this YouTube video!\" ## Download the video as video.mp4 print(\"Downloading...\") streams[0].download(filename=\"video.mp4\") print(\"Download completed.\") Downloading...\rDownload completed.\rUsing the Model (Running Inference) First we extract the audio track from the video and save it as a .wav file.\nWe use the ffmpeg application to do this extraction. On colab, ffmpeg is already pre-installed in the environment. So we just use Python’s subprocess to call ffmpeg to extract the audio.\nWe then display an audio player in the notebook to listen to the extracted audio.\nimport subprocess from pathlib import Path from IPython.display import Audio, display audio_directory = './temp_audio/' def extract_audio(input_file): Path(audio_directory).mkdir(parents=True, exist_ok=True) audio_file = audio_directory+'/temp.wav' command = [\"ffmpeg\", \"-i\", input_file, \"-ac\", \"1\", \"-ar\", \"16000\",\"-vn\", \"-f\", \"wav\", audio_file] subprocess.run(command) extract_audio('video.mp4') display(Audio(audio_directory+'/temp.wav')) Next we use the auditok library to segement the audio based on the silences in the video. This is useful for subtitling as we then have splits by the segments for each subtitle caption to be shown. We also ensure that the max duration of each audio segment is not too long (less than 8s), so that the subtitle for each segment is readable.\nWe display the first segement as an audio player in the notebook to listen.\nimport auditok def segment_audio(audio_name): audio_regions = auditok.split(audio_name, min_dur=1, # minimum duration of a valid audio in seconds max_dur=8, # maximum duration of an audio segment max_silence=0.8, # maximum duration of tolerated continuous silence within an event energy_threshold=55, # threshold of detection sampling_rate=16000 ) for i, r in enumerate(audio_regions): filename = r.save(audio_name[:-4]+f'_{r.meta.start:08.3f}-{r.meta.end:08.3f}.wav') segment_audio(audio_directory+'/temp.wav') segments = [f for f in Path(audio_directory).glob(f'temp_*.wav')] display(Audio(str(segments[0]))) Now we download and setup the model weights of the pre-trained model from the huggingface hub using the transformers library.\nWe download and load the OpenAI Whisper Base model using the convenience pipeline function in the library.\nfrom transformers import pipeline pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\") Downloading: 0%| | 0.00/1.96k [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/290M [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/827 [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/1.04M [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/494k [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/52.7k [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/2.11k [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/2.06k [00:00\u003c?, ?B/s]\rDownloading: 0%| | 0.00/185k [00:00\u003c?, ?B/s]\rNext we define a few functions to run the inference and create our .srt subtitle file.\nclean_text - This is just a simple post-processing function to clean the output text from the ASR model with some simple regex. get_srt_line - This function helps return a .srt format set of lines from an inferred text segment. This will include the start and ending time so the video player knows when to start and end the showing of the subtitle. get_subs - Our main function. This function will run each of the audio segment through the tokenizer and ASR model and save the inferred text output as a line in the subtitle file. The output_file is the .srt file of the transcribed audio. During the process, we also print out the inferred text for each segment. Here you can see it is pretty accurate. We are running on CPU on Colab so its not very fast. You can run on GPU if you would like to transcribe a large amount of speech at once.\nimport soundfile import re import torch import datetime def clean_text(text): clean_text = re.sub(r' ', ' ', text) clean_text = re.sub(r'\\bi\\s', 'I ', clean_text) clean_text = re.sub(r'\\si$', ' I', clean_text) clean_text = re.sub(r'i\\'', 'I\\'', clean_text) return clean_text def get_srt_line(inferred_text, line_count, limits): sep = ',' d = str(datetime.timedelta(seconds=float(limits[0]))) try: from_dur = '0' + str(d.split(\".\")[0]) + sep + str(d.split(\".\")[-1][:2]) except: from_dur = '0' + str(d) + sep + '00' d = str(datetime.timedelta(seconds=float(limits[1]))) try: to_dur = '0' + str(d.split(\".\")[0]) + sep + str(d.split(\".\")[-1][:2]) except: to_dur = '0' + str(d) + sep + '00' return f'{str(line_count)}\\n{from_dur} --\u003e {to_dur}\\n{inferred_text}\\n\\n' def get_subs(audio_directory, output_file): segments = sorted([f for f in Path(audio_directory).glob(f'temp_*.wav')]) line_count = 0 with open(output_file, 'w', encoding=\"utf-8\") as out_file: for audio_file in segments: # Run OpenAI Whisper inference on each segemented audio file. speech, rate = soundfile.read(audio_file) output_json = pipe(speech) inferred_text = output_json['text'] if len(inferred_text) \u003e 0: inferred_text = clean_text(inferred_text) print(inferred_text) else: inferred_text = '' limits = audio_file.name[:-4].split(\"_\")[-1].split(\"-\") limits = [float(limit) for limit in limits] out_file.write(get_srt_line(inferred_text, line_count, limits)) out_file.flush() line_count += 1 get_subs(audio_directory, './video.srt') avatars.\rRobotics and self-driving cars. We have a Jam Pack GTC.\rbefore we jump into data centers.\rI want to show you something we've been building. A conversational AI.\rToy me.\rYou're going to see speech understanding, natural language processing, on the largest model ever trained.\rspeech synthesis with my own voice, character animation, and beautiful ray trace graphics.\rall in real time. ToiMe was made with some amazing technologies that have become possible only recently.\rand barely so.\rI asked a few friends to ask this cute little guy some tough questions.\rLet's take a look.\rHi, hello, hello, Toijenson.\rHi, please ask me anything. One of the greatest threats of climate change.\rThe greatest threats of climate change are the effects on human health, food security, water supplies, and the environment.\rHow do astronomers look for exoplanets?\rMost exoplanets are found via the transit method. That was the case for the trappist one planets.\rTell me more about that method.\rThe transit method is when a planet passes in front of its star. The amount of light block tells astronomers the size of the planet.\rit tells astronomers the orbital period of the planet.\rWhat telescopes use that method?\rThe Kepler Space Telescope used a transit method to find thousands of exoplanets.\rI've heard about a satellite called Tess that is used to find planets. How does it work?\rTens is a space telescope that looks for transits.\rWhat rules do proteins play in ourselves?\rProteins are the building blocks of the body. They are the basic materials from which all living things are made. They are needed for the structure, function, and regular\rof the body's tissues and organs.\rHow do they work?\rProteins are made up of amino acids. Amino acids are the basic building blocks of proteins. There are 20 different amino acids that link together\rand different combinations to form different proteins.\rHow do they catalyze chemical reactions?\rProteins are biological catalysts. Cattleists are substances that speed up the rate of a chemical reaction without being consumed by the reaction.\rOur proteins that catalyzed increase the rate of biochemical reactions in the body. Thank you. Goodbye.\rsure anytime.\ryou\rNext we want to use the ffmpeg application again to render the subtitle text on the video itself so we can preview it. We use the subprocess library again to do so.\nfrom base64 import b64encode def combine_subtitles(input_file, subtitle_file, output_file): command = [\"ffmpeg\", \"-i\", input_file, \"-vf\", f\"subtitles={subtitle_file}\", output_file] subprocess.run(command) combine_subtitles('./video.mp4', './video.srt', './video_subbed.mp4') Now we want to display our subtitled video, because Google Colab has some problem using the IPython Video component, we have to use the workaround below which will load the video (as base64 encoded text) into memory and display it in a HTML5 video component in the browser. The subtitled text is at the bottom, you might need to hide the controls (move mouse away) to see the subtitles.\nfrom IPython.display import HTML from base64 import b64encode mp4 = open('./video_subbed.mp4','rb').read() data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode() HTML(f'","wordCount":"1662","inLanguage":"en","image":"https://news.machinelearning.sg/splash/Video_Subtitling_with_OpenAI_Whisper_9.png","datePublished":"2022-12-16T17:22:05+08:00","dateModified":"2022-12-16T17:22:05+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Video Subtitling with OpenAI Whisper</h1><div class=post-meta><span title='2022-12-16 17:22:05 +0800 +08'>December 16, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9_huc0c6ac2858e2e1cf1f7bf09f65a43095_726504_360x0_resize_box_3.png 360w ,https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9_huc0c6ac2858e2e1cf1f7bf09f65a43095_726504_480x0_resize_box_3.png 480w ,https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9_huc0c6ac2858e2e1cf1f7bf09f65a43095_726504_720x0_resize_box_3.png 720w ,https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9_huc0c6ac2858e2e1cf1f7bf09f65a43095_726504_1080x0_resize_box_3.png 1080w ,https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9.png 1200w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/video_subtitling_with_openai_whisper/splash/Video_Subtitling_with_OpenAI_Whisper_9.png alt="Video Subtitling with OpenAI Whisper" width=1200 height=630></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li><a href=#video-subtitling-with-openai-whisper aria-label="Video Subtitling with OpenAI Whisper">Video Subtitling with OpenAI Whisper</a></li><li><a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul><ul><li><a href=#setting-up-the-dependencies aria-label="Setting up the Dependencies">Setting up the Dependencies</a></li></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><ul><li><a href=#getting-a-video-from-youtube aria-label="Getting a Video from Youtube">Getting a Video from Youtube</a></li></ul></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong>
A step-by-step tutorial to automatically generate subtitles from a video using audio segmentation and OpenAI Whisper.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Video_Subtitling_with_OpenAI_Whisper.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h2 id=video-subtitling-with-openai-whisper>Video Subtitling with OpenAI Whisper<a hidden class=anchor aria-hidden=true href=#video-subtitling-with-openai-whisper>#</a></h2><p><a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Video_Subtitling_with_OpenAI_Whisper.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Notebook to automatically generate subtitles from a video using OpenAI&rsquo;s Whisper Speech-to-Text Model.</p><p>The notebook consists of a few steps to generate the subtitles. First the audio of the video needs to be extracted and segmented. Then <a href=https://paperswithcode.com/task/speech-recognition>Speech Recognition</a>, the task of recognising speech within audio and converting it into text, is performed on the segments. The text output is used to generate a subtitles file which is then rendered on the video.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the Environment</li><li>Getting Data</li><li>Using the Models (Running Inference)</li></ul><h2 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h2><h4 id=setting-up-the-dependencies>Setting up the Dependencies<a hidden class=anchor aria-hidden=true href=#setting-up-the-dependencies>#</a></h4><p>First we <code>pip install</code> the required dependencies. This downloads and installs the python libraries required from the PyPi repository online.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>pip install <span style=color:#f92672>-</span>q transformers<span style=color:#f92672>==</span><span style=color:#ae81ff>4.25.1</span> soundfile<span style=color:#f92672>==</span><span style=color:#ae81ff>0.11.0</span> auditok<span style=color:#f92672>==</span><span style=color:#ae81ff>0.2.0</span> pytube<span style=color:#f92672>==</span><span style=color:#ae81ff>12.1.0</span>
</span></span></code></pre></div><pre><code>[K     |████████████████████████████████| 5.8 MB 4.8 MB/s 
[K     |████████████████████████████████| 1.5 MB 50.3 MB/s 
[K     |████████████████████████████████| 56 kB 762 kB/s 
[K     |████████████████████████████████| 7.6 MB 46.3 MB/s 
[K     |████████████████████████████████| 182 kB 19.8 MB/s 
[?25h
</code></pre><h2 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h2><h4 id=getting-a-video-from-youtube>Getting a Video from Youtube<a hidden class=anchor aria-hidden=true href=#getting-a-video-from-youtube>#</a></h4><ul><li>Video of Jensen Huang, CEO of Nvidia, talking about new technologies like conversational AI at the GTC 2021 Fall Keynote.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## Youtube video URL</span>
</span></span><span style=display:flex><span>video_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://www.youtube.com/watch?v=oY4RjiKHACo&#39;</span>
</span></span></code></pre></div><p>We&rsquo;ll download this video to our local notebook storage using <code>pytube</code>. We are downloading a <code>progressive</code> version, a version with both audio and video.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytube <span style=color:#f92672>import</span> YouTube
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Choose a video stream with resolution of 360p</span>
</span></span><span style=display:flex><span><span style=color:#75715e>## streams = YouTube(video_url).streams.filter(adaptive=True, subtype=&#34;mp4&#34;, resolution=&#34;360p&#34;)</span>
</span></span><span style=display:flex><span>streams <span style=color:#f92672>=</span> YouTube(video_url)<span style=color:#f92672>.</span>streams<span style=color:#f92672>.</span>filter(progressive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, subtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;mp4&#34;</span>, resolution<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;360p&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Check if there is a valid stream</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> len(streams) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>raise</span> <span style=color:#e6db74>&#34;No suitable stream found for this YouTube video!&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Download the video as video.mp4</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Downloading...&#34;</span>)
</span></span><span style=display:flex><span>streams[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>download(filename<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;video.mp4&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Download completed.&#34;</span>)
</span></span></code></pre></div><pre><code>Downloading...
Download completed.
</code></pre><h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2><p>First we extract the audio track from the video and save it as a <code>.wav</code> file.</p><p>We use the <code>ffmpeg</code> application to do this extraction. On colab, <code>ffmpeg</code> is already pre-installed in the environment. So we just use Python&rsquo;s <code>subprocess</code> to call ffmpeg to extract the audio.</p><p>We then <code>display</code> an audio player in the notebook to listen to the extracted audio.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> subprocess
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> IPython.display <span style=color:#f92672>import</span> Audio, display
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>audio_directory <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./temp_audio/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_audio</span>(input_file):
</span></span><span style=display:flex><span>  Path(audio_directory)<span style=color:#f92672>.</span>mkdir(parents<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  audio_file <span style=color:#f92672>=</span> audio_directory<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/temp.wav&#39;</span>
</span></span><span style=display:flex><span>  command <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;ffmpeg&#34;</span>, <span style=color:#e6db74>&#34;-i&#34;</span>, input_file, <span style=color:#e6db74>&#34;-ac&#34;</span>, <span style=color:#e6db74>&#34;1&#34;</span>, <span style=color:#e6db74>&#34;-ar&#34;</span>, <span style=color:#e6db74>&#34;16000&#34;</span>,<span style=color:#e6db74>&#34;-vn&#34;</span>, <span style=color:#e6db74>&#34;-f&#34;</span>, <span style=color:#e6db74>&#34;wav&#34;</span>, audio_file]
</span></span><span style=display:flex><span>  subprocess<span style=color:#f92672>.</span>run(command)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>extract_audio(<span style=color:#e6db74>&#39;video.mp4&#39;</span>)
</span></span><span style=display:flex><span>display(Audio(audio_directory<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/temp.wav&#39;</span>))
</span></span></code></pre></div><p>Next we use the <code>auditok</code> library to segement the audio based on the silences in the video. This is useful for subtitling as we then have splits by the segments for each subtitle caption to be shown. We also ensure that the max duration of each audio segment is not too long (less than 8s), so that the subtitle for each segment is readable.</p><p>We <code>display</code> the first segement as an audio player in the notebook to listen.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> auditok
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>segment_audio</span>(audio_name):
</span></span><span style=display:flex><span>  audio_regions <span style=color:#f92672>=</span> auditok<span style=color:#f92672>.</span>split(audio_name,
</span></span><span style=display:flex><span>    min_dur<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,       <span style=color:#75715e># minimum duration of a valid audio in seconds</span>
</span></span><span style=display:flex><span>    max_dur<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,       <span style=color:#75715e># maximum duration of an audio segment</span>
</span></span><span style=display:flex><span>    max_silence<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, <span style=color:#75715e># maximum duration of tolerated continuous silence within an event</span>
</span></span><span style=display:flex><span>    energy_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>55</span>, <span style=color:#75715e># threshold of detection</span>
</span></span><span style=display:flex><span>    sampling_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>16000</span>
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> i, r <span style=color:#f92672>in</span> enumerate(audio_regions):
</span></span><span style=display:flex><span>    filename <span style=color:#f92672>=</span> r<span style=color:#f92672>.</span>save(audio_name[:<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>]<span style=color:#f92672>+</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;_</span><span style=color:#e6db74>{</span>r<span style=color:#f92672>.</span>meta<span style=color:#f92672>.</span>start<span style=color:#e6db74>:</span><span style=color:#e6db74>08.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>-</span><span style=color:#e6db74>{</span>r<span style=color:#f92672>.</span>meta<span style=color:#f92672>.</span>end<span style=color:#e6db74>:</span><span style=color:#e6db74>08.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>.wav&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>segment_audio(audio_directory<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/temp.wav&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>segments <span style=color:#f92672>=</span> [f <span style=color:#66d9ef>for</span> f <span style=color:#f92672>in</span> Path(audio_directory)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;temp_*.wav&#39;</span>)]
</span></span><span style=display:flex><span>display(Audio(str(segments[<span style=color:#ae81ff>0</span>])))
</span></span></code></pre></div><p>Now we download and setup the model weights of the pre-trained model from the huggingface hub using the <code>transformers</code> library.</p><p>We download and load the <a href=https://huggingface.co/openai/whisper-base>OpenAI Whisper Base</a> model using the convenience pipeline function in the library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#34;automatic-speech-recognition&#34;</span>, model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;openai/whisper-base&#34;</span>)
</span></span></code></pre></div><pre><code>Downloading:   0%|          | 0.00/1.96k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/290M [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/827 [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/1.04M [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/494k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/52.7k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/2.11k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/2.06k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/185k [00:00&lt;?, ?B/s]
</code></pre><p>Next we define a few functions to run the inference and create our <code>.srt</code> subtitle file.</p><ul><li><code>clean_text</code> - This is just a simple post-processing function to clean the output text from the ASR model with some simple regex.</li><li><code>get_srt_line</code> - This function helps return a <code>.srt</code> format set of lines from an inferred text segment. This will include the start and ending time so the video player knows when to start and end the showing of the subtitle.</li><li><code>get_subs</code> - Our main function. This function will run each of the audio segment through the tokenizer and ASR model and save the inferred text output as a line in the subtitle file. The <code>output_file</code> is the <code>.srt</code> file of the transcribed audio. During the process, we also print out the inferred text for each segment. Here you can see it is pretty accurate.</li></ul><p>We are running on CPU on Colab so its not very fast. You can run on GPU if you would like to transcribe a large amount of speech at once.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> soundfile
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>clean_text</span>(text):
</span></span><span style=display:flex><span>  clean_text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;  &#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>, text)
</span></span><span style=display:flex><span>  clean_text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\bi\s&#39;</span>, <span style=color:#e6db74>&#39;I &#39;</span>, clean_text)
</span></span><span style=display:flex><span>  clean_text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\si$&#39;</span>, <span style=color:#e6db74>&#39; I&#39;</span>, clean_text)
</span></span><span style=display:flex><span>  clean_text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;i</span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>&#39;</span>, <span style=color:#e6db74>&#39;I</span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>&#39;</span>, clean_text)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> clean_text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_srt_line</span>(inferred_text, line_count, limits):
</span></span><span style=display:flex><span>  sep <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;,&#39;</span>   
</span></span><span style=display:flex><span>  d <span style=color:#f92672>=</span> str(datetime<span style=color:#f92672>.</span>timedelta(seconds<span style=color:#f92672>=</span>float(limits[<span style=color:#ae81ff>0</span>])))
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>      from_dur <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;0&#39;</span> <span style=color:#f92672>+</span> str(d<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;.&#34;</span>)[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>+</span> sep <span style=color:#f92672>+</span> str(d<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;.&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][:<span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>      from_dur <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;0&#39;</span> <span style=color:#f92672>+</span> str(d) <span style=color:#f92672>+</span> sep <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;00&#39;</span>
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>  d <span style=color:#f92672>=</span> str(datetime<span style=color:#f92672>.</span>timedelta(seconds<span style=color:#f92672>=</span>float(limits[<span style=color:#ae81ff>1</span>])))
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>      to_dur <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;0&#39;</span> <span style=color:#f92672>+</span> str(d<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;.&#34;</span>)[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>+</span> sep <span style=color:#f92672>+</span> str(d<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;.&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][:<span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>      to_dur <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;0&#39;</span> <span style=color:#f92672>+</span> str(d) <span style=color:#f92672>+</span> sep <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;00&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>str(line_count)<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>from_dur<span style=color:#e6db74>}</span><span style=color:#e6db74> --&gt; </span><span style=color:#e6db74>{</span>to_dur<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>inferred_text<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_subs</span>(audio_directory, output_file):
</span></span><span style=display:flex><span>  segments <span style=color:#f92672>=</span> sorted([f <span style=color:#66d9ef>for</span> f <span style=color:#f92672>in</span> Path(audio_directory)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;temp_*.wav&#39;</span>)])
</span></span><span style=display:flex><span>  line_count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> open(output_file, <span style=color:#e6db74>&#39;w&#39;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> out_file:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> audio_file <span style=color:#f92672>in</span> segments:
</span></span><span style=display:flex><span>      <span style=color:#75715e># Run OpenAI Whisper inference on each segemented audio file.</span>
</span></span><span style=display:flex><span>      speech, rate <span style=color:#f92672>=</span> soundfile<span style=color:#f92672>.</span>read(audio_file) 
</span></span><span style=display:flex><span>      output_json <span style=color:#f92672>=</span> pipe(speech)
</span></span><span style=display:flex><span>      inferred_text <span style=color:#f92672>=</span> output_json[<span style=color:#e6db74>&#39;text&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> len(inferred_text) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        inferred_text <span style=color:#f92672>=</span> clean_text(inferred_text)
</span></span><span style=display:flex><span>        print(inferred_text)
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        inferred_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      limits <span style=color:#f92672>=</span> audio_file<span style=color:#f92672>.</span>name[:<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;_&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;-&#34;</span>)
</span></span><span style=display:flex><span>      limits <span style=color:#f92672>=</span> [float(limit) <span style=color:#66d9ef>for</span> limit <span style=color:#f92672>in</span> limits]
</span></span><span style=display:flex><span>      out_file<span style=color:#f92672>.</span>write(get_srt_line(inferred_text, line_count, limits))
</span></span><span style=display:flex><span>      out_file<span style=color:#f92672>.</span>flush()
</span></span><span style=display:flex><span>      line_count <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>get_subs(audio_directory, <span style=color:#e6db74>&#39;./video.srt&#39;</span>)
</span></span></code></pre></div><pre><code> avatars.
 Robotics and self-driving cars. We have a Jam Pack GTC.
 before we jump into data centers.
 I want to show you something we've been building. A conversational AI.
 Toy me.
 You're going to see speech understanding, natural language processing, on the largest model ever trained.
 speech synthesis with my own voice, character animation, and beautiful ray trace graphics.
 all in real time. ToiMe was made with some amazing technologies that have become possible only recently.
 and barely so.
 I asked a few friends to ask this cute little guy some tough questions.
 Let's take a look.
 Hi, hello, hello, Toijenson.
 Hi, please ask me anything. One of the greatest threats of climate change.
 The greatest threats of climate change are the effects on human health, food security, water supplies, and the environment.
 How do astronomers look for exoplanets?
 Most exoplanets are found via the transit method. That was the case for the trappist one planets.
 Tell me more about that method.
 The transit method is when a planet passes in front of its star. The amount of light block tells astronomers the size of the planet.
 it tells astronomers the orbital period of the planet.
 What telescopes use that method?
 The Kepler Space Telescope used a transit method to find thousands of exoplanets.
 I've heard about a satellite called Tess that is used to find planets. How does it work?
 Tens is a space telescope that looks for transits.
 What rules do proteins play in ourselves?
 Proteins are the building blocks of the body. They are the basic materials from which all living things are made. They are needed for the structure, function, and regular
 of the body's tissues and organs.
 How do they work?
 Proteins are made up of amino acids. Amino acids are the basic building blocks of proteins. There are 20 different amino acids that link together
 and different combinations to form different proteins.
 How do they catalyze chemical reactions?
 Proteins are biological catalysts. Cattleists are substances that speed up the rate of a chemical reaction without being consumed by the reaction.
 Our proteins that catalyzed increase the rate of biochemical reactions in the body. Thank you. Goodbye.
 sure anytime.
 you
</code></pre><p>Next we want to use the <code>ffmpeg</code> application again to render the subtitle text on the video itself so we can preview it. We use the <code>subprocess</code> library again to do so.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> base64 <span style=color:#f92672>import</span> b64encode
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>combine_subtitles</span>(input_file,  subtitle_file, output_file):
</span></span><span style=display:flex><span>  command <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;ffmpeg&#34;</span>, <span style=color:#e6db74>&#34;-i&#34;</span>, input_file, <span style=color:#e6db74>&#34;-vf&#34;</span>, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;subtitles=</span><span style=color:#e6db74>{</span>subtitle_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, output_file]
</span></span><span style=display:flex><span>  subprocess<span style=color:#f92672>.</span>run(command)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>combine_subtitles(<span style=color:#e6db74>&#39;./video.mp4&#39;</span>, <span style=color:#e6db74>&#39;./video.srt&#39;</span>, <span style=color:#e6db74>&#39;./video_subbed.mp4&#39;</span>)
</span></span></code></pre></div><p>Now we want to display our subtitled video, because Google Colab has some problem using the IPython Video component, we have to use the workaround below which will load the video (as base64 encoded text) into memory and display it in a HTML5 video component in the browser. The subtitled text is at the bottom, you might need to hide the controls (move mouse away) to see the subtitles.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> IPython.display <span style=color:#f92672>import</span> HTML
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> base64 <span style=color:#f92672>import</span> b64encode
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mp4 <span style=color:#f92672>=</span> open(<span style=color:#e6db74>&#39;./video_subbed.mp4&#39;</span>,<span style=color:#e6db74>&#39;rb&#39;</span>)<span style=color:#f92672>.</span>read()
</span></span><span style=display:flex><span>data_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;data:video/mp4;base64,&#34;</span> <span style=color:#f92672>+</span> b64encode(mp4)<span style=color:#f92672>.</span>decode()
</span></span><span style=display:flex><span>HTML(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;&lt;video width=1024 controls&gt;&lt;source src=&#34;</span><span style=color:#e6db74>{</span>data_url<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&#39;</span>)
</span></span></code></pre></div><p>We can connect to Google Drive with the following code. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> google.colab <span style=color:#f92672>import</span> drive
</span></span><span style=display:flex><span>drive<span style=color:#f92672>.</span>mount(<span style=color:#e6db74>&#39;/content/drive/&#39;</span>)
</span></span></code></pre></div><p>You can move the output files which are saved in the <code>/content/</code> directory to the root of your Google Drive.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> shutil
</span></span><span style=display:flex><span>shutil<span style=color:#f92672>.</span>move(<span style=color:#e6db74>&#39;/content/video_subbed.mp4&#39;</span>, <span style=color:#e6db74>&#39;/content/drive/My Drive/video_subbed.mp4&#39;</span>)
</span></span><span style=display:flex><span>shutil<span style=color:#f92672>.</span>move(<span style=color:#e6db74>&#39;/content/video.srt&#39;</span>, <span style=color:#e6db74>&#39;/content/drive/My Drive/video.srt&#39;</span>)
</span></span></code></pre></div><p>Note that there are also other OpenAI Whisper models, for example the best performing <a href=https://huggingface.co/openai/whisper-large-v2>whisper-large-v2</a> model.</p><p>More Notebooks @ <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> and do star or drop us some feedback on how to improve the notebooks on the <a href=https://github.com/eugenesiow/practical-ml/>Github repo</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/speech/>Speech</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/gpu/>GPU</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook/>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab/>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on twitter" href="https://twitter.com/intent/tweet/?text=Video%20Subtitling%20with%20OpenAI%20Whisper&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f&hashtags=Speech%2cDeepLearning%2cMachineLearning%2cGPU%2cSourceCode%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f&title=Video%20Subtitling%20with%20OpenAI%20Whisper&summary=Video%20Subtitling%20with%20OpenAI%20Whisper&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f&title=Video%20Subtitling%20with%20OpenAI%20Whisper"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on whatsapp" href="https://api.whatsapp.com/send?text=Video%20Subtitling%20with%20OpenAI%20Whisper%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Video Subtitling with OpenAI Whisper on telegram" href="https://telegram.me/share/url?text=Video%20Subtitling%20with%20OpenAI%20Whisper&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fvideo_subtitling_with_openai_whisper%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>