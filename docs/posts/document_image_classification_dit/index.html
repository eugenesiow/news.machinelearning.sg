<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Document Image Classification with Document Image Transformer (DiT) | News @ machinelearning.sg</title><meta name=keywords content="Computer Vision,Document AI,Deep Learning,Machine Learning,GPU,Source Code,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to automatically classify documents based on images of their contents. For example, automatically identify scientific papers or handwritten notes.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/document_image_classification_dit/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Document Image Classification with Document Image Transformer (DiT)"><meta property="og:description" content="tl;dr A step-by-step tutorial to automatically classify documents based on images of their contents. For example, automatically identify scientific papers or handwritten notes.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/document_image_classification_dit/"><meta property="og:image" content="https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-19T17:15:46+08:00"><meta property="article:modified_time" content="2022-12-19T17:15:46+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1.png"><meta name=twitter:title content="Document Image Classification with Document Image Transformer (DiT)"><meta name=twitter:description content="tl;dr A step-by-step tutorial to automatically classify documents based on images of their contents. For example, automatically identify scientific papers or handwritten notes.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"Document Image Classification with Document Image Transformer (DiT)","item":"https://news.machinelearning.sg/posts/document_image_classification_dit/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Document Image Classification with Document Image Transformer (DiT)","name":"Document Image Classification with Document Image Transformer (DiT)","description":"tl;dr A step-by-step tutorial to automatically classify documents based on images of their contents. For example, automatically identify scientific papers or handwritten notes.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.","keywords":["Computer Vision","Document AI","Deep Learning","Machine Learning","GPU","Source Code","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to automatically classify documents based on images of their contents. For example, automatically identify scientific papers or handwritten notes.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.\nDocument Image Classification with Document Image Transformer (DiT) Notebook to classify documents based on images of their contents. This task, called Document Image Classification might include classes of documents like letter, scientifica paper, form, email or resume.\nThe model used is a Document Image Transformer (DiT) model which is pre-trained on IIT-CDIP, a dataset that includes 42 million document images and fine-tuned on RVL-CDIP, a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class.\nThe notebook is structured as follows:\nSetting up the Environment Getting Data Using the Model (Running Inference) Setting up the Environment Setting up the Dependencies First we pip install the required dependencies. This downloads and installs the python libraries required from the PyPi repository online.\n!pip install -q transformers==4.25.1 pdf2image==1.16.0 Next we use install additional libraries to allow us to work with PDF files.\n!apt-get install -y poppler-utils Reading package lists... Done\rBuilding dependency tree Reading state information... Done\rpoppler-utils is already the newest version (0.62.0-2ubuntu2.14).\rThe following package was automatically installed and is no longer required:\rlibnvidia-common-460\rUse 'apt autoremove' to remove it.\r0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\rGetting Data Getting Document Images First we get a scientific/academic paper from OpenAI as a PDF file. Here we use wget to download the Whisper paper and save it as whisper.pdf. !wget https://cdn.openai.com/papers/whisper.pdf --2022-12-19 08:34:23-- https://cdn.openai.com/papers/whisper.pdf\rResolving cdn.openai.com (cdn.openai.com)... 13.107.246.40, 13.107.213.40, 2620:1ec:bdf::40, ...\rConnecting to cdn.openai.com (cdn.openai.com)|13.107.246.40|:443... connected.\rHTTP request sent, awaiting response... 200 OK\rLength: 857252 (837K) [application/pdf]\rSaving to: ‘whisper.pdf’\rwhisper.pdf 100%[===================\u003e] 837.16K --.-KB/s in 0.1s 2022-12-19 08:34:24 (7.69 MB/s) - ‘whisper.pdf’ saved [857252/857252]\rNext, we use the pdf2image library to convert the PDF and the cover page (page 0) to an image which is stored in the list whisper_pages. from pdf2image import convert_from_path ## 100 is the size to convert the image to. whisper_pages = convert_from_path('whisper.pdf', 100, first_page=0, last_page=1) Next, we use cv2_imshow() to display this image to see our PDF cover page of the paper rendered as an image. import numpy as np from google.colab.patches import cv2_imshow cv2_imshow(np.array(whisper_pages[0])) Great! Now let’s get a financial report from the National University of Singapore. We use wget once more. !wget https://nus.edu.sg/docs/default-source/annual-report/nus-financial-report-2022.pdf --2022-12-19 08:45:38-- https://nus.edu.sg/docs/default-source/annual-report/nus-financial-report-2022.pdf\rResolving nus.edu.sg (nus.edu.sg)... 45.60.33.225, 45.60.35.225\rConnecting to nus.edu.sg (nus.edu.sg)|45.60.33.225|:443... connected.\rHTTP request sent, awaiting response... 200 OK\rLength: 1536893 (1.5M) [application/pdf]\rSaving to: ‘nus-financial-report-2022.pdf’\rnus-financial-repor 100%[===================\u003e] 1.46M 768KB/s in 2.0s 2022-12-19 08:45:41 (768 KB/s) - ‘nus-financial-report-2022.pdf’ saved [1536893/1536893]\rAgain we use the pdf2image library to convert the PDF and one of the pages with the actual financial report (page 6) to an image which is stored in the list nus_pages. from pdf2image import convert_from_path nus_pages = convert_from_path('nus-financial-report-2022.pdf', 100, first_page=6, last_page=7) We use cv2_imshow() to display this image to see our PDF page of financials with a table of numbers rendered as an image. cv2_imshow(np.array(nus_pages[0])) Using the Model (Running Inference) First we download and setup the model weights of the pre-trained model from the huggingface hub using the transformers library.\nWe download and load the dit-base-finetuned-rvlcdip model using the convenience pipeline function in the library.\nfrom transformers import pipeline pipe = pipeline(task=\"image-classification\", model=\"microsoft/dit-base-finetuned-rvlcdip\") /usr/local/lib/python3.8/dist-packages/transformers/models/beit/image_processing_beit.py:111: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\rwarnings.warn(\rNow we can simply run the images that we have prepared against the pipeline to get the Document Image Classification results. First, we run the Whisper paper page image through the pipe. We see that the model is over 87% confident that this image is a scientific publication. That’s right!\noutput_json = pipe(whisper_pages[0]) output_json [{'score': 0.8787207007408142, 'label': 'scientific publication'},\r{'score': 0.02657017670571804, 'label': 'news article'},\r{'score': 0.02074269764125347, 'label': 'scientific report'},\r{'score': 0.006884910631924868, 'label': 'email'},\r{'score': 0.006216561887413263, 'label': 'file folder'}]\rNext we try with the NUS financial report page and viola, we get over 90% confidence that its a budget document. That’s the closest of the 16 classes and we’ve got that right too!\noutput_json = pipe(nus_pages[0]) output_json [{'score': 0.9057689309120178, 'label': 'budget'},\r{'score': 0.007138940971344709, 'label': 'form'},\r{'score': 0.006847307551652193, 'label': 'presentation'},\r{'score': 0.006752100307494402, 'label': 'advertisement'},\r{'score': 0.006463303696364164, 'label': 'email'}]\rYou can save the images to a file using the .save() method.\nnus_pages[0].save('nus_page.jpg', 'JPEG') whisper_pages[0].save('whisper_page.jpg', 'JPEG') We can connect to Google Drive with the following code. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the output files which are saved in the /content/ directory to the root of your Google Drive.\nimport shutil shutil.move('/content/whisper_page.jpg', '/content/drive/My Drive/whisper_page.jpg') More Notebooks @ eugenesiow/practical-ml and do star or drop us some feedback on how to improve the notebooks on the Github repo.\n","wordCount":"880","inLanguage":"en","image":"https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1.png","datePublished":"2022-12-19T17:15:46+08:00","dateModified":"2022-12-19T17:15:46+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/document_image_classification_dit/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Document Image Classification with Document Image Transformer (DiT)</h1><div class=post-meta><span title='2022-12-19 17:15:46 +0800 +08'>December 19, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1_hucbc077f7b2b4c7aa61c0b6634d6bdc65_536694_360x0_resize_box_3.png 360w ,https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1_hucbc077f7b2b4c7aa61c0b6634d6bdc65_536694_480x0_resize_box_3.png 480w ,https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1_hucbc077f7b2b4c7aa61c0b6634d6bdc65_536694_720x0_resize_box_3.png 720w ,https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1_hucbc077f7b2b4c7aa61c0b6634d6bdc65_536694_1080x0_resize_box_3.png 1080w ,https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1.png 1200w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/document_image_classification_dit/splash/Document_Image_Classification_DiT_1.png alt="Document Image Classification with Document Image Transformer (DiT)" width=1200 height=630></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li><a href=#document-image-classification-with-document-image-transformer-dit aria-label="Document Image Classification with Document Image Transformer (DiT)">Document Image Classification with Document Image Transformer (DiT)</a></li><li><a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul><ul><li><a href=#setting-up-the-dependencies aria-label="Setting up the Dependencies">Setting up the Dependencies</a></li></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><li><a href=#getting-document-images aria-label="Getting Document Images">Getting Document Images</a></li></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong>
A step-by-step tutorial to automatically classify documents based on images of their contents.
For example, automatically identify scientific papers or handwritten notes.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Document_Image_Classification_DiT.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h2 id=document-image-classification-with-document-image-transformer-dit>Document Image Classification with Document Image Transformer (DiT)<a hidden class=anchor aria-hidden=true href=#document-image-classification-with-document-image-transformer-dit>#</a></h2><p><a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Document_Image_Classification_DiT.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Notebook to classify documents based on images of their contents. This task, called <a href=https://paperswithcode.com/task/document-image-classification>Document Image Classification</a> might include classes of documents like letter, scientifica paper, form, email or resume.</p><p>The <a href=https://pytorch.org/hub/ultralytics_yolov5/>model used</a> is a Document Image Transformer (DiT) model which is pre-trained on <a href=https://data.nist.gov/od/id/mds2-2531>IIT-CDIP</a>, a dataset that includes 42 million document images and fine-tuned on <a href=https://paperswithcode.com/dataset/rvl-cdip>RVL-CDIP</a>, a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the Environment</li><li>Getting Data</li><li>Using the Model (Running Inference)</li></ul><h2 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h2><h4 id=setting-up-the-dependencies>Setting up the Dependencies<a hidden class=anchor aria-hidden=true href=#setting-up-the-dependencies>#</a></h4><p>First we <code>pip install</code> the required dependencies. This downloads and installs the python libraries required from the PyPi repository online.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>pip install <span style=color:#f92672>-</span>q transformers<span style=color:#f92672>==</span><span style=color:#ae81ff>4.25.1</span> pdf2image<span style=color:#f92672>==</span><span style=color:#ae81ff>1.16.0</span>
</span></span></code></pre></div><p>Next we use install additional libraries to allow us to work with PDF files.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>apt<span style=color:#f92672>-</span>get install <span style=color:#f92672>-</span>y poppler<span style=color:#f92672>-</span>utils
</span></span></code></pre></div><pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
poppler-utils is already the newest version (0.62.0-2ubuntu2.14).
The following package was automatically installed and is no longer required:
  libnvidia-common-460
Use 'apt autoremove' to remove it.
0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.
</code></pre><h2 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h2><h3 id=getting-document-images>Getting Document Images<a hidden class=anchor aria-hidden=true href=#getting-document-images>#</a></h3><ol><li>First we get a scientific/academic paper from OpenAI as a PDF file. Here we use <code>wget</code> to download the Whisper paper and save it as <code>whisper.pdf</code>.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>wget https:<span style=color:#f92672>//</span>cdn<span style=color:#f92672>.</span>openai<span style=color:#f92672>.</span>com<span style=color:#f92672>/</span>papers<span style=color:#f92672>/</span>whisper<span style=color:#f92672>.</span>pdf
</span></span></code></pre></div><pre><code>--2022-12-19 08:34:23--  https://cdn.openai.com/papers/whisper.pdf
Resolving cdn.openai.com (cdn.openai.com)... 13.107.246.40, 13.107.213.40, 2620:1ec:bdf::40, ...
Connecting to cdn.openai.com (cdn.openai.com)|13.107.246.40|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 857252 (837K) [application/pdf]
Saving to: ‘whisper.pdf’

whisper.pdf         100%[===================&gt;] 837.16K  --.-KB/s    in 0.1s    

2022-12-19 08:34:24 (7.69 MB/s) - ‘whisper.pdf’ saved [857252/857252]
</code></pre><ol start=2><li>Next, we use the <code>pdf2image</code> library to convert the PDF and the cover page (page 0) to an image which is stored in the list <code>whisper_pages</code>.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pdf2image <span style=color:#f92672>import</span> convert_from_path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 100 is the size to convert the image to.</span>
</span></span><span style=display:flex><span>whisper_pages <span style=color:#f92672>=</span> convert_from_path(<span style=color:#e6db74>&#39;whisper.pdf&#39;</span>, <span style=color:#ae81ff>100</span>, first_page<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, last_page<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><ol start=3><li>Next, we use <code>cv2_imshow()</code> to display this image to see our PDF cover page of the paper rendered as an image.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> google.colab.patches <span style=color:#f92672>import</span> cv2_imshow
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv2_imshow(np<span style=color:#f92672>.</span>array(whisper_pages[<span style=color:#ae81ff>0</span>]))
</span></span></code></pre></div><p><img loading=lazy src=output_13_0.png alt=png></p><ol start=4><li>Great! Now let&rsquo;s get a financial report from the National University of Singapore. We use <code>wget</code> once more.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>wget https:<span style=color:#f92672>//</span>nus<span style=color:#f92672>.</span>edu<span style=color:#f92672>.</span>sg<span style=color:#f92672>/</span>docs<span style=color:#f92672>/</span>default<span style=color:#f92672>-</span>source<span style=color:#f92672>/</span>annual<span style=color:#f92672>-</span>report<span style=color:#f92672>/</span>nus<span style=color:#f92672>-</span>financial<span style=color:#f92672>-</span>report<span style=color:#f92672>-</span><span style=color:#ae81ff>2022.</span>pdf
</span></span></code></pre></div><pre><code>--2022-12-19 08:45:38--  https://nus.edu.sg/docs/default-source/annual-report/nus-financial-report-2022.pdf
Resolving nus.edu.sg (nus.edu.sg)... 45.60.33.225, 45.60.35.225
Connecting to nus.edu.sg (nus.edu.sg)|45.60.33.225|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1536893 (1.5M) [application/pdf]
Saving to: ‘nus-financial-report-2022.pdf’

nus-financial-repor 100%[===================&gt;]   1.46M   768KB/s    in 2.0s    

2022-12-19 08:45:41 (768 KB/s) - ‘nus-financial-report-2022.pdf’ saved [1536893/1536893]
</code></pre><ol start=5><li>Again we use the <code>pdf2image</code> library to convert the PDF and one of the pages with the actual financial report (page 6) to an image which is stored in the list <code>nus_pages</code>.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pdf2image <span style=color:#f92672>import</span> convert_from_path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nus_pages <span style=color:#f92672>=</span> convert_from_path(<span style=color:#e6db74>&#39;nus-financial-report-2022.pdf&#39;</span>, <span style=color:#ae81ff>100</span>, first_page<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, last_page<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>)
</span></span></code></pre></div><ol start=6><li>We use <code>cv2_imshow()</code> to display this image to see our PDF page of financials with a table of numbers rendered as an image.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cv2_imshow(np<span style=color:#f92672>.</span>array(nus_pages[<span style=color:#ae81ff>0</span>]))
</span></span></code></pre></div><p><img loading=lazy src=output_19_0.png alt=png></p><h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2><p>First we download and setup the model weights of the pre-trained model from the huggingface hub using the <code>transformers</code> library.</p><p>We download and load the <a href=https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip><code>dit-base-finetuned-rvlcdip</code></a> model using the convenience pipeline function in the library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipeline(task<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;image-classification&#34;</span>, model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;microsoft/dit-base-finetuned-rvlcdip&#34;</span>)
</span></span></code></pre></div><pre><code>/usr/local/lib/python3.8/dist-packages/transformers/models/beit/image_processing_beit.py:111: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.
  warnings.warn(
</code></pre><p>Now we can simply run the images that we have prepared against the pipeline to get the Document Image Classification results. First, we run the Whisper paper page image through the pipe. We see that the model is over 87% confident that this image is a scientific publication. That&rsquo;s right!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>output_json <span style=color:#f92672>=</span> pipe(whisper_pages[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>output_json
</span></span></code></pre></div><pre><code>[{'score': 0.8787207007408142, 'label': 'scientific publication'},
 {'score': 0.02657017670571804, 'label': 'news article'},
 {'score': 0.02074269764125347, 'label': 'scientific report'},
 {'score': 0.006884910631924868, 'label': 'email'},
 {'score': 0.006216561887413263, 'label': 'file folder'}]
</code></pre><p>Next we try with the NUS financial report page and viola, we get over 90% confidence that its a budget document. That&rsquo;s the closest of the 16 classes and we&rsquo;ve got that right too!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>output_json <span style=color:#f92672>=</span> pipe(nus_pages[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>output_json
</span></span></code></pre></div><pre><code>[{'score': 0.9057689309120178, 'label': 'budget'},
 {'score': 0.007138940971344709, 'label': 'form'},
 {'score': 0.006847307551652193, 'label': 'presentation'},
 {'score': 0.006752100307494402, 'label': 'advertisement'},
 {'score': 0.006463303696364164, 'label': 'email'}]
</code></pre><p>You can save the images to a file using the <code>.save()</code> method.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>nus_pages[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#39;nus_page.jpg&#39;</span>, <span style=color:#e6db74>&#39;JPEG&#39;</span>)
</span></span><span style=display:flex><span>whisper_pages[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#39;whisper_page.jpg&#39;</span>, <span style=color:#e6db74>&#39;JPEG&#39;</span>)
</span></span></code></pre></div><p>We can connect to Google Drive with the following code. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> google.colab <span style=color:#f92672>import</span> drive
</span></span><span style=display:flex><span>drive<span style=color:#f92672>.</span>mount(<span style=color:#e6db74>&#39;/content/drive/&#39;</span>)
</span></span></code></pre></div><p>You can move the output files which are saved in the <code>/content/</code> directory to the root of your Google Drive.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> shutil
</span></span><span style=display:flex><span>shutil<span style=color:#f92672>.</span>move(<span style=color:#e6db74>&#39;/content/whisper_page.jpg&#39;</span>, <span style=color:#e6db74>&#39;/content/drive/My Drive/whisper_page.jpg&#39;</span>)
</span></span></code></pre></div><p>More Notebooks @ <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> and do star or drop us some feedback on how to improve the notebooks on the <a href=https://github.com/eugenesiow/practical-ml/>Github repo</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/computer-vision/>Computer Vision</a></li><li><a href=https://news.machinelearning.sg/tags/document-ai/>Document AI</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/gpu/>GPU</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook/>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab/>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on twitter" href="https://twitter.com/intent/tweet/?text=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f&hashtags=ComputerVision%2cDocumentAI%2cDeepLearning%2cMachineLearning%2cGPU%2cSourceCode%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f&title=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29&summary=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f&title=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on whatsapp" href="https://api.whatsapp.com/send?text=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Document Image Classification with Document Image Transformer (DiT) on telegram" href="https://telegram.me/share/url?text=Document%20Image%20Classification%20with%20Document%20Image%20Transformer%20%28DiT%29&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fdocument_image_classification_dit%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>