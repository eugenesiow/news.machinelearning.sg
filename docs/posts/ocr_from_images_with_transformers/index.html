<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OCR (Optical Character Recognition) from Images with Transformers | News @ machinelearning.sg</title><meta name=keywords content="Computer Vision,Deep Learning,Machine Learning,Source Code,PyTorch,Optical Character Recognition,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="OCR (Optical Character Recognition) from Images with Transformers"><meta property="og:description" content="tl;dr A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/"><meta property="og:image" content="https://news.machinelearning.sg/ocr_from_images.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-21T10:00:00+08:00"><meta property="article:modified_time" content="2021-10-21T10:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/ocr_from_images.jpg"><meta name=twitter:title content="OCR (Optical Character Recognition) from Images with Transformers"><meta name=twitter:description content="tl;dr A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"OCR (Optical Character Recognition) from Images with Transformers","item":"https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OCR (Optical Character Recognition) from Images with Transformers","name":"OCR (Optical Character Recognition) from Images with Transformers","description":"tl;dr A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.","keywords":["Computer Vision","Deep Learning","Machine Learning","Source Code","PyTorch","Optical Character Recognition","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.\nOCR (Optical Character Recognition) from Images with Transformers Notebook to recognise text automaticaly from an input image with either handwritten or printed text.\nOptical Character Recognition is the task of converting images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars…) or from subtitle text superimposed on an image (for example: from a television broadcast).\nThe transformer models used are from Microsoft’s TrOCR. The TrOCR models are encoder-decoder models, consisting of an image Transformer as encoder, and a text Transformer as decoder. We utilise the versions hosted on huggingface.co and use the awesome transformers library, for longevity and simplicity.\nThe notebook is structured as follows:\nSetting up the Environment Using the Model (Running Inference) Setting up the Environment Dependencies and Runtime If you’re running this notebook in Google Colab, most of the dependencies are already installed and we don’t need the GPU for this particular example.\nIf you decide to run this on many (\u003ethousands) images and want the inference to go faster though, you can select Runtime \u003e Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator.\nWe need to install huggingface transformers for this example to run, so execute the command below to setup the dependencies. We use the version compiled directly from the latest source (at the time of writing this is the only way to access the transforemrs TrOCR model code).\n!pip install -q git+https://github.com/huggingface/transformers.git Using the Model (Running Inference) Let’s define a function for us to get images from the web. We execute this function to download an image with a line of handwritten text and display it.\nimport requests from IPython.display import display from PIL import Image def show_image(url): img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\") display(img) return img handwriting1 = show_image('https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg') Now we want to load the model to recognise handwritten text.\nSpecifically we are running the following steps:\nLoad the processor, TrOCRProcessor, which processes our input image and converts it into a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. The processor also adds absolute position embeddings and this sequence is fed to the layers of the Transformer encoder. Load the model, VisionEncoderDecoderModel, which consists of the image encoder and the text decoder. Define ocr_image function - We define the function for inferencing which takes our src_img, the input image we have downloaded. It will then run both the processor and the model inference and produce the output OCR text that has been recognised from the image. import transformers from transformers import TrOCRProcessor, VisionEncoderDecoderModel processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten') model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten') def ocr_image(src_img): pixel_values = processor(images=src_img, return_tensors=\"pt\").pixel_values generated_ids = model.generate(pixel_values) return processor.batch_decode(generated_ids, skip_special_tokens=True)[0] We now run our ocr_image function on the line of handwritten text in the image we have downloaded previously (and stored in handwriting1).\nocr_image(handwriting1) 'industry, \" Mr. Brown commented icily. \" Let us have a' Lets try on another image with handwritten text.\nocr_image(show_image('https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoolxi9yWGAT5SLZShv8vVd0bz47UWRzQC19fDTeE8GmGv_Rn-PCF1pP1rrUx8kOjA4gg\u0026usqp=CAU')) 'Hope you have done it.' import transformers from transformers import TrOCRProcessor, VisionEncoderDecoderModel print_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed') print_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed') def ocr_print_image(src_img): pixel_values = print_processor(images=src_img, return_tensors=\"pt\").pixel_values generated_ids = print_model.generate(pixel_values) return print_processor.batch_decode(generated_ids, skip_special_tokens=True)[0] We download an image with noisy printed text, a scanned receipt.\nreceipt = show_image('https://github.com/zzzDavid/ICDAR-2019-SROIE/raw/master/data/img/000.jpg') As the model processes a line of text, we crop the image to include on of the lines of text in the receipt and send it to our model.\nreceipt_crop = receipt.crop((0, 80, receipt.size[0], 110)) display(receipt_crop) ocr_print_image(receipt_crop) 'BOOK TA,K(TAMAN DAYA) SDN BHD' More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\nAlternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\nGoogle Colab vs Paperspace Gradient ","wordCount":"709","inLanguage":"en","image":"https://news.machinelearning.sg/ocr_from_images.jpg","datePublished":"2021-10-21T10:00:00+08:00","dateModified":"2021-10-21T10:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>OCR (Optical Character Recognition) from Images with Transformers</h1><div class=post-meta><span title='2021-10-21 10:00:00 +0800 +08'>October 21, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images_hu075a28758daa5f35654693d188869f65_431609_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images_hu075a28758daa5f35654693d188869f65_431609_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images_hu075a28758daa5f35654693d188869f65_431609_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images_hu075a28758daa5f35654693d188869f65_431609_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images_hu075a28758daa5f35654693d188869f65_431609_1500x0_resize_q75_box.jpg 1500w ,https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images.jpg 3975w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/ocr_from_images_with_transformers/ocr_from_images.jpg alt="Optical Character Recognition from Images with TrOCR" width=3975 height=1879></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li><a href=#ocr-optical-character-recognition-from-images-with-transformers aria-label="OCR (Optical Character Recognition) from Images with Transformers">OCR (Optical Character Recognition) from Images with Transformers</a></li><li><a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul><ul><li><a href=#dependencies-and-runtime aria-label="Dependencies and Runtime">Dependencies and Runtime</a></li></ul></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li><li><a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li><a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> A step-by-step tutorial to automatically recognise text (OCR) from images of handwritten and printed text using transformer encoder-decoder models.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/OCR_from_Images_with_Transformers.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h2 id=ocr-optical-character-recognition-from-images-with-transformers>OCR (Optical Character Recognition) from Images with Transformers<a hidden class=anchor aria-hidden=true href=#ocr-optical-character-recognition-from-images-with-transformers>#</a></h2><p>Notebook to recognise text automaticaly from an input image with either handwritten or printed text.</p><p><a href=https://paperswithcode.com/task/optical-character-recognition>Optical Character Recognition</a> is the task of converting images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars&mldr;) or from subtitle text superimposed on an image (for example: from a television broadcast).</p><p>The <a href=https://malaya-speech.readthedocs.io/en/latest/tts-singlish.html>transformer models used</a> are from Microsoft&rsquo;s TrOCR. The TrOCR models are encoder-decoder models, consisting of an image Transformer as encoder, and a text Transformer as decoder. We utilise the versions hosted on <a href="https://huggingface.co/models?search=microsoft/trocr">huggingface.co</a> and use the awesome transformers library, for longevity and simplicity.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the Environment</li><li>Using the Model (Running Inference)</li></ul><h2 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h2><h4 id=dependencies-and-runtime>Dependencies and Runtime<a hidden class=anchor aria-hidden=true href=#dependencies-and-runtime>#</a></h4><p>If you&rsquo;re running this notebook in Google Colab, most of the dependencies are already installed and we don&rsquo;t need the GPU for this particular example.</p><p>If you decide to run this on many (>thousands) images and want the inference to go faster though, you can select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>.</p><p>We need to install huggingface <code>transformers</code> for this example to run, so execute the command below to setup the dependencies. We use the version compiled directly from the latest source (at the time of writing this is the only way to access the transforemrs TrOCR model code).</p><pre tabindex=0><code>!pip install -q git+https://github.com/huggingface/transformers.git
</code></pre><h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2><p>Let&rsquo;s define a function for us to get images from the web. We execute this function to download an image with a line of handwritten text and display it.</p><pre tabindex=0><code>import requests
from IPython.display import display
from PIL import Image

def show_image(url):
  img = Image.open(requests.get(url, stream=True).raw).convert(&#34;RGB&#34;)
  display(img)
  return img

handwriting1 = show_image(&#39;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&#39;)
</code></pre><p><img loading=lazy src=OCR_from_Images_with_Transformers_files/OCR_from_Images_with_Transformers_8_0.png alt=png></p><p>Now we want to load the model to recognise handwritten text.</p><p>Specifically we are running the following steps:</p><ul><li>Load the processor, <code>TrOCRProcessor</code>, which processes our input image and converts it into a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. The processor also adds absolute position embeddings and this sequence is fed to the layers of the Transformer encoder.</li><li>Load the model, <code>VisionEncoderDecoderModel</code>, which consists of the image encoder and the text decoder.</li><li>Define <code>ocr_image</code> function - We define the function for inferencing which takes our <code>src_img</code>, the input image we have downloaded. It will then run both the processor and the model inference and produce the output OCR text that has been recognised from the image.</li></ul><pre tabindex=0><code>import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel

processor = TrOCRProcessor.from_pretrained(&#39;microsoft/trocr-base-handwritten&#39;)
model = VisionEncoderDecoderModel.from_pretrained(&#39;microsoft/trocr-base-handwritten&#39;)

def ocr_image(src_img):
  pixel_values = processor(images=src_img, return_tensors=&#34;pt&#34;).pixel_values
  generated_ids = model.generate(pixel_values)
  return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
</code></pre><p>We now run our <code>ocr_image</code> function on the line of handwritten text in the image we have downloaded previously (and stored in <code>handwriting1</code>).</p><pre tabindex=0><code>ocr_image(handwriting1)
</code></pre><pre><code>'industry, &quot; Mr. Brown commented icily. &quot; Let us have a'
</code></pre><p>Lets try on another image with handwritten text.</p><pre tabindex=0><code>ocr_image(show_image(&#39;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoolxi9yWGAT5SLZShv8vVd0bz47UWRzQC19fDTeE8GmGv_Rn-PCF1pP1rrUx8kOjA4gg&amp;usqp=CAU&#39;))
</code></pre><p><img loading=lazy src=OCR_from_Images_with_Transformers_files/OCR_from_Images_with_Transformers_14_0.png alt=png></p><pre><code>'Hope you have done it.'
</code></pre><pre tabindex=0><code>import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel

print_processor = TrOCRProcessor.from_pretrained(&#39;microsoft/trocr-base-printed&#39;)
print_model = VisionEncoderDecoderModel.from_pretrained(&#39;microsoft/trocr-base-printed&#39;)

def ocr_print_image(src_img):
  pixel_values = print_processor(images=src_img, return_tensors=&#34;pt&#34;).pixel_values
  generated_ids = print_model.generate(pixel_values)
  return print_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
</code></pre><p>We download an image with noisy printed text, a scanned receipt.</p><pre tabindex=0><code>receipt = show_image(&#39;https://github.com/zzzDavid/ICDAR-2019-SROIE/raw/master/data/img/000.jpg&#39;)
</code></pre><p><img loading=lazy src=OCR_from_Images_with_Transformers_files/OCR_from_Images_with_Transformers_17_0.png alt=png></p><p>As the model processes a line of text, we crop the image to include on of the lines of text in the receipt and send it to our model.</p><pre tabindex=0><code>receipt_crop = receipt.crop((0, 80, receipt.size[0], 110))
display(receipt_crop)
ocr_print_image(receipt_crop)
</code></pre><p><img loading=lazy src=OCR_from_Images_with_Transformers_files/OCR_from_Images_with_Transformers_19_0.png alt=png></p><pre><code>'BOOK TA,K(TAMAN DAYA) SDN BHD'
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2><p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p><iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe><h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2><p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p><ul><li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/computer-vision/>Computer Vision</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch/>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/optical-character-recognition/>Optical Character Recognition</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook/>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab/>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on twitter" href="https://twitter.com/intent/tweet/?text=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f&hashtags=ComputerVision%2cDeepLearning%2cMachineLearning%2cSourceCode%2cPyTorch%2cOpticalCharacterRecognition%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f&title=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers&summary=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f&title=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on whatsapp" href="https://api.whatsapp.com/send?text=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share OCR (Optical Character Recognition) from Images with Transformers on telegram" href="https://telegram.me/share/url?text=OCR%20%28Optical%20Character%20Recognition%29%20from%20Images%20with%20Transformers&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2focr_from_images_with_transformers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>