<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Top 10 Computer Vision Code Repositories from Singapore | News @ machinelearning.sg</title><meta name=keywords content="Github,Deep Learning,Singapore,Machine Learning,Source Code,PyTorch,TensorFlow,Computer Vision"><meta name=description content="tl;dr We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.
The ranking is decided based on the total Github stars of the repositories.
10. Dual Path Networks Architecture of DPNs. Retrieved from Github. This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs)."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Top 10 Computer Vision Code Repositories from Singapore"><meta property="og:description" content="tl;dr We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.
The ranking is decided based on the total Github stars of the repositories.
10. Dual Path Networks Architecture of DPNs. Retrieved from Github. This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs)."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/"><meta property="og:image" content="https://news.machinelearning.sg/cv_repo_nuscenes.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-16T08:00:00+08:00"><meta property="article:modified_time" content="2020-12-16T08:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/cv_repo_nuscenes.jpg"><meta name=twitter:title content="Top 10 Computer Vision Code Repositories from Singapore"><meta name=twitter:description content="tl;dr We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.
The ranking is decided based on the total Github stars of the repositories.
10. Dual Path Networks Architecture of DPNs. Retrieved from Github. This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"Top 10 Computer Vision Code Repositories from Singapore","item":"https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Top 10 Computer Vision Code Repositories from Singapore","name":"Top 10 Computer Vision Code Repositories from Singapore","description":"tl;dr We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.\nThe ranking is decided based on the total Github stars of the repositories.\n10. Dual Path Networks Architecture of DPNs. Retrieved from Github. This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs).","keywords":["Github","Deep Learning","Singapore","Machine Learning","Source Code","PyTorch","TensorFlow","Computer Vision"],"articleBody":" tl;dr We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.\nThe ranking is decided based on the total Github stars of the repositories.\n10. Dual Path Networks Architecture of DPNs. Retrieved from Github. This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs).\nThe paper, Dual Path Networks, is published at NeurIPS 2017.\nRepository cypw/DPNs License Not Specified Author Yunpeng (cypw) Vocation Senior research scientist at YITU Teach Language Stars Forks Open Issues Python 534 150 16 9. PointPillars for KITTI object detection Detecting objects from Point Clouds. Retrieved from Github. This repo demonstrates how to reproduce the results from PointPillars: Fast Encoders for Object Detection from Point Clouds (CVPR 2019).\nPointPillars is a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird’s eye view KITTI benchmarks.\nRepository nutonomy/second.pytorch License MIT License Author Motional (nutonomy) Vocation We’re making self-driving vehicles a safe, reliable, and accessible reality. at None Language Stars Forks Open Issues Python 544 143 1 8. 👀 Eye Tracking Library Eye/gaze tracking in real-time. A Python (2 and 3) library that provides a webcam-based eye tracking system. It gives you the exact position of the pupils and the gaze direction, in real time.\nRepository antoinelame/GazeTracking License MIT License Author Antoine Lamé (antoinelame) Vocation Software Engineer. Laravel, Python, Go. Currently building high frequency trading algorithms. Language Stars Forks Open Issues Python 574 183 32 7. PyTorch Video Activity Recognition. Detecting the 'applying lipstick' activity in a video. PyTorch implemented C3D, R3D, R2Plus1D models for video activity recognition. The models are trained on the on UCF101 and HMDB51 datasets.\nRepository jfzhang95/pytorch-video-recognition License MIT License Author Pyjcsx (jfzhang95) Vocation PhD student at NUS at National University of Singapore Language Stars Forks Open Issues Python 640 170 32 6. PyTorch Implementation of FaceBoxes The architecture of FaceBoxes. Retrieved from Github. A PyTorch implementation of the FaceBoxes: A CPU Real-time Face Detector with High Accuracy. The original was implemented in caffe. FaceBoxes is a novel CPU-based face detector, with superior performance on both speed and accuracy. The speed of FaceBoxes is invariant to the number of faces.\nRepository zisianw/FaceBoxes.PyTorch License MIT License Author Zi Sian Wong (zisianw) Vocation Computer Vision \u0026 Deep Learning Language Stars Forks Open Issues Python 659 173 10 5. :angel: Morph faces with Python, Numpy, Scipy Morphing a face automatically from a source to destination image. Retrieved from Github. Warp, average and morph human faces!\nLocates face points Align faces by resizing, centering and cropping to given size Given 2 images and its face points, warp one image to the other Triangulates face points Affine transforms each triangle with bilinear interpolation 4a. Morph between 2 or more images 4b. Average faces from 2 or more images Optional blending of warped image: Weighted average Alpha feathering Poisson blend Repository alyssaq/face_morpher License Not Specified Author Alyssa Quek (alyssaq) Vocation Software Engineer at Apple Language Stars Forks Open Issues Python 705 196 32 4. nuScenes Dataset Devkit The nuScenes dataset is a public large-scale dataset for autonomous driving developed by the team at Motional (formerly nuTonomy). The dataset is meant to support public research into computer vision and autonomous driving. The dataset contains 1000 driving scenes in Boston and Singapore, two cities that are known for their dense traffic and highly challenging driving situations. The scenes of 20 second length are manually selected to show a diverse and interesting set of driving maneuvers, traffic situations and unexpected behaviors. To facilitate common computer vision tasks, such as object detection and tracking, the dataset contains annotations for 23 object classes with accurate 3D bounding boxes at 2Hz over the entire dataset.\nThis repository contains the devkit of the nuImages and nuScenes dataset.\nRepository nutonomy/nuscenes-devkit License Other Author Motional (nutonomy) Vocation Motional (formerly nutonomy), making self-driving vehicles Language Stars Forks Open Issues Python 777 287 2 3. PyTorch Implementation of EfficientDet Architecture of EfficientDet including a weighted bi-directional feature pyramid network (BiFPN). A PyTorch implementation of the 2019 Computer Vision paper EfficientDet: Scalable and Efficient Object Detection from Google Brain. The official implementation by Google Brain is in TensorFlow.\nRepository toandaominh1997/EfficientDet.Pytorch License MIT Author Đào Minh Toàn (toandaominh1997) Vocation Data Scientist at VinID Language Stars Forks Watchers Open Issues Python 1,337 300 44 112 2. Object Detection: YOLO3 Object bounding boxes on video footage as predicted by YOLO3. Retrieved from the official YOLO site. A Computer Vision repository with code for training and evaluation of a YOLO3 model for the Object Detection task. YOLO, You Only Look Once, is a state-of-the-art, real-time object detection model. Its claim to fame is its extremely fast and accurate and you can trade-off speed and accuracy without re-training by changing the model size. Multi-GPU training is also implemented.\nRepository experiencor/keras-yolo3 License MIT Author Huynh Ngoc Anh (experiencor) Vocation Machine Learning Engineer at Grab Language Stars Forks Watchers Open Issues Python 1,362 753 54 217 1. DeepLab v3+ model in PyTorch Some results of the deep labelling model on various datasets. Retrieved from Github. A computer vision repository which started with an early PyTorch implementation (circa 2018) of DeepLab-V3-Plus (in PyTorch 0.4.1). DeepLab is a series of image semantic segmentation models whose latest version, v3+, is state-of-art on the semantic segmentation task. It can use Modified Aligned Xception and ResNet as backbone. The authors train DeepLab V3 Plus using Pascal VOC 2012, SBD and Cityscapes datasets. Pre-trained models on ResNet, MobileNet and DRN are provided.\nRepository jfzhang95/pytorch-deeplab-xception License MIT Author Jianfeng Zhang (jfzhang95) Vocation PhD Student at NUS Language Stars Forks Watchers Open Issues Python 2,057 634 47 95 More CV repositories Visit machinelearning.sg to view a full list of ML repositories from Singapore.\n","wordCount":"1059","inLanguage":"en","image":"https://news.machinelearning.sg/cv_repo_nuscenes.jpg","datePublished":"2020-12-16T08:00:00+08:00","dateModified":"2020-12-16T08:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Top 10 Computer Vision Code Repositories from Singapore</h1><div class=post-meta><span title='2020-12-16 08:00:00 +0800 +08'>December 16, 2020</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes_hu216c6720eb1c52458e848fcecf04300c_245565_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes_hu216c6720eb1c52458e848fcecf04300c_245565_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes_hu216c6720eb1c52458e848fcecf04300c_245565_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes_hu216c6720eb1c52458e848fcecf04300c_245565_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes.jpg 1440w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/top_10_computer_vision_code_repositories_from_singapore/cv_repo_nuscenes.jpg alt="nuScenes dataset from Motional (formerly nuTonomy) featuring 1000 driving scenes in Boston and Singapore. Retrieved from Github." width=1440 height=750><p>nuScenes dataset from Motional (formerly nuTonomy) featuring 1000 driving scenes in Boston and Singapore.</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#10-dual-path-networks aria-label="10. Dual Path Networks">10. Dual Path Networks</a></li><li><a href=#9-pointpillars-for-kitti-object-detection aria-label="9. PointPillars for KITTI object detection">9. PointPillars for KITTI object detection</a></li><li><a href=#8--eye-tracking-library aria-label="8. 👀 Eye Tracking Library">8. 👀 Eye Tracking Library</a></li><li><a href=#7-pytorch-video-activity-recognition aria-label="7. PyTorch Video Activity Recognition.">7. PyTorch Video Activity Recognition.</a></li><li><a href=#6-pytorch-implementation-of-faceboxes aria-label="6. PyTorch Implementation of FaceBoxes">6. PyTorch Implementation of FaceBoxes</a></li><li><a href=#5-angel-morph-faces-with-python-numpy-scipy aria-label="5. :angel: Morph faces with Python, Numpy, Scipy">5. :angel: Morph faces with Python, Numpy, Scipy</a></li><li><a href=#4-nuscenes-dataset-devkit aria-label="4. nuScenes Dataset Devkit">4. nuScenes Dataset Devkit</a></li><li><a href=#3-pytorch-implementation-of-efficientdet aria-label="3. PyTorch Implementation of EfficientDet">3. PyTorch Implementation of EfficientDet</a></li><li><a href=#2-object-detection-yolo3 aria-label="2. Object Detection: YOLO3">2. Object Detection: YOLO3</a></li><li><a href=#1-deeplab-v3-model-in-pytorch aria-label="1. DeepLab v3+ model in PyTorch">1. DeepLab v3+ model in PyTorch</a></li><li><a href=#more-cv-repositories aria-label="More CV repositories">More CV repositories</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> We feature 10 of the top Computer Vision (CV) code repositories from Singapore. These include popular
implementations of YOLO3, EfficientDet, DeepLab, FaceBoxes and other models ranging from activity recognition to eye tracking.<br>The ranking is decided based on the total Github stars of the repositories.</p></blockquote><h2 id=10-dual-path-networks>10. Dual Path Networks<a hidden class=anchor aria-hidden=true href=#10-dual-path-networks>#</a></h2><figure><img loading=lazy src=cv_repo_dpns.png><figcaption>Architecture of DPNs. Retrieved from Github.</figcaption></figure><p>This repository contains the code and trained models of Dual Path Networks which won the 1st place in Object Localization
Task in ILSVRC 2017, and was a Top 3 team with on all competition tasks (Team: NUS-Qihoo_DPNs).</p><p>The paper, <a href=https://arxiv.org/abs/1707.01629>Dual Path Networks</a>, is published at NeurIPS 2017.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/cypw/DPNs>cypw/DPNs</a></td></tr><tr><td>License</td><td>Not Specified</td></tr><tr><td>Author</td><td><a href=https://cypw.github.io/>Yunpeng</a> (<a href=https://github.com/cypw>cypw</a>)</td></tr><tr><td>Vocation</td><td>Senior research scientist at YITU Teach</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>534</td><td style=text-align:right>150</td><td style=text-align:right>16</td></tr></tbody></table><h2 id=9-pointpillars-for-kitti-object-detection>9. PointPillars for KITTI object detection<a hidden class=anchor aria-hidden=true href=#9-pointpillars-for-kitti-object-detection>#</a></h2><figure><img loading=lazy src=cv_repo_pointpillars.png><figcaption>Detecting objects from Point Clouds. Retrieved from Github.</figcaption></figure><p>This repo demonstrates how to reproduce the results from <a href=https://arxiv.org/abs/1812.05784>PointPillars: Fast Encoders for Object Detection from
Point Clouds (CVPR 2019)</a>.</p><p>PointPillars is a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical
columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture,
we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous
encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection
pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and
bird&rsquo;s eye view KITTI benchmarks.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/nutonomy/second.pytorch>nutonomy/second.pytorch</a></td></tr><tr><td>License</td><td><a href=https://api.github.com/licenses/mit>MIT License</a></td></tr><tr><td>Author</td><td><a href=motional.com>Motional</a> (<a href=https://github.com/nutonomy>nutonomy</a>)</td></tr><tr><td>Vocation</td><td>We&rsquo;re making self-driving vehicles a safe, reliable, and accessible reality. at None</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>544</td><td style=text-align:right>143</td><td style=text-align:right>1</td></tr></tbody></table><h2 id=8--eye-tracking-library>8. 👀 Eye Tracking Library<a hidden class=anchor aria-hidden=true href=#8--eye-tracking-library>#</a></h2><figure><img loading=lazy src=cv_repo_eye_tracking.jpg><figcaption>Eye/gaze tracking in real-time.</figcaption></figure><p>A Python (2 and 3) library that provides a webcam-based eye tracking system.
It gives you the exact position of the pupils and the gaze direction, in real time.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/antoinelame/GazeTracking>antoinelame/GazeTracking</a></td></tr><tr><td>License</td><td><a href=https://api.github.com/licenses/mit>MIT License</a></td></tr><tr><td>Author</td><td><a href=http://antoinelame.fr>Antoine Lamé</a> (<a href=https://github.com/antoinelame>antoinelame</a>)</td></tr><tr><td>Vocation</td><td>Software Engineer. Laravel, Python, Go. Currently building high frequency trading algorithms.</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>574</td><td style=text-align:right>183</td><td style=text-align:right>32</td></tr></tbody></table><h2 id=7-pytorch-video-activity-recognition>7. PyTorch Video Activity Recognition.<a hidden class=anchor aria-hidden=true href=#7-pytorch-video-activity-recognition>#</a></h2><figure><img loading=lazy src=cv_repo_video.jpg><figcaption>Detecting the 'applying lipstick' activity in a video.</figcaption></figure><p>PyTorch implemented C3D, R3D, R2Plus1D models for video activity recognition. The models are trained on the on UCF101
and HMDB51 datasets.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/jfzhang95/pytorch-video-recognition>jfzhang95/pytorch-video-recognition</a></td></tr><tr><td>License</td><td><a href=https://api.github.com/licenses/mit>MIT License</a></td></tr><tr><td>Author</td><td><a href=http://jeff95.me>Pyjcsx</a> (<a href=https://github.com/jfzhang95>jfzhang95</a>)</td></tr><tr><td>Vocation</td><td>PhD student at NUS at National University of Singapore</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>640</td><td style=text-align:right>170</td><td style=text-align:right>32</td></tr></tbody></table><h2 id=6-pytorch-implementation-of-faceboxes>6. PyTorch Implementation of FaceBoxes<a hidden class=anchor aria-hidden=true href=#6-pytorch-implementation-of-faceboxes>#</a></h2><figure><img loading=lazy src=cv_repo_faceboxes.jpg><figcaption>The architecture of FaceBoxes. Retrieved from Github.</figcaption></figure><p>A PyTorch implementation of the <a href=https://arxiv.org/abs/1708.05234>FaceBoxes: A CPU Real-time Face Detector with High Accuracy</a>.
The original was implemented in <a href=https://github.com/sfzhang15/FaceBoxes>caffe</a>. FaceBoxes is a novel CPU-based face detector,
with superior performance on both speed and accuracy. The speed of FaceBoxes is invariant to the number of faces.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/zisianw/FaceBoxes.PyTorch>zisianw/FaceBoxes.PyTorch</a></td></tr><tr><td>License</td><td><a href=https://api.github.com/licenses/mit>MIT License</a></td></tr><tr><td>Author</td><td>Zi Sian Wong (<a href=https://github.com/zisianw>zisianw</a>)</td></tr><tr><td>Vocation</td><td>Computer Vision & Deep Learning</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>659</td><td style=text-align:right>173</td><td style=text-align:right>10</td></tr></tbody></table><h2 id=5-angel-morph-faces-with-python-numpy-scipy>5. :angel: Morph faces with Python, Numpy, Scipy<a hidden class=anchor aria-hidden=true href=#5-angel-morph-faces-with-python-numpy-scipy>#</a></h2><figure><img loading=lazy src=cv_repo_face_morpher.png><figcaption>Morphing a face automatically from a source to destination image. Retrieved from Github.</figcaption></figure><p>Warp, average and morph human faces!</p><ol><li>Locates face points</li><li>Align faces by resizing, centering and cropping to given size</li><li>Given 2 images and its face points, warp one image to the other<ul><li>Triangulates face points</li><li>Affine transforms each triangle with bilinear interpolation
4a. Morph between 2 or more images
4b. Average faces from 2 or more images</li></ul></li><li>Optional blending of warped image:<ul><li>Weighted average</li><li>Alpha feathering</li><li>Poisson blend</li></ul></li></ol><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/alyssaq/face_morpher>alyssaq/face_morpher</a></td></tr><tr><td>License</td><td>Not Specified</td></tr><tr><td>Author</td><td><a href=https://alyssaq.github.io>Alyssa Quek</a> (<a href=https://github.com/alyssaq>alyssaq</a>)</td></tr><tr><td>Vocation</td><td>Software Engineer at Apple</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>705</td><td style=text-align:right>196</td><td style=text-align:right>32</td></tr></tbody></table><h2 id=4-nuscenes-dataset-devkit>4. nuScenes Dataset Devkit<a hidden class=anchor aria-hidden=true href=#4-nuscenes-dataset-devkit>#</a></h2><figure><img loading=lazy src=cv_repo_nuscenes2.png></figure><p>The nuScenes dataset is a public large-scale dataset for autonomous driving developed by the team at Motional
(formerly nuTonomy). The dataset is meant to support public research into computer vision and autonomous driving.
The dataset contains 1000 driving scenes in Boston and Singapore, two cities that are known for their dense traffic and
highly challenging driving situations. The scenes of 20 second length are manually selected to show a diverse and
interesting set of driving maneuvers, traffic situations and unexpected behaviors. To facilitate common computer vision
tasks, such as object detection and tracking, the dataset contains annotations for 23 object classes with accurate
3D bounding boxes at 2Hz over the entire dataset.</p><p>This repository contains the devkit of the nuImages and nuScenes dataset.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/nutonomy/nuscenes-devkit>nutonomy/nuscenes-devkit</a></td></tr><tr><td>License</td><td><a href=None>Other</a></td></tr><tr><td>Author</td><td><a href=motional.com>Motional</a> (<a href=https://github.com/nutonomy>nutonomy</a>)</td></tr><tr><td>Vocation</td><td><a href=motional.com>Motional</a> (formerly nutonomy), making self-driving vehicles</td></tr></tbody></table><table><thead><tr><th>Language</th><th style=text-align:right>Stars</th><th style=text-align:right>Forks</th><th style=text-align:right>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td style=text-align:right>777</td><td style=text-align:right>287</td><td style=text-align:right>2</td></tr></tbody></table><h2 id=3-pytorch-implementation-of-efficientdet>3. PyTorch Implementation of EfficientDet<a hidden class=anchor aria-hidden=true href=#3-pytorch-implementation-of-efficientdet>#</a></h2><figure><img loading=lazy src=cv_repo_efficientdet_archi.png><figcaption>Architecture of EfficientDet including a weighted bi-directional feature pyramid network (BiFPN).</figcaption></figure><p>A PyTorch implementation of the 2019 <strong>Computer Vision</strong> paper <a href=https://arxiv.org/abs/1911.09070>EfficientDet: Scalable and Efficient Object Detection</a> from Google Brain.
The <a href=https://github.com/google/automl/tree/master/efficientdet>official implementation</a> by Google Brain is in TensorFlow.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/toandaominh1997/EfficientDet.Pytorch>toandaominh1997/EfficientDet.Pytorch</a></td></tr><tr><td>License</td><td><a href=https://github.com/toandaominh1997/EfficientDet.Pytorch/blob/master/LICENSE>MIT</a></td></tr><tr><td>Author</td><td><a href=https://twitter.com/toandaominh1997>Đào Minh Toàn</a> (<a href=https://github.com/toandaominh1997>toandaominh1997</a>)</td></tr><tr><td>Vocation</td><td><a href=https://www.linkedin.com/in/toandaominh1997>Data Scientist</a> at <a href=https://medium.com/vinid>VinID</a></td></tr></tbody></table><table><thead><tr><th>Language</th><th>Stars</th><th>Forks</th><th>Watchers</th><th>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td>1,337</td><td>300</td><td>44</td><td>112</td></tr></tbody></table><h2 id=2-object-detection-yolo3>2. Object Detection: YOLO3<a hidden class=anchor aria-hidden=true href=#2-object-detection-yolo3>#</a></h2><figure><img loading=lazy src=cv_repo_yolo3.png><figcaption>Object bounding boxes on video footage as predicted by YOLO3. Retrieved from the official YOLO site.</figcaption></figure><p>A <strong>Computer Vision</strong> repository with code for training and evaluation of a YOLO3 model for the <strong>Object Detection</strong> task.
<a href=https://pjreddie.com/darknet/yolo/>YOLO</a>, You Only Look Once, is a state-of-the-art, real-time object detection model.
Its claim to fame is its extremely fast and accurate and you can trade-off speed and accuracy without re-training
by changing the model size. Multi-GPU training is also implemented.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/experiencor/keras-yolo3>experiencor/keras-yolo3</a></td></tr><tr><td>License</td><td><a href=https://github.com/experiencor/keras-yolo3/blob/master/LICENSE>MIT</a></td></tr><tr><td>Author</td><td><a href=https://experiencor.github.io/>Huynh Ngoc Anh</a> (<a href=https://github.com/experiencor>experiencor</a>)</td></tr><tr><td>Vocation</td><td><a href=https://sg.linkedin.com/in/ngoca>Machine Learning Engineer</a> at <a href=https://engineering.grab.com/>Grab</a></td></tr></tbody></table><table><thead><tr><th>Language</th><th>Stars</th><th>Forks</th><th>Watchers</th><th>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td>1,362</td><td>753</td><td>54</td><td>217</td></tr></tbody></table><h2 id=1-deeplab-v3-model-in-pytorch>1. DeepLab v3+ model in PyTorch<a hidden class=anchor aria-hidden=true href=#1-deeplab-v3-model-in-pytorch>#</a></h2><figure><img loading=lazy src=cv_repo_pytorch_deeplab.png><figcaption>Some results of the deep labelling model on various datasets. Retrieved from Github.</figcaption></figure><p>A computer vision repository which started with an early PyTorch implementation (circa 2018) of DeepLab-V3-Plus (in PyTorch 0.4.1).
DeepLab is a series of <strong>image semantic segmentation</strong> models whose latest version, v3+, is state-of-art on the semantic segmentation task.
It can use Modified Aligned Xception and ResNet as backbone.
The authors train DeepLab V3 Plus using Pascal VOC 2012, SBD and Cityscapes datasets. Pre-trained models on ResNet,
MobileNet and DRN are provided.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Repository</td><td><a href=https://github.com/jfzhang95/pytorch-deeplab-xception>jfzhang95/pytorch-deeplab-xception</a></td></tr><tr><td>License</td><td><a href=https://github.com/jfzhang95/pytorch-deeplab-xception/blob/master/LICENSE>MIT</a></td></tr><tr><td>Author</td><td><a href=http://jeff95.me/>Jianfeng Zhang</a> (<a href=https://github.com/jfzhang95>jfzhang95</a>)</td></tr><tr><td>Vocation</td><td>PhD Student at NUS</td></tr></tbody></table><table><thead><tr><th>Language</th><th>Stars</th><th>Forks</th><th>Watchers</th><th>Open Issues</th></tr></thead><tbody><tr><td>Python</td><td>2,057</td><td>634</td><td>47</td><td>95</td></tr></tbody></table><h2 id=more-cv-repositories>More CV repositories<a hidden class=anchor aria-hidden=true href=#more-cv-repositories>#</a></h2><p>Visit <a href=https://machinelearning.sg/repo/>machinelearning.sg</a> to view a full list of ML repositories from Singapore.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/github/>Github</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/singapore/>Singapore</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch/>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/tensorflow/>TensorFlow</a></li><li><a href=https://news.machinelearning.sg/tags/computer-vision/>Computer Vision</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on twitter" href="https://twitter.com/intent/tweet/?text=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f&hashtags=Github%2cDeepLearning%2cSingapore%2cMachineLearning%2cSourceCode%2cPyTorch%2cTensorFlow%2cComputerVision"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f&title=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore&summary=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f&title=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on whatsapp" href="https://api.whatsapp.com/send?text=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Top 10 Computer Vision Code Repositories from Singapore on telegram" href="https://telegram.me/share/url?text=Top%2010%20Computer%20Vision%20Code%20Repositories%20from%20Singapore&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2ftop_10_computer_vision_code_repositories_from_singapore%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>