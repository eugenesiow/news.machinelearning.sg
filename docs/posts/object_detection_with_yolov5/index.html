<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Object Detection with YOLOv5: Detecting People in Images | News @ machinelearning.sg</title><meta name=keywords content="Computer Vision,Deep Learning,Machine Learning,Source Code,PyTorch,Object Detection,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/object_detection_with_yolov5/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-185405110-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Object Detection with YOLOv5: Detecting People in Images"><meta property="og:description" content="tl;dr A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/object_detection_with_yolov5/"><meta property="og:image" content="https://news.machinelearning.sg/object_detection.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-30T16:00:00+08:00"><meta property="article:modified_time" content="2021-05-30T16:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/object_detection.jpg"><meta name=twitter:title content="Object Detection with YOLOv5: Detecting People in Images"><meta name=twitter:description content="tl;dr A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.
Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://news.machinelearning.sg/posts/"},{"@type":"ListItem","position":2,"name":"Object Detection with YOLOv5: Detecting People in Images","item":"https://news.machinelearning.sg/posts/object_detection_with_yolov5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Object Detection with YOLOv5: Detecting People in Images","name":"Object Detection with YOLOv5: Detecting People in Images","description":"tl;dr A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.","keywords":["Computer Vision","Deep Learning","Machine Learning","Source Code","PyTorch","Object Detection","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.\nPractical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. Continue on if you prefer reading the code here.\nDetect Persons From An Image with YOLOv5 Object Detection Notebook to detect persons from a image and to export clippings of the persons and an image with bounding boxes drawn. It can detect classes other than persons as well.\nObject detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. Pre-trained YOLOv5 models are used in this one-stage method that prioritizes inference speed.\nThe model used is one of the pre-trained ultralytics/yolov5 models. It was trained on the COCO train2017 for object detection set, it has 80 classes.\nThe notebook is structured as follows:\nSetting up the Environment Getting Data Using the Model (Running Inference) Setting up the Environment Dependencies and Runtime If you’re running this notebook in Google Colab, all the dependencies are already installed and we don’t need the GPU for this particular example.\nIf you decide to run this on many (\u003ethousands) images and want the inference to go faster though, you can select Runtime \u003e Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator.\nGetting Data Getting Foreground Images Foreground image #1: A photo of a baseball team lined up by Wade Austin Ellis. From Unsplash. Foreground image #2: An anime fanart image of a gundam pilot by basedheero. From Danbooru. We’ll save these images to our local storage and view a preview of them in our notebook.\nimport cv2 from urllib.request import urlretrieve from google.colab.patches import cv2_imshow # save the foreground and background to our local storage urlretrieve('https://images.unsplash.com/photo-1526497127495-3b388dc87620?auto=format\u0026fit=crop\u0026w=640\u0026q=80', '/content/foreground1.jpg') urlretrieve('https://danbooru.donmai.us/data/original/c0/4e/__tatsumi_hori_gundam_and_2_more__c04e8425ff3685202a67386027ea555d.png', '/content/foreground2.jpg') # display the images in the notebook cv2_imshow(cv2.imread('/content/foreground1.jpg')) cv2_imshow(cv2.imread('/content/foreground2.jpg')) Using the Model (Running Inference) First, we need to install the required dependencies for YOLOv5 by running the code below.\nWe will also print out the torch version at the end and if we are using CPU or GPU. Both are fine for the inference because YOLOv5 is very fast. If we are running this on thousands of images, we might want to use a GPU. See the Setting up the Environment section above for more details.\n!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt import torch print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\") Now we want to load and run the model on our 2 foregrounds. We only need to use the yolov5s model checkpoints, which is very fast (just 2ms on a V100) and small (just 14.1mb). If you want higher object detection accuracy, try the larger models, they come in s, m, l and x sizes.\nSpecifically we are running the following steps:\ntorch.hub.load() - Loads the pre-trained model from torchhub. In particular, we specify to use the small model, yolov5s. create a list called imgs with the 2 file paths of the 2 foregrounds in the list. model() - We run the inference using the model we loaded on the imgs list. results.print() - We print out a summary of the inference run and results from the results object which is returned from the previous step. import torch import torch from IPython.display import clear_output # Load the model from torch.hub model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Setup the paths of the 2 foregrounds in a list to run the model on dir = '/content/' imgs = [dir + f for f in ('foreground1.jpg', 'foreground2.jpg')] # Inference, run the model on the 2 foreground images results = model(imgs, size=640) clear_output() # Print out a summary of the inference job, we ran it on 2 images, at what speed and what detections were made results.print() We can also convert each of the results returned into a pandas DataFrame. We run the results.pandas().xyxy[0] to return the pandas DataFrame for the first foreground result (foreground #1). Note that foreground #2 is stored in the second element of the list .xyxy[1].\nWe see that the pandas DataFrame consists of rows of the 11 persons and 1 baseball glove detected in foreground #1. Each row contains the bounding box (xmin, ymin, xmax and ymax), the confidence of the detection and the class of the detection (0 is person and 35 is baseball glove).\nresults.pandas().xyxy[0] Now let’s clip out the first row result of foreground #1 and display the detected person.\n# get the tensor for foreground #1 detection results # and convert to an integer numpy array, return the positions x0, y0, x1, y1 x0, y0, x1, y1, _, _ = results.xyxy[0][0].numpy().astype(int) # crop/clip the image and show it cropped_image = results.imgs[0][y0:y1, x0:x1] cv2_imshow(cropped_image) If we wanted a view of the bounding boxes, classes and original image all rendered, we can just call results.render() and then show the resulting image from the .imgs list in results.\nWe show an example of calling render() and then displaying foreground #2 with the rendered bounding boxes and labels.\nresults.render() cv2_imshow(results.imgs[1]) We can save the rendered image to disk using the imwrite() function, with the output filepath and the result image array as parameters.\ncv2.imwrite('/content/foreground2_results.jpg', results.imgs[1]) We can connect to Google Drive with the following code. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the output files which are saved in the /content/ directory to the root of your Google Drive.\nimport shutil shutil.move('/content/foreground2_results.jpg', '/content/drive/My Drive/foreground2_results.jpg') We can also play around with some settings when we run the model inference. For example, we can set the confidence threshold very high (\u003e0.8) to ensure that only detections which we are 90% or more confident of will be shown.\nTo detect only persons, we can specify only the 0 class in model.classes.\nmodel.conf = 0.8 # confidence threshold (0-1) model.iou = 0.45 # NMS IoU threshold (0-1) model.classes = [0] # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs results = model(imgs, size=640) results.pandas().xyxy[0] More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\nAlternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\nGoogle Colab vs Paperspace Gradient ","wordCount":"1120","inLanguage":"en","image":"https://news.machinelearning.sg/object_detection.jpg","datePublished":"2021-05-30T16:00:00+08:00","dateModified":"2021-05-30T16:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/object_detection_with_yolov5/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h title="News @ machinelearning.sg (Alt + H)">News @ machinelearning.sg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://news.machinelearning.sg/about/ title=About><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Object Detection with YOLOv5: Detecting People in Images</h1><div class=post-meta><span title='2021-05-30 16:00:00 +0800 +08'>May 30, 2021</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img loading=lazy srcset="https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection_hu075a28758daa5f35654693d188869f65_570207_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection_hu075a28758daa5f35654693d188869f65_570207_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection_hu075a28758daa5f35654693d188869f65_570207_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection_hu075a28758daa5f35654693d188869f65_570207_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection_hu075a28758daa5f35654693d188869f65_570207_1500x0_resize_q75_box.jpg 1500w ,https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection.jpg 3975w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/object_detection_with_yolov5/object_detection.jpg alt="Object Detection with YOLOv5: Detecting People in Images" width=3975 height=1879></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li></ul><li><a href=#detect-persons-from-an-image-with-yolov5-object-detection aria-label="Detect Persons From An Image with YOLOv5 Object Detection">Detect Persons From An Image with YOLOv5 Object Detection</a></li><li><a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul><ul><ul><li><a href=#dependencies-and-runtime aria-label="Dependencies and Runtime">Dependencies and Runtime</a></li></ul></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><ul><ul><li><a href=#getting-foreground-images aria-label="Getting Foreground Images">Getting Foreground Images</a></li></ul></ul></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a><ul><li><a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li><a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul></li></ul></div></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> A step-by-step tutorial to detect people in photos automatically using the ultra-fast You-Only-Look-Once (YOLOv5) model.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Detect_Persons_From_Image_YOLOv5.ipynb title="Open in Colab"><img loading=lazy src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h1 id=detect-persons-from-an-image-with-yolov5-object-detection>Detect Persons From An Image with YOLOv5 Object Detection<a hidden class=anchor aria-hidden=true href=#detect-persons-from-an-image-with-yolov5-object-detection>#</a></h1><p>Notebook to detect persons from a image and to export clippings of the persons and an image with bounding boxes drawn. It can detect classes other than persons as well.</p><p><a href=https://paperswithcode.com/task/object-detection>Object detection</a> is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. Pre-trained YOLOv5 models are used in this one-stage method that prioritizes inference speed.</p><p>The <a href=https://pytorch.org/hub/ultralytics_yolov5/>model used</a> is one of the pre-trained <code>ultralytics/yolov5</code> models. It was trained on the COCO train2017 for object detection set, it has <a href=https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#1-create-datasetyaml>80 classes</a>.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the Environment</li><li>Getting Data</li><li>Using the Model (Running Inference)</li></ul><h1 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h1><h4 id=dependencies-and-runtime>Dependencies and Runtime<a hidden class=anchor aria-hidden=true href=#dependencies-and-runtime>#</a></h4><p>If you&rsquo;re running this notebook in Google Colab, all the dependencies are already installed and we don&rsquo;t need the GPU for this particular example.</p><p>If you decide to run this on many (>thousands) images and want the inference to go faster though, you can select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>.</p><h1 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h1><h4 id=getting-foreground-images>Getting Foreground Images<a hidden class=anchor aria-hidden=true href=#getting-foreground-images>#</a></h4><ul><li>Foreground image #1: A photo of a <a href=https://unsplash.com/photos/GudTmd-Q3Cg>baseball team</a> lined up by Wade Austin Ellis. From Unsplash.</li><li>Foreground image #2: An anime fanart image of a <a href=https://danbooru.donmai.us/posts/4549761>gundam pilot</a> by basedheero. From Danbooru.</li></ul><p>We&rsquo;ll save these images to our local storage and view a preview of them in our notebook.</p><pre tabindex=0><code>import cv2
from urllib.request import urlretrieve
from google.colab.patches import cv2_imshow

# save the foreground and background to our local storage
urlretrieve(&#39;https://images.unsplash.com/photo-1526497127495-3b388dc87620?auto=format&amp;fit=crop&amp;w=640&amp;q=80&#39;, &#39;/content/foreground1.jpg&#39;)
urlretrieve(&#39;https://danbooru.donmai.us/data/original/c0/4e/__tatsumi_hori_gundam_and_2_more__c04e8425ff3685202a67386027ea555d.png&#39;, &#39;/content/foreground2.jpg&#39;)

# display the images in the notebook
cv2_imshow(cv2.imread(&#39;/content/foreground1.jpg&#39;))
cv2_imshow(cv2.imread(&#39;/content/foreground2.jpg&#39;))
</code></pre><p><img loading=lazy src=Detect_Persons_From_Image_YOLOv5_files/Detect_Persons_From_Image_YOLOv5_6_0.png alt=png></p><p><img loading=lazy src=Detect_Persons_From_Image_YOLOv5_files/Detect_Persons_From_Image_YOLOv5_6_1.png alt=png></p><h1 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h1><p>First, we need to install the required dependencies for YOLOv5 by running the code below.</p><p>We will also print out the torch version at the end and if we are using CPU or GPU. Both are fine for the inference because YOLOv5 is very fast. If we are running this on thousands of images, we might want to use a GPU. See the <code>Setting up the Environment</code> section above for more details.</p><pre tabindex=0><code>!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt

import torch
print(f&#34;Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else &#39;CPU&#39;})&#34;)
</code></pre><p>Now we want to load and run the model on our 2 foregrounds. We only need to use the <a href=https://github.com/ultralytics/yolov5/releases/tag/v5.0>yolov5s model checkpoints</a>, which is very <a href=https://github.com/ultralytics/yolov5#pretrained-checkpoints>fast</a> (just 2ms on a V100) and small (just 14.1mb). If you want higher object detection accuracy, try the larger models, they come in s, m, l and x sizes.</p><p><img loading=lazy src=https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png alt="YOLOv5 Model Comparison"></p><p>Specifically we are running the following steps:</p><ul><li><code>torch.hub.load()</code> - Loads the pre-trained model from torchhub. In particular, we specify to use the small model, <code>yolov5s</code>.</li><li>create a list called <code>imgs</code> with the 2 file paths of the 2 foregrounds in the list.</li><li><code>model()</code> - We run the inference using the model we loaded on the <code>imgs</code> list.</li><li><code>results.print()</code> - We print out a summary of the inference run and results from the <code>results</code> object which is returned from the previous step.</li></ul><pre tabindex=0><code>import torch
import torch
from IPython.display import clear_output

# Load the model from torch.hub
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# Setup the paths of the 2 foregrounds in a list to run the model on 
dir = &#39;/content/&#39;
imgs = [dir + f for f in (&#39;foreground1.jpg&#39;, &#39;foreground2.jpg&#39;)]

# Inference, run the model on the 2 foreground images
results = model(imgs, size=640)
clear_output()

# Print out a summary of the inference job, we ran it on 2 images, at what speed and what detections were made
results.print()
</code></pre><p>We can also convert each of the results returned into a pandas DataFrame. We run the <code>results.pandas().xyxy[0]</code> to return the pandas DataFrame for the first foreground result (foreground #1). Note that foreground #2 is stored in the second element of the list <code>.xyxy[1]</code>.</p><p>We see that the pandas DataFrame consists of rows of the 11 persons and 1 baseball glove detected in foreground #1. Each row contains the bounding box (xmin, ymin, xmax and ymax), the confidence of the detection and the class of the detection (0 is person and 35 is baseball glove).</p><pre tabindex=0><code>results.pandas().xyxy[0]
</code></pre><p>Now let&rsquo;s clip out the first row result of foreground #1 and display the detected person.</p><pre tabindex=0><code># get the tensor for foreground #1 detection results 
# and convert to an integer numpy array, return the positions x0, y0, x1, y1
x0, y0, x1, y1, _, _ = results.xyxy[0][0].numpy().astype(int) 

# crop/clip the image and show it
cropped_image = results.imgs[0][y0:y1, x0:x1]
cv2_imshow(cropped_image)
</code></pre><p><img loading=lazy src=Detect_Persons_From_Image_YOLOv5_files/Detect_Persons_From_Image_YOLOv5_15_0.png alt=png></p><p>If we wanted a view of the bounding boxes, classes and original image all rendered, we can just call <code>results.render()</code> and then show the resulting image from the <code>.imgs</code> list in results.</p><p>We show an example of calling <code>render()</code> and then displaying foreground #2 with the rendered bounding boxes and labels.</p><pre tabindex=0><code>results.render()
cv2_imshow(results.imgs[1])
</code></pre><p><img loading=lazy src=Detect_Persons_From_Image_YOLOv5_files/Detect_Persons_From_Image_YOLOv5_17_0.png alt=png></p><p>We can save the rendered image to disk using the <code>imwrite()</code> function, with the output filepath and the result image array as parameters.</p><pre tabindex=0><code>cv2.imwrite(&#39;/content/foreground2_results.jpg&#39;, results.imgs[1])
</code></pre><p>We can connect to Google Drive with the following code. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><pre tabindex=0><code>from google.colab import drive
drive.mount(&#39;/content/drive/&#39;)
</code></pre><p>You can move the output files which are saved in the <code>/content/</code> directory to the root of your Google Drive.</p><pre tabindex=0><code>import shutil
shutil.move(&#39;/content/foreground2_results.jpg&#39;, &#39;/content/drive/My Drive/foreground2_results.jpg&#39;)
</code></pre><p>We can also play around with some settings when we run the model inference. For example, we can set the confidence threshold very high (>0.8) to ensure that only detections which we are 90% or more confident of will be shown.</p><p>To detect only persons, we can specify only the <code>0</code> class in <code>model.classes</code>.</p><pre tabindex=0><code>model.conf = 0.8  # confidence threshold (0-1)
model.iou = 0.45  # NMS IoU threshold (0-1)
model.classes = [0]  # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs

results = model(imgs, size=640)
results.pandas().xyxy[0]
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2><p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p><iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe><h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2><p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p><ul><li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/computer-vision/>Computer Vision</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning/>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning/>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/source-code/>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch/>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/object-detection/>Object Detection</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook/>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab/>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on twitter" href="https://twitter.com/intent/tweet/?text=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f&hashtags=ComputerVision%2cDeepLearning%2cMachineLearning%2cSourceCode%2cPyTorch%2cObjectDetection%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f&title=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images&summary=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f&title=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on whatsapp" href="https://api.whatsapp.com/send?text=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Object Detection with YOLOv5: Detecting People in Images on telegram" href="https://telegram.me/share/url?text=Object%20Detection%20with%20YOLOv5%3a%20Detecting%20People%20in%20Images&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fobject_detection_with_yolov5%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>