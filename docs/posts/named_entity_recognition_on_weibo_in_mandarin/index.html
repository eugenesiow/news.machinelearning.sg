<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Named Entity Recognition on Weibo in Mandarin | News @ machinelearning.sg</title>
<meta name=keywords content="Natural Language Processing,Deep Learning,Machine Learning,GPU,Source Code,PyTorch,Named Entity Recognition,命名实体识别,Jupyter Notebook,Colab">
<meta name=description content="tl;dr A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition (NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats the state-of-the-art by 20+ percentage points.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<meta name=author content="Eugene">
<link rel=canonical href=https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/>
<link href=https://news.machinelearning.sg/assets/css/stylesheet.min.d1ce48677bc3d2b81f6398ed4ae6ddf7262d885c01ffefc464636bbea61c0201.css integrity="sha256-0c5IZ3vD0rgfY5jtSubd9yYtiFwB/+/EZGNrvqYcAgE=" rel="preload stylesheet" as=style>
<link rel=icon href=https://news.machinelearning.sg/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png>
<link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png>
<link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-185405110-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<meta property="og:title" content="Named Entity Recognition on Weibo in Mandarin">
<meta property="og:description" content="tl;dr A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition (NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats the state-of-the-art by 20+ percentage points.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/">
<meta property="og:image" content="https://news.machinelearning.sg/ner_weibo.png"><meta property="article:published_time" content="2020-12-24T08:00:00+08:00">
<meta property="article:modified_time" content="2020-12-24T08:00:00+08:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://news.machinelearning.sg/ner_weibo.png">
<meta name=twitter:title content="Named Entity Recognition on Weibo in Mandarin">
<meta name=twitter:description content="tl;dr A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition (NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats the state-of-the-art by 20+ percentage points.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Named Entity Recognition on Weibo in Mandarin","name":"Named Entity Recognition on Weibo in Mandarin","description":"tl;dr A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition (NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats …","keywords":["Natural Language Processing","Deep Learning","Machine Learning","GPU","Source Code","PyTorch","Named Entity Recognition","命名实体识别","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition (NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats the state-of-the-art by 20+ percentage points.\n Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. \nContinue on if you prefer reading the code here.\nNamed Entity Recognition in Mandarin on a Weibo Social Media Dataset \nNotebook to train a flair model in mandarin using stacked embeddings (with word and BERT embeddings) to perform named entity recognition (NER).\nThe dataset used contains 1,890 Sina Weibo messages annotated with four entity types (person, organization, location and geo-political entity), including named and nominal mentions from the paper Peng et al. (2015) and with revised annotated data from He et al. (2016).\nThe current state-of-the-art model on this dataset is from Peng et al. (2016) with an average F1-score of 47.0% (Table 1) and from Peng et al. (2015) with an F1-score of 44.1% (Table 2). The authors say that the poor results on the test set show the “difficulty of this task” - which is true a sense because the dataset is really quite small for the NER task with 4 classes (x2 as they differentiate nominal and named entities) with a test set of only 270 sentences.\nOur flair model is able to improve the state-of-the-art with an F1-score of 67.5%, which is a cool 20+ absolute percentage points better than the current state-of-the-art performance.\nThe notebook is structured as follows:\n Setting up the GPU Environment Getting Data Training and Testing the Model Using the Model (Running Inference)  Task Description  Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens.\n Setting up the GPU Environment Ensure we have a GPU runtime If you’re running this notebook in Google Colab, select Runtime  Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator. This will allow us to use the GPU to train the model subsequently.\nInstall Dependencies pip install -q flair Getting Data The dataset, including the train, test and dev sets, has just been included in the 0.7 release of flair, hence, we just use the flair.datasets loader to load the WEIBO_NER dataset into the flair Corpus. The raw datasets are also available on Github.\nimport flair.datasets from flair.data import Corpus corpus = flair.datasets.WEIBO_NER() print(corpus) We can see that the total 1,890 sentences have already been split into train (1,350), dev (270) and test (270) sets in a 5:1:1 ratio.\nTraining and Testing the Model Train the Model To train the flair SequenceTagger, we use the ModelTrainer object with the corpus and the tagger to be trained. We use flair’s sensible default options in the .train() method, while specifying the output folder for the SequenceTagger model to be /content/model/. We also set the embeddings_storage_mode to be gpu to utilise the GPU to store the embeddings for more speed. Note that if you run this with a larger dataset you might run out of GPU memory, so be sure to set this option to cpu - it will still use the GPU to train but the embeddings will not be stored in the CPU and there will be a transfer to the GPU each epoch.\nBe prepared to allow the training to run for about 0.5 to 1 hour. We set the max_epochs to 50 so the the training will complete faster, for higher F1-score you can increase this number to 100 or 150.\nimport flair from typing import List from flair.trainers import ModelTrainer from flair.models import SequenceTagger from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings, BytePairEmbeddings tag_type = 'ner' tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) # For an even faster training time, you can comment out the BytePairEmbeddings # Note: there will be a small drop in performance if you do so. embedding_types: List[TokenEmbeddings] = [ WordEmbeddings('zh-crawl'), BytePairEmbeddings('zh'), BertEmbeddings('bert-base-chinese'), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) trainer: ModelTrainer = ModelTrainer(tagger, corpus) trainer.train('/content/model/', learning_rate=0.1, mini_batch_size=32, max_epochs=50, embeddings_storage_mode='gpu') We see that the output accuracy (F1-score) for our new model is 67.5% (F1-score (micro) 0.6748). We use micro F1-score (rather than macro F1-score) as there are multiple entity classes in this setup with class imbalance.\n We have a new SOTA NER model in mandarin, over 20 percentage points (absolute) better than the previous SOTA for this Weibo dataset!\n Using the Model (Running Inference) Running the model to do some predictions/inference is as simple as calling tagger.predict(sentence). Do note that for mandarin each character needs to be split with spaces between each character (e.g. 一 节 课 的 时 间) so that the tokenizer will work properly to split them to tokens (if you’re processing them for input into the model when building an app). For more information on this, check out the flair tutorial on tokenization.\nfrom flair.data import Sentence from flair.models import SequenceTagger from flair.data import Corpus # Load the model that we trained, you can comment this out if you already have # the model loaded (e.g. if you just ran the training) tagger: SequenceTagger = SequenceTagger.load(\"/content/model/final-model.pt\") # Load the WEIBO corpus and use the first 5 sentences from the test set corpus = flair.datasets.WEIBO_NER() for idx in range(0, 5): sentence = corpus.test[idx] tagger.predict(sentence) print(sentence.to_tagged_string()) We can connect to Google Drive with the following code to save any files you want to persist. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the model files from our local directory to your Google Drive.\nimport shutil shutil.move('/content/model/', \"/content/drive/My Drive/model/\") More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\n AI Glossary in Mandarin Visit or star the eugenesiow/ai-glossary-mandarin repository on Github if you need an English-to-Mandarin dictionary of AI terminology grouped topically by areas (e.g. NLP) and tasks (e.g. NER):\n Alternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\n Google Colab vs Paperspace Gradient  ","wordCount":"1092","inLanguage":"en","image":"https://news.machinelearning.sg/ner_weibo.png","datePublished":"2020-12-24T08:00:00+08:00","dateModified":"2020-12-24T08:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script>
<script async data-id=defa0fb0-3f27-4afe-b567-4f57d47c86e9 src=https://tinyads.io/e></script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>.theme-toggle,.top-link{display:none}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://news.machinelearning.sg/ accesskey=h>News
</a>
<span class=logo-switches>
<span class=theme-toggle>
<a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a>
</span>
<span class=lang-switch>
<ul>
<li>
<a href=https://machinelearning.sg>
machinelearning.sg
</a>
</li>
</ul>
</span>
</span>
</div>
<ul class=menu id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://news.machinelearning.sg/about/>
<span>
About
</span>
</a>
</li>
<li>
<a href=https://news.machinelearning.sg/archives/>
<span>
Archive
</span>
</a>
</li>
<li>
<a href=https://news.machinelearning.sg/tags/>
<span>
Tags
</span>
</a>
</li></ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Named Entity Recognition on Weibo in Mandarin
</h1>
<div class=post-meta>
December 24, 2020&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Eugene
</div>
</header>
<figure class=entry-cover>
<img srcset="https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo_hu07326c7fd950c1f195619bbed592737e_289587_360x0_resize_box_3.png 360w ,https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo_hu07326c7fd950c1f195619bbed592737e_289587_480x0_resize_box_3.png 480w ,https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo_hu07326c7fd950c1f195619bbed592737e_289587_720x0_resize_box_3.png 720w ,https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo_hu07326c7fd950c1f195619bbed592737e_289587_1080x0_resize_box_3.png 1080w ,https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo.png 1476w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/named_entity_recognition_on_weibo_in_mandarin/ner_weibo.png alt="Named Entity Recognition on Weibo in Mandarin">
</figure>
<div class=toc>
<details>
<summary>
<div class=details accesskey=c>Table of Contents</div>
</summary>
<blockquote><ul><li>
<a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li>
<a href=#named-entity-recognition-in-mandarin-on-a-weibo-social-media-dataset aria-label="Named Entity Recognition in Mandarin on a Weibo Social Media Dataset">Named Entity Recognition in Mandarin on a Weibo Social Media Dataset</a><ul>
<ul>
<li>
<a href=#task-description aria-label="Task Description">Task Description</a></li></ul>
</ul>
</li><li>
<a href=#setting-up-the-gpu-environment aria-label="Setting up the GPU Environment">Setting up the GPU Environment</a><ul>
<ul>
<li>
<a href=#ensure-we-have-a-gpu-runtime aria-label="Ensure we have a GPU runtime">Ensure we have a GPU runtime</a></li><li>
<a href=#install-dependencies aria-label="Install Dependencies">Install Dependencies</a></li></ul>
</ul>
</li><li>
<a href=#getting-data aria-label="Getting Data">Getting Data</a></li><li>
<a href=#training-and-testing-the-model aria-label="Training and Testing the Model">Training and Testing the Model</a><ul>
<ul>
<li>
<a href=#train-the-model aria-label="Train the Model">Train the Model</a></li></ul>
</ul>
</li><li>
<a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li><li>
<a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li>
<a href=#ai-glossary-in-mandarin aria-label="AI Glossary in Mandarin">AI Glossary in Mandarin</a></li><li>
<a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul>
</blockquote>
</details>
</div>
<div class=post-content>
<blockquote>
<p><strong>tl;dr</strong> A step-by-step tutorial to train a state-of-the-art model with flair and BERT for named entity recognition
(NER) in mandarin, 中文命名实体识别, on a Weibo dataset. Our model beats the state-of-the-art by 20+ percentage points.</p>
</blockquote>
<h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2>
<p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p>
<p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_Mandarin_Weibo.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p>
<p>Continue on if you prefer reading the code here.</p>
<h2 id=named-entity-recognition-in-mandarin-on-a-weibo-social-media-dataset>Named Entity Recognition in Mandarin on a Weibo Social Media Dataset<a hidden class=anchor aria-hidden=true href=#named-entity-recognition-in-mandarin-on-a-weibo-social-media-dataset>#</a></h2>
<p><a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_Mandarin_Weibo.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p>
<p>Notebook to train a <a href=https://github.com/flairNLP/flair>flair</a> model in mandarin using stacked embeddings (with word and BERT embeddings) to perform named entity recognition (NER).</p>
<p>The <a href=https://github.com/hltcoe/golden-horse>dataset</a> used contains 1,890 Sina Weibo messages annotated with four entity types (person, organization, location and geo-political entity), including named and nominal mentions from the paper <a href=https://www.aclweb.org/anthology/D15-1064/>Peng et al. (2015)</a> and with revised annotated data from <a href=https://arxiv.org/abs/1611.04234>He et al. (2016)</a>.</p>
<p>The current state-of-the-art model on this dataset is from <a href=https://www.aclweb.org/anthology/P16-2025/>Peng et al. (2016)</a> with an average F1-score of <strong>47.0%</strong> (Table 1) and from <a href=https://www.aclweb.org/anthology/D15-1064.pdf>Peng et al. (2015)</a> with an F1-score of <strong>44.1%</strong> (Table 2). The authors say that the poor results on the test set show the &ldquo;difficulty of this task&rdquo; - which is true a sense because the dataset is really quite small for the NER task with 4 classes (x2 as they differentiate nominal and named entities) with a test set of only 270 sentences.</p>
<p>Our flair model is able to improve the state-of-the-art with an F1-score of <strong>67.5%</strong>, which is a cool 20+ absolute percentage points better than the current state-of-the-art performance.</p>
<p>The notebook is structured as follows:</p>
<ul>
<li>Setting up the GPU Environment</li>
<li>Getting Data</li>
<li>Training and Testing the Model</li>
<li>Using the Model (Running Inference)</li>
</ul>
<h4 id=task-description>Task Description<a hidden class=anchor aria-hidden=true href=#task-description>#</a></h4>
<blockquote>
<p>Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens.</p>
</blockquote>
<h2 id=setting-up-the-gpu-environment>Setting up the GPU Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-gpu-environment>#</a></h2>
<h4 id=ensure-we-have-a-gpu-runtime>Ensure we have a GPU runtime<a hidden class=anchor aria-hidden=true href=#ensure-we-have-a-gpu-runtime>#</a></h4>
<p>If you&rsquo;re running this notebook in Google Colab, select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>. This will allow us to use the GPU to train the model subsequently.</p>
<h4 id=install-dependencies>Install Dependencies<a hidden class=anchor aria-hidden=true href=#install-dependencies>#</a></h4>
<pre tabindex=0><code>pip install -q flair
</code></pre><h2 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h2>
<p>The dataset, including the train, test and dev sets, has just been included in the <code>0.7 release</code> of flair, hence, we just use the <code>flair.datasets</code> loader to load the <code>WEIBO_NER</code> dataset into the flair <code>Corpus</code>. The <a href=https://github.com/87302380/WEIBO_NER>raw datasets</a> are also available on Github.</p>
<pre tabindex=0><code>import flair.datasets
from flair.data import Corpus
corpus = flair.datasets.WEIBO_NER()
print(corpus)
</code></pre><p>We can see that the total 1,890 sentences have already been split into train (1,350), dev (270) and test (270) sets in a 5:1:1 ratio.</p>
<h2 id=training-and-testing-the-model>Training and Testing the Model<a hidden class=anchor aria-hidden=true href=#training-and-testing-the-model>#</a></h2>
<h4 id=train-the-model>Train the Model<a hidden class=anchor aria-hidden=true href=#train-the-model>#</a></h4>
<p>To train the flair <code>SequenceTagger</code>, we use the <code>ModelTrainer</code> object with the corpus and the tagger to be trained. We use flair&rsquo;s sensible default options in the <code>.train()</code> method, while specifying the output folder for the <code>SequenceTagger</code> model to be <code>/content/model/</code>. We also set the <code>embeddings_storage_mode</code> to be <code>gpu</code> to utilise the GPU to store the embeddings for more speed. Note that if you run this with a larger dataset you might run out of GPU memory, so be sure to set this option to <code>cpu</code> - it will still use the GPU to train but the embeddings will not be stored in the CPU and there will be a transfer to the GPU each epoch.</p>
<p>Be prepared to allow the training to run for about 0.5 to 1 hour. We set the <code>max_epochs</code> to 50 so the the training will complete faster, for higher F1-score you can increase this number to 100 or 150.</p>
<pre tabindex=0><code>import flair
from typing import List
from flair.trainers import ModelTrainer
from flair.models import SequenceTagger
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings, BytePairEmbeddings

tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

# For an even faster training time, you can comment out the BytePairEmbeddings
# Note: there will be a small drop in performance if you do so.
embedding_types: List[TokenEmbeddings] = [
    WordEmbeddings('zh-crawl'),
    BytePairEmbeddings('zh'),
    BertEmbeddings('bert-base-chinese'),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train('/content/model/',
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=50,
              embeddings_storage_mode='gpu')
</code></pre><p>We see that the output accuracy (F1-score) for our new model is <strong>67.5%</strong> (F1-score (micro) 0.6748). We use micro F1-score (rather than macro F1-score) as there are multiple entity classes in this setup with <a href=https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin>class imbalance</a>.</p>
<blockquote>
<p>We have a new SOTA NER model in mandarin, over 20 percentage points (absolute) better than the previous SOTA for this Weibo dataset!</p>
</blockquote>
<h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2>
<p>Running the model to do some predictions/inference is as simple as calling <code>tagger.predict(sentence)</code>. Do note that for mandarin each character needs to be split with spaces between each character (e.g. <code>一 节 课 的 时 间</code>) so that the tokenizer will work properly to split them to tokens (if you&rsquo;re processing them for input into the model when building an app). For more information on this, check out the <a href=https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_1_BASICS.md#tokenization>flair tutorial on tokenization</a>.</p>
<pre tabindex=0><code>from flair.data import Sentence
from flair.models import SequenceTagger
from flair.data import Corpus

# Load the model that we trained, you can comment this out if you already have 
# the model loaded (e.g. if you just ran the training)
tagger: SequenceTagger = SequenceTagger.load(&quot;/content/model/final-model.pt&quot;)

# Load the WEIBO corpus and use the first 5 sentences from the test set
corpus = flair.datasets.WEIBO_NER()
for idx in range(0, 5):
  sentence = corpus.test[idx]
  tagger.predict(sentence)
  print(sentence.to_tagged_string())
</code></pre><p>We can connect to Google Drive with the following code to save any files you want to persist. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p>
<p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p>
<pre tabindex=0><code>from google.colab import drive
drive.mount('/content/drive/')
</code></pre><p>You can move the model files from our local directory to your Google Drive.</p>
<pre tabindex=0><code>import shutil
shutil.move('/content/model/', &quot;/content/drive/My Drive/model/&quot;)
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2>
<p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p>
<iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe>
<h2 id=ai-glossary-in-mandarin>AI Glossary in Mandarin<a hidden class=anchor aria-hidden=true href=#ai-glossary-in-mandarin>#</a></h2>
<p>Visit or star the <a href=https://github.com/eugenesiow/ai-glossary-mandarin>eugenesiow/ai-glossary-mandarin</a> repository on
Github if you need an English-to-Mandarin dictionary of AI terminology grouped topically by areas (e.g. NLP) and tasks (e.g. NER):</p>
<iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=ai-glossary-mandarin&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="AI Glossary in Mandarin"></iframe>
<h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2>
<p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p>
<ul>
<li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://news.machinelearning.sg/tags/natural-language-processing>Natural Language Processing</a></li>
<li><a href=https://news.machinelearning.sg/tags/deep-learning>Deep Learning</a></li>
<li><a href=https://news.machinelearning.sg/tags/machine-learning>Machine Learning</a></li>
<li><a href=https://news.machinelearning.sg/tags/gpu>GPU</a></li>
<li><a href=https://news.machinelearning.sg/tags/source-code>Source Code</a></li>
<li><a href=https://news.machinelearning.sg/tags/pytorch>PyTorch</a></li>
<li><a href=https://news.machinelearning.sg/tags/named-entity-recognition>Named Entity Recognition</a></li>
<li><a href=https://news.machinelearning.sg/tags/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB>命名实体识别</a></li>
<li><a href=https://news.machinelearning.sg/tags/jupyter-notebook>Jupyter Notebook</a></li>
<li><a href=https://news.machinelearning.sg/tags/colab>Colab</a></li>
</ul>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on twitter" href="https://twitter.com/intent/tweet/?text=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f&hashtags=NaturalLanguageProcessing%2cDeepLearning%2cMachineLearning%2cGPU%2cSourceCode%2cPyTorch%2cNamedEntityRecognition%2c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f&title=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin&summary=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f&title=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on whatsapp" href="https://api.whatsapp.com/send?text=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Named Entity Recognition on Weibo in Mandarin on telegram" href="https://telegram.me/share/url?text=Named%20Entity%20Recognition%20on%20Weibo%20in%20Mandarin&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fnamed_entity_recognition_on_weibo_in_mandarin%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main><footer class=footer>
<span>&copy; 2021 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top" accesskey=g>
<button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=https://news.machinelearning.sg/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(a){a.preventDefault();var b=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(b)}']`).scrollIntoView({behavior:"smooth"})})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>