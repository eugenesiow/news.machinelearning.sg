<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Anime Image Super Resolution with PyTorch and Waifu2x | News @ machinelearning.sg</title><meta name=keywords content="Computer Vision,Deep Learning,Machine Learning,Source Code,PyTorch,Image Super Resolution,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. 
Continue on if you prefer reading the code here."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/><link href=https://news.machinelearning.sg/assets/css/stylesheet.min.62447981861ad5bc7761e73be43801fe136a2aaaa8b7afd18ce582c358716848.css integrity="sha256-YkR5gYYa1bx3Yec75DgB/hNqKqqot6/RjOWCw1hxaEg=" rel="preload stylesheet" as=style><link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-185405110-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="Anime Image Super Resolution with PyTorch and Waifu2x"><meta property="og:description" content="tl;dr A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. 
Continue on if you prefer reading the code here."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/"><meta property="og:image" content="https://news.machinelearning.sg/anime_super_resolution.jpg"><meta property="article:published_time" content="2021-05-30T16:00:00+08:00"><meta property="article:modified_time" content="2021-05-30T16:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/anime_super_resolution.jpg"><meta name=twitter:title content="Anime Image Super Resolution with PyTorch and Waifu2x"><meta name=twitter:description content="tl;dr A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. 
Continue on if you prefer reading the code here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Anime Image Super Resolution with PyTorch and Waifu2x","name":"Anime Image Super Resolution with PyTorch and Waifu2x","description":"tl;dr A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.\n Practical Machine Learning - Learn Step-by-Step to Train a Model A â€¦","keywords":["Computer Vision","Deep Learning","Machine Learning","Source Code","PyTorch","Image Super Resolution","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.\n Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. \nContinue on if you prefer reading the code here.\nAnime Super Resolution in PyTorch Notebook to do image super resolution on a single anime image automatically with pre-trained deep learning models in PyTorch.\nImage Super-Resolution techniques reconstruct a higher-resolution image or sequence from the observed lower-resolution images. In particular we do single-image super-resolution (SISR) which just use a single input image.\nWe utilise yu45020â€™s neat PyTorch-based code which re-implements the original waifu2x and includes other image super-resolution models for us to quickly compare results.\nThe notebook is structured as follows:\n Setting up the Environment Getting the data Using the Models (Running Inference)  Setting up the Environment Setup Runtime The models run on CPU and thereâ€™s no need to add a GPU hardware accelerator.\nSetup Dependencies We need to setup the yu45020/Waifu2x library and model checkpoints to run, so execute the command below to get the repository from Github.\n!git clone -l -s https://github.com/yu45020/Waifu2x.git waifu2x %cd waifu2x Cloning into 'waifu2x'... warning: --local is ignored remote: Enumerating objects: 546, done.\u001b[K remote: Total 546 (delta 0), reused 0 (delta 0), pack-reused 546\u001b[K Receiving objects: 100% (546/546), 138.07 MiB | 25.76 MiB/s, done. Resolving deltas: 100% (284/284), done. /content/waifu2x  We need to extract the model checkpoints we intend to use with the following code.\n%cd model_check_points/Upconv_7/ !7z e anime.7z %cd ../CRAN_V2/ !unzip -qq -o CRAN_V2_02_28_2019.zip %cd ../vgg_7/ !7z e art.7z %cd ../../ Getting Data Getting Sample Images  An image of Nico Robin from anidb.  Weâ€™ll save this image to our local storage and view a preview in our notebook.\nimport cv2 from urllib.request import urlretrieve from google.colab.patches import cv2_imshow # save the image to our local storage urlretrieve('https://cdn-us.anidb.net/images/main/80187.jpg', '/content/image.jpg') # display the images in the notebook cv2_imshow(cv2.imread('/content/image.jpg')) Using the Model (Running Inference) Now we want to define a function to load and run the waifu2x pre-trained model checkpoints from the directory we have unzipped to. We are running 2x image super resolution.\nSpecifically we are running the following steps:\n Load the input image as img and create a baseline resized image called img_t with bicubic scaling to compare against our super res outputs. Add this to a tensor, final, which we will use to make the grid of images to compare at the end. Load (.load_pre_train_weights()) each of the model checkpoints specified in the models argument, which is a list of checkpoint .json files. Split our input image img, if its too large, into patches using the .split_img_tensor() method. Run the loaded model against each of the patches model(i). Merge the output patches with .merge_img_tensor(out). Concat the output super resolution upscaled image to the previous set of outputs (including the baseline img_t) using the torch.cat() function. Save the final tensor as an image grid of all the outputs using the TorchVision utility save_image() function.  from utils.prepare_images import * from Models import * from torchvision.utils import save_image def run_waifu2x(input_file, models, output_file): model = UpConv_7() img = Image.open(input_file).convert(\"RGB\") img_resized = img.resize((img.size[0] * 2, img.size[1] * 2), Image.BICUBIC) img_t = to_tensor(img_resized).unsqueeze(0) # put the original image in the final tensor final = img_t for model_file in models: model.load_pre_train_weights(json_file=model_file) # overlapping split # if input image is too large, then split it into overlapped patches # details can be found at [here](https://github.com/nagadomi/waifu2x/issues/238) img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3) img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0) with torch.no_grad(): out = [model(i) for i in img_patches] img_upscale = img_splitter.merge_img_tensor(out) # concat the upscaled image to the final tensor final = torch.cat([final, img_upscale]) # save the image using the torchvision util that allows us to draw a grid of images from a tensor save_image(final, output_file, nrow=(1+len(models))) Run the model on our input image /content/image.jpg to get an output grid of images for comparison, /content/waifu2x_out.jpg. We will run the noise0, noise2 and no noise models against our image and compare with the baseline.\nrun_waifu2x('/content/image.jpg', ['model_check_points/Upconv_7/noise0_scale2.0x_model.json', 'model_check_points/Upconv_7/noise2_scale2.0x_model.json', 'model_check_points/Upconv_7/scale2.0x_model.json'], '/content/waifu2x_out.jpg') Display the image grid output. We can see that the image is now 2x the original size. The leftmost image in the grid is our baseline that was scaled using Image.BICUBIC while the rest of the images are using Waifu2x models. I prefer the noise2 model, the third image in the grid, most of the time.\n# display the output grid of images cv2_imshow(cv2.imread('/content/waifu2x_out.jpg')) Now we try the CARN V2 model. Similarly we define a function to run against our input image.\nfrom utils.prepare_images import * from Models import * from torchvision.utils import save_image def run_carn(input_file, models, output_file): model = CARN_V2(color_channels=3, mid_channels=64, conv=nn.Conv2d, single_conv_size=3, single_conv_group=1, scale=2, activation=nn.LeakyReLU(0.1), SEBlock=True, repeat_blocks=3, atrous=(1, 1, 1)) model = network_to_half(model) img = Image.open(input_file).convert(\"RGB\") img_resized = img.resize((img.size[0] * 2, img.size[1] * 2), Image.BICUBIC) img_t = to_tensor(img_resized).unsqueeze(0) # put the original image in the final tensor final = img_t for model_file in models: model.load_state_dict(torch.load(model_file, 'cpu')) model = model.float() # overlapping split # if input image is too large, then split it into overlapped patches # details can be found at [here](https://github.com/nagadomi/waifu2x/issues/238) img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3) img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0) with torch.no_grad(): out = [model(i) for i in img_patches] img_upscale = img_splitter.merge_img_tensor(out) # concat the upscaled image to the final tensor final = torch.cat([final, img_upscale]) # save the image using the torchvision util that allows us to draw a grid of images from a tensor save_image(final, output_file, nrow=(1+len(models))) We run the function against our input image and output the file to CARN_V2_out.jpg and display it using cv2_imshow. The left side image is the baseline while the right side image is the upscaled super-res image.\nrun_carn('/content/image.jpg', ['model_check_points/CRAN_V2/CARN_model_checkpoint.pt'], '/content/CARN_V2_out.jpg') cv2_imshow(cv2.imread('/content/CARN_V2_out.jpg')) If you want to save only your output image, change the output function to save_image(img_upscale, '/content/CARN_V2_single.jpg'). The code below does just this.\nfrom utils.prepare_images import * from Models import * from torchvision.utils import save_image model = CARN_V2(color_channels=3, mid_channels=64, conv=nn.Conv2d, single_conv_size=3, single_conv_group=1, scale=2, activation=nn.LeakyReLU(0.1), SEBlock=True, repeat_blocks=3, atrous=(1, 1, 1)) model = network_to_half(model) model.load_state_dict(torch.load('model_check_points/CRAN_V2/CARN_model_checkpoint.pt', 'cpu')) model = model.float() img = Image.open('/content/image.jpg').convert(\"RGB\") img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3) img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0) with torch.no_grad(): out = [model(i) for i in img_patches] img_upscale = img_splitter.merge_img_tensor(out) save_image(img_upscale, '/content/CARN_V2_single.jpg') cv2_imshow(cv2.imread('/content/CARN_V2_single.jpg')) We can connect to Google Drive with the following code. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the output files which are saved in the /content/ directory to the root of your Google Drive.\nimport shutil shutil.move('/content/CARN_V2_single.jpg', '/content/drive/My Drive/CARN_V2_single.jpg') More Notebooks @ eugenesiow/practical-ml and do star or drop us some feedback on how to improve the notebooks on the Github repo.\n","wordCount":"1161","inLanguage":"en","image":"https://news.machinelearning.sg/anime_super_resolution.jpg","datePublished":"2021-05-30T16:00:00+08:00","dateModified":"2021-05-30T16:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script><script async data-id=defa0fb0-3f27-4afe-b567-4f57d47c86e9 src=https://tinyads.io/e></script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h>News</a>
<span class=logo-switches><span class=theme-toggle><a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span><span class=lang-switch><ul><li><a href=https://machinelearning.sg>machinelearning.sg</a></li></ul></span></span></div><ul class=menu id=menu onscroll=menu_on_scroll()><li><a href=https://news.machinelearning.sg/about/><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Anime Image Super Resolution with PyTorch and Waifu2x</h1><div class=post-meta>May 30, 2021&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;Eugene</div></header><figure class=entry-cover><img srcset="https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution_hu4484701ed3273ce13f59859a947496bf_207757_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution_hu4484701ed3273ce13f59859a947496bf_207757_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution_hu4484701ed3273ce13f59859a947496bf_207757_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution_hu4484701ed3273ce13f59859a947496bf_207757_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution_hu4484701ed3273ce13f59859a947496bf_207757_1500x0_resize_q75_box.jpg 1500w ,https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution.jpg 1806w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/anime_super_resolution_with_pytorch/anime_super_resolution.jpg alt="Anime Image Super Resolution with PyTorch and Waifu2x"></figure><div class=toc><details><summary><div class=details accesskey=c>Table of Contents</div></summary><blockquote><ul><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li></ul><li><a href=#anime-super-resolution-in-pytorch aria-label="Anime Super Resolution in PyTorch">Anime Super Resolution in PyTorch</a></li><li><a href=#setting-up-the-environment aria-label="Setting up the Environment">Setting up the Environment</a><ul><ul><ul><li><a href=#setup-runtime aria-label="Setup Runtime">Setup Runtime</a></li><li><a href=#setup-dependencies aria-label="Setup Dependencies">Setup Dependencies</a></li></ul></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><ul><ul><li><a href=#getting-sample-images aria-label="Getting Sample Images">Getting Sample Images</a></li></ul></ul></ul></li><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li></ul></blockquote></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> A step-by-step tutorial to upscale anime images by 2x using Waifu2x and CARN V2 super resolution models.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Detect_Persons_From_Image_YOLOv5.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h1 id=anime-super-resolution-in-pytorch>Anime Super Resolution in PyTorch<a hidden class=anchor aria-hidden=true href=#anime-super-resolution-in-pytorch>#</a></h1><p>Notebook to do image super resolution on a single anime image automatically with pre-trained deep learning models in PyTorch.</p><p><a href=https://paperswithcode.com/task/image-super-resolution>Image Super-Resolution</a> techniques reconstruct a higher-resolution image or sequence from the observed lower-resolution images. In particular we do single-image super-resolution (SISR) which just use a single input image.</p><p>We utilise <a href=https://github.com/yu45020>yu45020&rsquo;s</a> neat <a href=https://github.com/yu45020/Waifu2x>PyTorch-based code</a> which re-implements the original <a href=https://github.com/nagadomi/waifu2x>waifu2x</a> and includes other image super-resolution models for us to quickly compare results.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the Environment</li><li>Getting the data</li><li>Using the Models (Running Inference)</li></ul><h1 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h1><h4 id=setup-runtime>Setup Runtime<a hidden class=anchor aria-hidden=true href=#setup-runtime>#</a></h4><p>The models run on <code>CPU</code> and there&rsquo;s no need to add a <code>GPU</code> hardware accelerator.</p><h4 id=setup-dependencies>Setup Dependencies<a hidden class=anchor aria-hidden=true href=#setup-dependencies>#</a></h4><p>We need to setup the <code>yu45020/Waifu2x</code> library and model checkpoints to run, so execute the command below to get the repository from Github.</p><pre><code>!git clone -l -s https://github.com/yu45020/Waifu2x.git waifu2x
%cd waifu2x
</code></pre><pre><code>Cloning into 'waifu2x'...
warning: --local is ignored
remote: Enumerating objects: 546, done.[K
remote: Total 546 (delta 0), reused 0 (delta 0), pack-reused 546[K
Receiving objects: 100% (546/546), 138.07 MiB | 25.76 MiB/s, done.
Resolving deltas: 100% (284/284), done.
/content/waifu2x
</code></pre><p>We need to extract the model checkpoints we intend to use with the following code.</p><pre><code>%cd model_check_points/Upconv_7/
!7z e anime.7z
%cd ../CRAN_V2/
!unzip -qq -o CRAN_V2_02_28_2019.zip
%cd ../vgg_7/
!7z e art.7z
%cd ../../
</code></pre><h1 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h1><h4 id=getting-sample-images>Getting Sample Images<a hidden class=anchor aria-hidden=true href=#getting-sample-images>#</a></h4><ul><li>An image of Nico Robin from anidb.</li></ul><p>We&rsquo;ll save this image to our local storage and view a preview in our notebook.</p><pre><code>import cv2
from urllib.request import urlretrieve
from google.colab.patches import cv2_imshow

# save the image to our local storage
urlretrieve('https://cdn-us.anidb.net/images/main/80187.jpg', '/content/image.jpg')

# display the images in the notebook
cv2_imshow(cv2.imread('/content/image.jpg'))
</code></pre><p><img src=Anime_Super_Resolution_PyTorch_files/Anime_Super_Resolution_PyTorch_10_0.png alt=png></p><h1 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h1><p>Now we want to define a function to load and run the waifu2x pre-trained model checkpoints from the directory we have unzipped to. We are running 2x image super resolution.</p><p>Specifically we are running the following steps:</p><ul><li>Load the input image as <code>img</code> and create a baseline resized image called <code>img_t</code> with bicubic scaling to compare against our super res outputs. Add this to a tensor, <code>final</code>, which we will use to make the grid of images to compare at the end.</li><li>Load (<code>.load_pre_train_weights()</code>) each of the model checkpoints specified in the <code>models</code> argument, which is a list of checkpoint <code>.json</code> files.</li><li>Split our input image <code>img</code>, if its too large, into patches using the <code>.split_img_tensor()</code> method.</li><li>Run the loaded model against each of the patches <code>model(i)</code>.</li><li>Merge the output patches with <code>.merge_img_tensor(out)</code>.</li><li>Concat the output super resolution upscaled image to the previous set of outputs (including the baseline <code>img_t</code>) using the <code>torch.cat()</code> function.</li><li>Save the <code>final</code> tensor as an image grid of all the outputs using the TorchVision utility <code>save_image()</code> function.</li></ul><pre><code>from utils.prepare_images import *
from Models import *
from torchvision.utils import save_image


def run_waifu2x(input_file, models, output_file):
  model = UpConv_7()
  img = Image.open(input_file).convert(&quot;RGB&quot;)
  img_resized = img.resize((img.size[0] * 2, img.size[1] * 2), Image.BICUBIC) 
  img_t = to_tensor(img_resized).unsqueeze(0) 
  # put the original image in the final tensor
  final = img_t
  for model_file in models:
    model.load_pre_train_weights(json_file=model_file)
    
    # overlapping split
    # if input image is too large, then split it into overlapped patches 
    # details can be found at [here](https://github.com/nagadomi/waifu2x/issues/238)
    img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3)
    img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0)
    with torch.no_grad():
        out = [model(i) for i in img_patches]
    img_upscale = img_splitter.merge_img_tensor(out)

    # concat the upscaled image to the final tensor
    final = torch.cat([final, img_upscale])
  
  # save the image using the torchvision util that allows us to draw a grid of images from a tensor
  save_image(final, output_file, nrow=(1+len(models)))
</code></pre><p>Run the model on our input image <code>/content/image.jpg</code> to get an output grid of images for comparison, <code>/content/waifu2x_out.jpg</code>. We will run the <code>noise0</code>, <code>noise2</code> and <code>no noise</code> models against our image and compare with the baseline.</p><pre><code>run_waifu2x('/content/image.jpg', ['model_check_points/Upconv_7/noise0_scale2.0x_model.json',
                                   'model_check_points/Upconv_7/noise2_scale2.0x_model.json',
                                   'model_check_points/Upconv_7/scale2.0x_model.json'], '/content/waifu2x_out.jpg')
</code></pre><p>Display the image grid output. We can see that the image is now 2x the original size. The leftmost image in the grid is our baseline that was scaled using <code>Image.BICUBIC</code> while the rest of the images are using <code>Waifu2x</code> models. I prefer the <code>noise2</code> model, the third image in the grid, most of the time.</p><pre><code># display the output grid of images
cv2_imshow(cv2.imread('/content/waifu2x_out.jpg'))
</code></pre><p><img src=Anime_Super_Resolution_PyTorch_files/Anime_Super_Resolution_PyTorch_17_0.png alt=png></p><p>Now we try the CARN V2 model. Similarly we define a function to run against our input image.</p><pre><code>from utils.prepare_images import *
from Models import *
from torchvision.utils import save_image


def run_carn(input_file, models, output_file):
  model = CARN_V2(color_channels=3, mid_channels=64, conv=nn.Conv2d,
                        single_conv_size=3, single_conv_group=1,
                        scale=2, activation=nn.LeakyReLU(0.1),
                        SEBlock=True, repeat_blocks=3, atrous=(1, 1, 1))   
  model = network_to_half(model)
  img = Image.open(input_file).convert(&quot;RGB&quot;)
  img_resized = img.resize((img.size[0] * 2, img.size[1] * 2), Image.BICUBIC) 
  img_t = to_tensor(img_resized).unsqueeze(0) 
  # put the original image in the final tensor
  final = img_t
  for model_file in models:
    model.load_state_dict(torch.load(model_file, 'cpu'))
    model = model.float() 

    # overlapping split
    # if input image is too large, then split it into overlapped patches 
    # details can be found at [here](https://github.com/nagadomi/waifu2x/issues/238)
    img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3)
    img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0)
    with torch.no_grad():
        out = [model(i) for i in img_patches]
    img_upscale = img_splitter.merge_img_tensor(out)

    # concat the upscaled image to the final tensor
    final = torch.cat([final, img_upscale])
  
  # save the image using the torchvision util that allows us to draw a grid of images from a tensor
  save_image(final, output_file, nrow=(1+len(models)))
</code></pre><p>We run the function against our input image and output the file to <code>CARN_V2_out.jpg</code> and display it using <code>cv2_imshow</code>. The left side image is the baseline while the right side image is the upscaled super-res image.</p><pre><code>run_carn('/content/image.jpg', ['model_check_points/CRAN_V2/CARN_model_checkpoint.pt'], '/content/CARN_V2_out.jpg')
cv2_imshow(cv2.imread('/content/CARN_V2_out.jpg'))
</code></pre><p><img src=Anime_Super_Resolution_PyTorch_files/Anime_Super_Resolution_PyTorch_21_0.png alt=png></p><p>If you want to save only your output image, change the output function to <code>save_image(img_upscale, '/content/CARN_V2_single.jpg')</code>. The code below does just this.</p><pre><code>from utils.prepare_images import *
from Models import *
from torchvision.utils import save_image

model = CARN_V2(color_channels=3, mid_channels=64, conv=nn.Conv2d,
                      single_conv_size=3, single_conv_group=1,
                      scale=2, activation=nn.LeakyReLU(0.1),
                      SEBlock=True, repeat_blocks=3, atrous=(1, 1, 1))   
model = network_to_half(model)
model.load_state_dict(torch.load('model_check_points/CRAN_V2/CARN_model_checkpoint.pt', 'cpu'))
model = model.float() 
img = Image.open('/content/image.jpg').convert(&quot;RGB&quot;)
img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3)
img_patches = img_splitter.split_img_tensor(img, scale_method=None, img_pad=0)
with torch.no_grad():
    out = [model(i) for i in img_patches]
img_upscale = img_splitter.merge_img_tensor(out)
save_image(img_upscale, '/content/CARN_V2_single.jpg')
cv2_imshow(cv2.imread('/content/CARN_V2_single.jpg'))
</code></pre><p><img src=Anime_Super_Resolution_PyTorch_files/Anime_Super_Resolution_PyTorch_23_0.png alt=png></p><p>We can connect to Google Drive with the following code. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><pre><code>from google.colab import drive
drive.mount('/content/drive/')
</code></pre><p>You can move the output files which are saved in the <code>/content/</code> directory to the root of your Google Drive.</p><pre><code>import shutil
shutil.move('/content/CARN_V2_single.jpg', '/content/drive/My Drive/CARN_V2_single.jpg')
</code></pre><p>More Notebooks @ <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> and do star or drop us some feedback on how to improve the notebooks on the <a href=https://github.com/eugenesiow/practical-ml/>Github repo</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/computer-vision>Computer Vision</a></li><li><a href=https://news.machinelearning.sg/tags/deep-learning>Deep Learning</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/source-code>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/image-super-resolution>Image Super Resolution</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on twitter" href="https://twitter.com/intent/tweet/?text=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f&hashtags=ComputerVision%2cDeepLearning%2cMachineLearning%2cSourceCode%2cPyTorch%2cImageSuperResolution%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f&title=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x&summary=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f&title=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on whatsapp" href="https://api.whatsapp.com/send?text=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Anime Image Super Resolution with PyTorch and Waifu2x on telegram" href="https://telegram.me/share/url?text=Anime%20Image%20Super%20Resolution%20with%20PyTorch%20and%20Waifu2x&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fanime_super_resolution_with_pytorch%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top" accesskey=g><button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=https://news.machinelearning.sg/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>