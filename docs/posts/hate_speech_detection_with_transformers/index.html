<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Hate Speech Detection with Transformers | News @ machinelearning.sg</title><meta name=keywords content="Natural Language Processing,Machine Learning,GPU,Source Code,PyTorch,Hate Speech Detection,Jupyter Notebook,Colab"><meta name=description content="tl;dr A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The trained model has a BERT-based transformer architecture.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta name=author content="Eugene"><link rel=canonical href=https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/><link href=https://news.machinelearning.sg/assets/css/stylesheet.min.62447981861ad5bc7761e73be43801fe136a2aaaa8b7afd18ce582c358716848.css integrity="sha256-YkR5gYYa1bx3Yec75DgB/hNqKqqot6/RjOWCw1hxaEg=" rel="preload stylesheet" as=style><link rel=icon href=https://news.machinelearning.sg/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://news.machinelearning.sg/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://news.machinelearning.sg/favicon-32x32.png><link rel=apple-touch-icon href=https://news.machinelearning.sg/apple-touch-icon.png><link rel=mask-icon href=https://news.machinelearning.sg/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-185405110-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="Hate Speech Detection with Transformers"><meta property="og:description" content="tl;dr A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The trained model has a BERT-based transformer architecture.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><meta property="og:type" content="article"><meta property="og:url" content="https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/"><meta property="og:image" content="https://news.machinelearning.sg/hate_speech.jpg"><meta property="article:published_time" content="2021-01-06T08:00:00+08:00"><meta property="article:modified_time" content="2021-01-06T08:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://news.machinelearning.sg/hate_speech.jpg"><meta name=twitter:title content="Hate Speech Detection with Transformers"><meta name=twitter:description content="tl;dr A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The trained model has a BERT-based transformer architecture.
 Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.
Hit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Hate Speech Detection with Transformers","name":"Hate Speech Detection with Transformers","description":"tl;dr A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The trained model has a BERT-based transformer architecture. …","keywords":["Natural Language Processing","Machine Learning","GPU","Source Code","PyTorch","Hate Speech Detection","Jupyter Notebook","Colab"],"articleBody":" tl;dr A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The trained model has a BERT-based transformer architecture.\n Practical Machine Learning - Learn Step-by-Step to Train a Model A great way to learn is by going step-by-step through the process of training and evaluating the model.\nHit the Open in Colab button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough. \nContinue on if you prefer reading the code here.\nHate Speech Detection on Dynabench \nNotebook to train an RoBERTa model to perform hate speech detection. The dataset used is the Dynabench Task - Dynamically Generated Hate Speech Dataset from the paper by Vidgen et al. (2020).\n The dataset provides 40,623 examples with annotations for fine-grained labels, including a large number of challenging contrastive perturbation examples. Unusually for an abusive content dataset, it comprises 54% hateful and 46% not hateful entries.\n There is no published state-of-the-art model on this dataset at this point though the task on Dynabench reports mean MER (Model Error Rate) scores. We are able to train the model to a F1 score of 86.6% after only 1 epoch.\nThe notebook is structured as follows:\n Setting up the GPU Environment Getting Data Training and Testing the Model Using the Model (Running Inference)  Task Description  Hate Speech Detection is the automated task of detecting if a piece of text contains hate speech.\n Setting up the GPU Environment Ensure we have a GPU runtime If you’re running this notebook in Google Colab, select Runtime  Change Runtime Type from the menubar. Ensure that GPU is selected as the Hardware accelerator. This will allow us to use the GPU to train the model subsequently.\nInstall Dependencies and Restart Runtime !pip install -q transformers !pip install -q simpletransformers You might see the error ERROR: google-colab X.X.X has requirement ipykernel~=X.X, but you'll have ipykernel X.X.X which is incompatible after installing the dependencies. This is normal and caused by the simpletransformers library.\nThe solution to this will be to reset the execution environment now. Go to the menu Runtime  Restart runtime then continue on from the next section to download and process the data.\nGetting Data Pulling the data from Github The code below uses pandas to pull the dataset as a CSV file from the official Github repository. The dataset is now stored as a dataframe, in which we can transform for use with the simpletransformers library to train the model. So we pull the CSV from Github and split the CSV into training and test sets by using the column split in the CSV file which indicates which example/sample is a training sample and which is a test sample.\nimport pandas as pd df = pd.read_csv('https://raw.githubusercontent.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset/main/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv') train_df = df[df['split'] == 'train'] # split the dataset by the column 'split' which labels 'train' and 'test' samples test_df = df[df['split'] == 'test'] # split the dataset by the column 'split' which labels 'train' and 'test' samples Once done we can take a look at the head() of the training set to check if our data has been retrieved properly.\ntrain_df.head() We transform the dataframe column label so that the labels hate and nothate are now integers 1 and 0 respectively. This input format of labels is required for our training step with the transformers library.\ntrain_df = train_df.replace({'label': {'hate': 1, 'nothate': 0}}) # relabel the `label` column, hate is 1 and nothate is 0 test_df = test_df.replace({'label': {'hate': 1, 'nothate': 0}}) # relabel the `label` column, hate is 1 and nothate is 0 train_df.head() We also rename the label column to labels as this also conforms to the input format required for the simpletransformers library.\ntrain_df = train_df.rename(columns={'label': 'labels'}) test_df = test_df.rename(columns={'label': 'labels'}) We can now take a look at the train and test set sizes. We see that this dataset is quite special as the hate and nothate class sizes are actually quite close in proportion.\ndata = [[train_df.labels.value_counts()[0], test_df.labels.value_counts()[0]], [train_df.labels.value_counts()[1], test_df.labels.value_counts()[1]]] # Prints out the dataset sizes of train test and validate as per the table. pd.DataFrame(data, columns=['Train', 'Test']) Training and Testing the Model Set the Hyperparmeters First we setup the hyperparamters. We train to 1 epoch only as want the training to complete fast. The important parameters are the max_seq_length, which we set to 64 and sliding_window to true. As we don’t have a high RAM GPU on Colab we can’t set the max_seq_length to too large a value and by using sliding_window we at least are able to handle longer sequences without truncation. See the simpletransformers documentation for a more detailed explanation of a sliding window.\ntrain_args = { 'reprocess_input_data': True, 'overwrite_output_dir': True, 'sliding_window': True, 'max_seq_length': 64, 'num_train_epochs': 1, 'train_batch_size': 128, 'fp16': True, 'output_dir': '/outputs/', } Train the Model Once we have setup the hyperparemeters in the train_args dictionary, the next step would be to train the model. We use the RoBERTa model from the awesome Hugging Face Transformers library and use the Simple Transformers library on top of it to make it so we can train the classification model with just 2 lines of code.\nRoBERTa is an optimized BERT model by Facebook Research with better performance on the masked language modeling objective that modifies key hyperparameters in BERT, including removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. In short, its a bigger but generally better performing BERT model we can easily plug in here with the transformers library.\nfrom simpletransformers.classification import ClassificationModel import pandas as pd import logging import sklearn logging.basicConfig(level=logging.DEBUG) transformers_logger = logging.getLogger('transformers') transformers_logger.setLevel(logging.WARNING) # We use the XLNet base cased pre-trained model. model = ClassificationModel('roberta', 'roberta-base', num_labels=2, args=train_args) # Train the model, there is no development or validation set for this dataset # https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping model.train_model(train_df) # Evaluate the model in terms of accuracy score result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score) The model has been trained and evaluating on the test set after training to only 1 epoch gives an accuracy of 85.9%. We want to also evaluate the F1 score which is a better measure as the dataset is slightly imbalanced.\nresult, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.f1_score) We see that the output F1 score from the model after training for 1 epoch is 86.6% (‘acc’: 0.8661675245671503).\nUsing the Model (Running Inference) Running the model to do some predictions/inference is as simple as calling model.predict(input_list).\nsamples = [test_df[test_df['labels'] == 0].sample(1).iloc[0]['text']] # get a random sample from the test set which is nothate predictions, _ = model.predict(samples) label_dict = {0: 'nothate', 1: 'hate'} for idx, sample in enumerate(samples): print('{} - {}: {}'.format(idx, label_dict[predictions[idx]], sample)) We can also generate a results.txt file from the test set. The file is stored in our Colab environment. Hit the folder icon at the side and you can download the results.txt file from the file browser. You can submit this .txt file to Dynabench for evaluation if you wish to.\npredictions, _ = model.predict(test_df['text'].tolist()) df = pd.DataFrame(predictions) df.to_csv('results.txt', index=False, header=False) # saves the prediction results to a file in the colab environment We can connect to Google Drive with the following code to save any files you want to persist. You can also click the Files icon on the left panel and click Mount Drive to mount your Google Drive.\nThe root of your Google Drive will be mounted to /content/drive/My Drive/. If you have problems mounting the drive, you can check out this tutorial.\nfrom google.colab import drive drive.mount('/content/drive/') You can move the model checkpount files which are saved in the /content/outputs/best_model/ directory to your Google Drive.\nimport shutil shutil.move('/outputs/', \"/content/drive/My Drive/outputs/\") More Such Notebooks Visit or star the eugenesiow/practical-ml repository on Github for more such notebooks:\n Alternatives to Colab Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:\n Google Colab vs Paperspace Gradient  ","wordCount":"1299","inLanguage":"en","image":"https://news.machinelearning.sg/hate_speech.jpg","datePublished":"2021-01-06T08:00:00+08:00","dateModified":"2021-01-06T08:00:00+08:00","author":{"@type":"Person","name":"Eugene"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/"},"publisher":{"@type":"Organization","name":"News @ machinelearning.sg","logo":{"@type":"ImageObject","url":"https://news.machinelearning.sg/favicon.ico"}}}</script><script async data-id=defa0fb0-3f27-4afe-b567-4f57d47c86e9 src=https://tinyads.io/e></script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://news.machinelearning.sg/ accesskey=h>News</a>
<span class=logo-switches><span class=theme-toggle><a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span><span class=lang-switch><ul><li><a href=https://machinelearning.sg>machinelearning.sg</a></li></ul></span></span></div><ul class=menu id=menu onscroll=menu_on_scroll()><li><a href=https://news.machinelearning.sg/about/><span>About</span></a></li><li><a href=https://news.machinelearning.sg/archives/><span>Archive</span></a></li><li><a href=https://news.machinelearning.sg/tags/><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Hate Speech Detection with Transformers</h1><div class=post-meta>January 6, 2021&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Eugene</div></header><figure class=entry-cover><img srcset="https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech_hu6ede1b70bbb84b43076765c3bd29f7aa_140679_360x0_resize_q75_box.jpg 360w ,https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech_hu6ede1b70bbb84b43076765c3bd29f7aa_140679_480x0_resize_q75_box.jpg 480w ,https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech_hu6ede1b70bbb84b43076765c3bd29f7aa_140679_720x0_resize_q75_box.jpg 720w ,https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech_hu6ede1b70bbb84b43076765c3bd29f7aa_140679_1080x0_resize_q75_box.jpg 1080w ,https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech_hu6ede1b70bbb84b43076765c3bd29f7aa_140679_1500x0_resize_q75_box.jpg 1500w ,https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech.jpg 1879w" sizes="(min-width: 768px) 720px, 100vw" src=https://news.machinelearning.sg/posts/hate_speech_detection_with_transformers/hate_speech.jpg alt="Hate Speech Detection on Dynabench"></figure><div class=toc><details><summary><div class=details accesskey=c>Table of Contents</div></summary><blockquote><ul><ul><li><a href=#practical-machine-learning---learn-step-by-step-to-train-a-model aria-label="Practical Machine Learning - Learn Step-by-Step to Train a Model">Practical Machine Learning - Learn Step-by-Step to Train a Model</a></li><li><a href=#hate-speech-detection-on-dynabench aria-label="Hate Speech Detection on Dynabench">Hate Speech Detection on Dynabench</a><ul><ul><li><a href=#task-description aria-label="Task Description">Task Description</a></li></ul></ul></li><li><a href=#setting-up-the-gpu-environment aria-label="Setting up the GPU Environment">Setting up the GPU Environment</a><ul><ul><li><a href=#ensure-we-have-a-gpu-runtime aria-label="Ensure we have a GPU runtime">Ensure we have a GPU runtime</a></li><li><a href=#install-dependencies-and-restart-runtime aria-label="Install Dependencies and Restart Runtime">Install Dependencies and Restart Runtime</a></li></ul></ul></li><li><a href=#getting-data aria-label="Getting Data">Getting Data</a><ul><ul><li><a href=#pulling-the-data-from-github aria-label="Pulling the data from Github">Pulling the data from Github</a></li></ul></ul></li></ul><li><a href=#training-and-testing-the-model aria-label="Training and Testing the Model">Training and Testing the Model</a><ul><ul><ul><li><a href=#set-the-hyperparmeters aria-label="Set the Hyperparmeters">Set the Hyperparmeters</a></li><li><a href=#train-the-model aria-label="Train the Model">Train the Model</a></li></ul></ul><li><a href=#using-the-model-running-inference aria-label="Using the Model (Running Inference)">Using the Model (Running Inference)</a></li><li><a href=#more-such-notebooks aria-label="More Such Notebooks">More Such Notebooks</a></li><li><a href=#alternatives-to-colab aria-label="Alternatives to Colab">Alternatives to Colab</a></li></ul></li></ul></blockquote></details></div><div class=post-content><blockquote><p><strong>tl;dr</strong> A step-by-step tutorial to train a hate speech detection model to classify text containing hate speech. The
trained model has a BERT-based transformer architecture.</p></blockquote><h2 id=practical-machine-learning---learn-step-by-step-to-train-a-model>Practical Machine Learning - Learn Step-by-Step to Train a Model<a hidden class=anchor aria-hidden=true href=#practical-machine-learning---learn-step-by-step-to-train-a-model>#</a></h2><p>A great way to learn is by going step-by-step through the process of training and evaluating the model.</p><p>Hit the <strong><code>Open in Colab</code></strong> button below to launch a Jupyter Notebook in the cloud with a step-by-step walkthrough.
<a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Hate_Speech_Detection_Dynabench.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Continue on if you prefer reading the code here.</p><h2 id=hate-speech-detection-on-dynabench>Hate Speech Detection on Dynabench<a hidden class=anchor aria-hidden=true href=#hate-speech-detection-on-dynabench>#</a></h2><p><a href=https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Hate_Speech_Detection_Dynabench.ipynb title="Open in Colab"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Notebook to train an RoBERTa model to perform hate speech detection. The <a href=https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset>dataset</a> used is the Dynabench Task - Dynamically Generated Hate Speech Dataset from the paper by <a href=https://arxiv.org/abs/2012.15761>Vidgen et al. (2020)</a>.</p><blockquote><p>The dataset provides 40,623 examples with annotations for fine-grained labels, including a large number of challenging contrastive perturbation examples. Unusually for an abusive content dataset, it comprises 54% hateful and 46% not hateful entries.</p></blockquote><p>There is no published state-of-the-art model on this dataset at this point though the task on Dynabench reports mean MER (Model Error Rate) scores. We are able to train the model to a F1 score of <strong>86.6%</strong> after only 1 epoch.</p><p>The notebook is structured as follows:</p><ul><li>Setting up the GPU Environment</li><li>Getting Data</li><li>Training and Testing the Model</li><li>Using the Model (Running Inference)</li></ul><h4 id=task-description>Task Description<a hidden class=anchor aria-hidden=true href=#task-description>#</a></h4><blockquote><p>Hate Speech Detection is the automated task of detecting if a piece of text contains hate speech.</p></blockquote><h2 id=setting-up-the-gpu-environment>Setting up the GPU Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-gpu-environment>#</a></h2><h4 id=ensure-we-have-a-gpu-runtime>Ensure we have a GPU runtime<a hidden class=anchor aria-hidden=true href=#ensure-we-have-a-gpu-runtime>#</a></h4><p>If you&rsquo;re running this notebook in Google Colab, select <code>Runtime</code> > <code>Change Runtime Type</code> from the menubar. Ensure that <code>GPU</code> is selected as the <code>Hardware accelerator</code>. This will allow us to use the GPU to train the model subsequently.</p><h4 id=install-dependencies-and-restart-runtime>Install Dependencies and Restart Runtime<a hidden class=anchor aria-hidden=true href=#install-dependencies-and-restart-runtime>#</a></h4><pre><code>!pip install -q transformers
!pip install -q simpletransformers
</code></pre><p>You might see the error <code>ERROR: google-colab X.X.X has requirement ipykernel~=X.X, but you'll have ipykernel X.X.X which is incompatible</code> after installing the dependencies. <strong>This is normal</strong> and caused by the <code>simpletransformers</code> library.</p><p>The <strong>solution</strong> to this will be to <strong>reset the execution environment</strong> now. Go to the menu <code>Runtime</code> > <code>Restart runtime</code> then continue on from the next section to download and process the data.</p><h2 id=getting-data>Getting Data<a hidden class=anchor aria-hidden=true href=#getting-data>#</a></h2><h4 id=pulling-the-data-from-github>Pulling the data from Github<a hidden class=anchor aria-hidden=true href=#pulling-the-data-from-github>#</a></h4><p>The code below uses <code>pandas</code> to pull the dataset as a CSV file from the <a href=https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset>official Github repository</a>. The dataset is now stored as a dataframe, in which we can transform for use with the <code>simpletransformers</code> library to train the model. So we pull the CSV from Github and split the CSV into training and test sets by using the column <code>split</code> in the CSV file which indicates which example/sample is a training sample and which is a test sample.</p><pre><code>import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset/main/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv')
train_df = df[df['split'] == 'train'] # split the dataset by the column 'split' which labels 'train' and 'test' samples
test_df = df[df['split'] == 'test'] # split the dataset by the column 'split' which labels 'train' and 'test' samples
</code></pre><p>Once done we can take a look at the <code>head()</code> of the training set to check if our data has been retrieved properly.</p><pre><code>train_df.head()
</code></pre><p>We transform the dataframe column <code>label</code> so that the labels <code>hate</code> and <code>nothate</code> are now integers <code>1</code> and <code>0</code> respectively. This input format of labels is required for our training step with the <code>transformers</code> library.</p><pre><code>train_df = train_df.replace({'label': {'hate': 1, 'nothate': 0}}) # relabel the `label` column, hate is 1 and nothate is 0
test_df = test_df.replace({'label': {'hate': 1, 'nothate': 0}}) # relabel the `label` column, hate is 1 and nothate is 0
train_df.head()
</code></pre><p>We also rename the <code>label</code> column to <code>labels</code> as this also conforms to the input format required for the <code>simpletransformers</code> library.</p><pre><code>train_df = train_df.rename(columns={'label': 'labels'})
test_df = test_df.rename(columns={'label': 'labels'})
</code></pre><p>We can now take a look at the train and test set sizes. We see that this dataset is quite special as the <code>hate</code> and <code>nothate</code> class sizes are actually quite close in proportion.</p><pre><code>data = [[train_df.labels.value_counts()[0], test_df.labels.value_counts()[0]], 
        [train_df.labels.value_counts()[1], test_df.labels.value_counts()[1]]]
# Prints out the dataset sizes of train test and validate as per the table.
pd.DataFrame(data, columns=['Train', 'Test'])
</code></pre><h1 id=training-and-testing-the-model>Training and Testing the Model<a hidden class=anchor aria-hidden=true href=#training-and-testing-the-model>#</a></h1><h4 id=set-the-hyperparmeters>Set the Hyperparmeters<a hidden class=anchor aria-hidden=true href=#set-the-hyperparmeters>#</a></h4><p>First we setup the hyperparamters. We train to 1 epoch only as want the training to complete fast. The important parameters are the <code>max_seq_length</code>, which we set to <code>64</code> and <code>sliding_window</code> to true. As we don&rsquo;t have a high RAM GPU on Colab we can&rsquo;t set the <code>max_seq_length</code> to too large a value and by using <code>sliding_window</code> we at least are able to handle longer sequences without truncation. See the <a href=https://simpletransformers.ai/docs/classification-specifics/#training-with-sliding-window>simpletransformers documentation</a> for a more detailed explanation of a sliding window.</p><pre><code>train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'sliding_window': True,
    'max_seq_length': 64,
    'num_train_epochs': 1,
    'train_batch_size': 128,
    'fp16': True,
    'output_dir': '/outputs/',
}
</code></pre><h4 id=train-the-model>Train the Model<a hidden class=anchor aria-hidden=true href=#train-the-model>#</a></h4><p>Once we have setup the hyperparemeters in the <code>train_args</code> dictionary, the next step would be to train the model. We use the RoBERTa model from the awesome <a href=https://github.com/huggingface/transformers>Hugging Face Transformers</a> library and use the <a href=https://simpletransformers.ai/docs/classification-models/>Simple Transformers library</a> on top of it to make it so we can train the classification model with just 2 lines of code.</p><p>RoBERTa is an optimized BERT model by Facebook Research with better performance on the masked language modeling objective that modifies key hyperparameters in BERT, including removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. In short, its a bigger but generally better performing BERT model we can easily plug in here with the transformers library.</p><pre><code>from simpletransformers.classification import ClassificationModel
import pandas as pd
import logging
import sklearn

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model = ClassificationModel('roberta', 'roberta-base', num_labels=2, args=train_args) 

# Train the model, there is no development or validation set for this dataset 
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model.train_model(train_df)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)
</code></pre><p>The model has been trained and evaluating on the test set after training to only 1 epoch gives an accuracy of <strong>85.9%</strong>. We want to also evaluate the F1 score which is a better measure as the dataset is slightly imbalanced.</p><pre><code>result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.f1_score)
</code></pre><p>We see that the output F1 score from the model after training for 1 epoch is <strong>86.6%</strong> (&lsquo;acc&rsquo;: 0.8661675245671503).</p><h2 id=using-the-model-running-inference>Using the Model (Running Inference)<a hidden class=anchor aria-hidden=true href=#using-the-model-running-inference>#</a></h2><p>Running the model to do some predictions/inference is as simple as calling <code>model.predict(input_list)</code>.</p><pre><code>samples = [test_df[test_df['labels'] == 0].sample(1).iloc[0]['text']] # get a random sample from the test set which is nothate
predictions, _ = model.predict(samples)
label_dict = {0: 'nothate', 1: 'hate'}
for idx, sample in enumerate(samples):
  print('{} - {}: {}'.format(idx, label_dict[predictions[idx]], sample))
</code></pre><p>We can also generate a <code>results.txt</code> file from the test set. The file is stored in our Colab environment. Hit the <code>folder</code> icon at the side and you can download the <code>results.txt</code> file from the file browser. You can submit this <code>.txt</code> file to <a href=https://dynabench.org/tasks/5#overall>Dynabench</a> for evaluation if you wish to.</p><pre><code>predictions, _ = model.predict(test_df['text'].tolist())
df = pd.DataFrame(predictions)
df.to_csv('results.txt', index=False, header=False) # saves the prediction results to a file in the colab environment
</code></pre><p>We can connect to Google Drive with the following code to save any files you want to persist. You can also click the <code>Files</code> icon on the left panel and click <code>Mount Drive</code> to mount your Google Drive.</p><p>The root of your Google Drive will be mounted to <code>/content/drive/My Drive/</code>. If you have problems mounting the drive, you can check out this <a href=https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166>tutorial</a>.</p><pre><code>from google.colab import drive
drive.mount('/content/drive/')
</code></pre><p>You can move the model checkpount files which are saved in the <code>/content/outputs/best_model/</code> directory to your Google Drive.</p><pre><code>import shutil
shutil.move('/outputs/', &quot;/content/drive/My Drive/outputs/&quot;)
</code></pre><h2 id=more-such-notebooks>More Such Notebooks<a hidden class=anchor aria-hidden=true href=#more-such-notebooks>#</a></h2><p>Visit or star the <a href=https://github.com/eugenesiow/practical-ml>eugenesiow/practical-ml</a> repository on Github for more such notebooks:</p><iframe src="https://ghbtns.com/github-btn.html?user=eugenesiow&repo=practical-ml&type=star&count=true&size=large" frameborder=0 scrolling=0 width=170 height=30 title="Practical Machine Learning"></iframe><h2 id=alternatives-to-colab>Alternatives to Colab<a hidden class=anchor aria-hidden=true href=#alternatives-to-colab>#</a></h2><p>Here are some alternatives to Google Colab to train models or run Jupyter Notebooks in the cloud:</p><ul><li><a href=https://news.machinelearning.sg/posts/google_colab_vs_paperspace_gradient/>Google Colab vs Paperspace Gradient</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://news.machinelearning.sg/tags/natural-language-processing>Natural Language Processing</a></li><li><a href=https://news.machinelearning.sg/tags/machine-learning>Machine Learning</a></li><li><a href=https://news.machinelearning.sg/tags/gpu>GPU</a></li><li><a href=https://news.machinelearning.sg/tags/source-code>Source Code</a></li><li><a href=https://news.machinelearning.sg/tags/pytorch>PyTorch</a></li><li><a href=https://news.machinelearning.sg/tags/hate-speech-detection>Hate Speech Detection</a></li><li><a href=https://news.machinelearning.sg/tags/jupyter-notebook>Jupyter Notebook</a></li><li><a href=https://news.machinelearning.sg/tags/colab>Colab</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on twitter" href="https://twitter.com/intent/tweet/?text=Hate%20Speech%20Detection%20with%20Transformers&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f&hashtags=NaturalLanguageProcessing%2cMachineLearning%2cGPU%2cSourceCode%2cPyTorch%2cHateSpeechDetection%2cJupyterNotebook%2cColab"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f&title=Hate%20Speech%20Detection%20with%20Transformers&summary=Hate%20Speech%20Detection%20with%20Transformers&source=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f&title=Hate%20Speech%20Detection%20with%20Transformers"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on whatsapp" href="https://api.whatsapp.com/send?text=Hate%20Speech%20Detection%20with%20Transformers%20-%20https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Hate Speech Detection with Transformers on telegram" href="https://telegram.me/share/url?text=Hate%20Speech%20Detection%20with%20Transformers&url=https%3a%2f%2fnews.machinelearning.sg%2fposts%2fhate_speech_detection_with_transformers%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://news.machinelearning.sg/>News @ machinelearning.sg</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top" accesskey=g><button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=https://news.machinelearning.sg/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>